{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Icechunk - Open-source, cloud-native transactional tensor storage engine","text":"<p>Home</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Home / contributing</p>"},{"location":"contributing/#contributing","title":"Contributing","text":"<p>\ud83d\udc4b Hi! Thanks for your interest in contributing to Icechunk!</p> <p>Icechunk is an open source (Apache 2.0) project and welcomes contributions in the form of:</p> <ul> <li>Usage questions - open a GitHub issue</li> <li>Bug reports - open a GitHub issue</li> <li>Feature requests - open a GitHub issue</li> <li>Documentation improvements - open a GitHub pull request</li> <li>Bug fixes and enhancements - open a GitHub pull request</li> </ul>"},{"location":"contributing/#development","title":"Development","text":""},{"location":"contributing/#python-development-workflow","title":"Python Development Workflow","text":"<p>Create / activate a virtual environment:</p> VenvConda / Mamba <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>mamba create -n icechunk python=3.12 rust zarr\nmamba activate icechunk\n</code></pre> <p>Install <code>maturin</code>:</p> <pre><code>pip install maturin\n</code></pre> <p>Build the project in dev mode:</p> <pre><code>maturin develop\n</code></pre> <p>or build the project in editable mode:</p> <pre><code>pip install -e icechunk@.\n</code></pre>"},{"location":"contributing/#rust-development-workflow","title":"Rust Development Workflow","text":"<p>TODO</p>"},{"location":"contributing/#roadmap","title":"Roadmap","text":"<p>The initial release of Icechunk is just the beginning. We have a lot more planned for the format and the API.</p>"},{"location":"contributing/#roadmap-to-icechunk-10","title":"Roadmap to Icechunk 1.0","text":""},{"location":"contributing/#core-format","title":"Core format","text":"<p>The core format is where we\u2019ve put most of our effort to date and we plan to continue work in this area. Leading up to the 1.0 release, we will be focused on stabilizing data structures for snapshots, chunk manifests, attribute files and references. We\u2019ll also document and add more mechanisms for on-disk format evolution. The intention is to guarantee that any new version of Icechunk can always read repositories generated with any previous versions. We expect to evolve the spec and the Rust implementation as we stabilize things.</p>"},{"location":"contributing/#features","title":"Features","text":"<ul> <li>Commit conflict detection, resolution and rebase</li> <li>Current session status (git status)</li> <li>Support Google Cloud Storage</li> <li>Support Azure Blob Storage</li> <li>Distributed write support with dask.array</li> <li>Credential sets for virtual datasets</li> <li> <p>Complete Python API:</p> <ul> <li>list refs</li> <li>read hierarchy</li> <li>repo size</li> </ul> </li> <li> <p>Better documentation and examples</p> </li> </ul>"},{"location":"contributing/#performance","title":"Performance","text":"<ul> <li>Create scale benchmark of daily updated dataset (15M chunks, 30k commits)</li> <li>Create performance benchmark for read and write, compare to Zarr 3 + fsspec/s3</li> <li>Optimize if needed based on benchmarks: manifest splitting, history splitting, attribute files.</li> <li>Optimize virtual dataset prefixes</li> <li>Improve list_dir performance (which will improve other functions)</li> <li>Improve performance of get_size</li> </ul>"},{"location":"contributing/#refactoring","title":"Refactoring","text":"<ul> <li>Improve Python API<ul> <li>Separate Repo and Zarr Store</li> <li>Make it clear at the API level what methods require commit and which ones don't</li> <li>Transactions as context managers</li> <li>Better <code>repr</code></li> </ul> </li> </ul>"},{"location":"contributing/#correctness","title":"Correctness","text":"<ul> <li>Ingest native datasets: hrrr, gfs, sentinel data cube</li> <li>Ingest virtual datasets: arco-era5, lens, cmip6</li> <li>Add property and stateful tests from Zarr 3 and Arraylake</li> <li>Document and exercise on-disk versioning</li> </ul>"},{"location":"contributing/#roadmap-beyond-icechunk-10","title":"Roadmap beyond Icechunk 1.0","text":""},{"location":"contributing/#features_1","title":"Features","text":"<ul> <li>Persistent configuration</li> <li>More powerful conflict detection and resolution</li> <li>Better error messages</li> <li>Version expiration, garbage collection</li> <li>Efficient rename</li> </ul>"},{"location":"contributing/#performance_1","title":"Performance","text":"<p>While the initial performance benchmarks of Icechunk are very encouraging, we know that we have only scratched the surface of what is possible. We are looking forward to investing in a number of optimizations that will really make Icechunk fly!</p> <ul> <li>Request batching and splitting</li> <li>Manifest compression and serialization improvements</li> <li>Manifest split heuristics</li> <li>Bringing parts of the codec pipeline to the Rust side</li> <li>Better caching, in memory and optionally on local disk</li> </ul>"},{"location":"contributing/#other-utilities","title":"Other Utilities","text":"<p>On top of the foundation of the Icechunk format, we are looking to build a suite of other utilities that operate on data stored in Icechunk. Some examples:</p> <ul> <li>Garbage collection - version controlled data has the potential to accumulate data that is no longer needed but is still included in the store. A garbage collection process will allow users to safely cleanup data from old versions of an Icechunk dataset.</li> <li>Chunk compaction - data written by Zarr may result in many small chunks in object storage. A chunk compaction service will allow users to retroactively compact small chunks into larger objects (similar to Zarr\u2019s sharding format), resulting in potential performance improvements and fewer objects in storage.</li> <li>Manifest optimization - knowing how the data is queried would allow to optimize the shape and splits of the chunk manifests, in such a way as to minimize the amount of data needed to execute the most frequent queries.</li> </ul>"},{"location":"contributing/#zarr-related","title":"Zarr-related","text":"<p>We\u2019re very excited about a number of extensions to Zarr that would work great with Icechunk.</p> <ul> <li>Variable length chunks</li> <li>Chunk-level statistics</li> </ul>"},{"location":"contributing/#miscellaneous","title":"Miscellaneous","text":"<p>There\u2019s much more than what we\u2019ve written above on the roadmap. Some examples:</p> <ul> <li>Multi-language support (R, Julia, \u2026)</li> <li>Exposing high level API (groups and arrays) to Python users</li> <li>Make more details of the format accessible through configuration</li> <li>Improve Xarray backend to integrate more directly with Icechunk</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Home / faq</p>"},{"location":"faq/#faq","title":"FAQ","text":""},{"location":"faq/#why-was-icechunk-created","title":"Why was Icechunk created?","text":"<p>Icechunk was created by Earthmover as the open-source format for its cloud data platform Arraylake.</p> <p>Icechunk builds on the successful Zarr project. Zarr is a great foundation for storing and querying large multidimensional array data in a flexible, scalable way. But when people started using Zarr together with cloud object storage in a collaborative way, it became clear that Zarr alone could not offer the sort of consistency many users desired. Icechunk makes Zarr work a little bit more like a database, enabling different users / processes to safely read and write concurrently, while still only using object storage as a persistence layer.</p> <p>Another motivation for Icechunk was the success of Kerchunk. The Kerchunk project showed that it was possible to map many existing archival formats (e.g. HDF5, NetCDF, GRIB) to the Zarr data model without actually rewriting any bytes, by creating \"virtual\" Zarr datasets referencing binary chunks inside other files. Doing this at scale requires tracking millions of \"chunk references.\" Icechunk's storage model allows for these virtual chunks to be stored seamlessly alongside native Zarr chunks.</p> <p>Finally, Icechunk provides a universal I/O layer for cloud object storage, implementing numerous performance optimizations designed to accelerate data-intensive applications.</p> <p>Solving these problems in one go via a powerful, open-source, Rust-based library will bring massive benefits to the cloud-native scientific data community.</p>"},{"location":"faq/#where-does-the-name-icechunk-come-from","title":"Where does the name \"Icechunk\" come from?","text":"<p>Icechunk was inspired partly by Apache Iceberg, a popular cloud-native table format. However, instead of storing tabular data, Icechunk stores multidimensional arrays, for which the individual unit of storage is the chunk.</p>"},{"location":"faq/#when-should-i-use-icechunk","title":"When should I use Icechunk?","text":"<p>Here are some scenarios where it makes sense to use Icechunk:</p> <ul> <li>You want to store large, dynamically evolving multi-dimensional array (a.k.a. tensor) in cloud object storage.</li> <li>You want to allow multiple uncoordinated processes to access your data at the same time (like a database).</li> <li>You want to be able to safely roll back failed updates or revert Zarr data to an earlier state.</li> <li>You want to use concepts from data version control (e.g. tagging, branching, snapshots) with Zarr data.</li> <li>You want to achieve cloud-native performance on archival file formats (HDF5, NetCDF, GRIB) by exposing them as virtual Zarr datasets and need to store chunk references in a a robust, scalable, interoperable way.</li> <li>You want to get the best possible performance for reading / writing tensor data in AI / ML workflows.</li> </ul>"},{"location":"faq/#what-are-the-downsides-to-using-icechunk","title":"What are the downsides to using Icechunk?","text":"<p>As with all things in technology, the benefits of Icechunk come with some tradeoffs:</p> <ul> <li>There may be slightly higher cold-start latency to opening Icechunk datasets compared with regular Zarr.</li> <li>The on-disk format is less transparent than regular Zarr.</li> <li>The process for distributed writes is more complex to coordinate.</li> </ul> <p>Warning</p> <p>Another downside of Icechunk in its current state is its immaturity. The library is very new, likely contains bugs, and is not recommended for production usage at this point in time.</p>"},{"location":"faq/#what-is-icechunks-relationship-to-zarr","title":"What is Icechunk's relationship to Zarr?","text":"<p>The Zarr format and protocol is agnostic to the underlying storage system (\"store\" in Zarr terminology) and communicates with the store via a simple key / value interface. Zarr tells the store which keys and values it wants to get or set, and it's the store's job to figure out how to persist or retrieve the required bytes.</p> <p>Most existing Zarr stores have a simple 1:1 mapping between Zarr's keys and the underlying file / object names. For example, if Zarr asks for a key call <code>myarray/c/0/0</code>, the store may just look up a key of the same name in an underlying cloud object storage bucket.</p> <p>Icechunk is a storage engine which creates a layer of indirection between the Zarr keys and the actual files in storage. A Zarr library doesn't have to know explicitly how Icechunk works or how it's storing data on disk. It just gets / sets keys as it would with any store. Icechunk figures out how to materialize these keys based on its storage schema.</p> <ul> <li> <p>Standard Zarr + Fsspec</p> <p>In standard Zarr usage (without Icechunk), fsspec sits between the Zarr library and the object store, translating Zarr keys directly to object store keys.</p> <pre><code>flowchart TD\n    zarr-python[Zarr Library] &lt;-- key / value--&gt; icechunk[fsspec]\n    icechunk &lt;-- key / value --&gt; storage[(Object Storage)]\n</code></pre> </li> <li> <p>Zarr + Icechunk</p> <p>With Icechunk, the Icechunk library intercepts the Zarr keys and translates them to the Icechunk schema, storing data in object storage using its own format.</p> <pre><code>flowchart TD\n    zarr-python[Zarr Library] &lt;-- key / value--&gt; icechunk[Icechunk Library]\n    icechunk &lt;-- icechunk data / metadata files --&gt; storage[(Object Storage)]\n</code></pre> </li> </ul> <p>Implementing Icechunk this way allows Icechunk's specification to evolve independently from Zarr's, maintaining interoperability while enabling rapid iteration and promoting innovation on the I/O layer.</p>"},{"location":"faq/#is-icechunk-part-of-the-zarr-spec","title":"Is Icechunk part of the Zarr Spec?","text":"<p>No. At the moment, the Icechunk spec is completely independent of the Zarr spec.</p> <p>In the future, we may choose to propose Icechunk as a Zarr extension. However, because it sits below Zarr in the stack, it's not immediately clear how to do that.</p>"},{"location":"faq/#should-i-implement-icechunk-on-my-own-based-on-the-spec","title":"Should I implement Icechunk on my own based on the spec?","text":"<p>No, we do not recommend implementing Icechunk independently of the existing Rust library. There are two reasons for this:</p> <ol> <li>The spec has not yet been stabilized and is still evolving rapidly.</li> <li>It's probably much easier to bind to the Rust library from your language of choice,    rather than re-implement from scratch.</li> </ol> <p>We welcome contributions from folks interested in developing Icechunk bindings for other languages!</p>"},{"location":"faq/#is-icechunk-stable","title":"Is Icechunk stable?","text":"<p>The Icechunk library is reasonably well-tested and performant. The Rust-based core library provides a solid foundation of correctness, safety, and speed.</p> <p>However, the actual on disk format is still evolving and may change from one alpha release to the next. Until Icechunk reaches v1.0, we can't commit to long-term stability of the on-disk format. This means Icechunk can't yet be used for production uses which require long-term persistence of data.</p> <p>\ud83d\ude05 Don't worry! We are working as fast as we can and aim to release v1.0 soon!</p>"},{"location":"faq/#is-icechunk-fast","title":"Is Icechunk fast?","text":"<p>We have not yet begun the process of optimizing Icechunk for performance. Our focus so far has been on correctness and delivering the features needed for full interoperability with Zarr and Xarray.</p> <p>However, preliminary investigations indicate that Icechunk is at least as fast as the existing Zarr / Dask / fsspec stack and in many cases achieves significantly lower latency and higher throughput. Furthermore, Icechunk achieves this without using Dask, by implementing its own asynchronous multithreaded I/O pipeline.</p>"},{"location":"faq/#how-does-icechunk-compare-to-x","title":"How does Icechunk compare to X?","text":""},{"location":"faq/#array-formats","title":"Array Formats","text":"<p>Array formats are file formats for storing multi-dimensional array (tensor) data. Icechunk is an array format. Here is how Icechunk compares to other popular array formats.</p>"},{"location":"faq/#hdf5","title":"HDF5","text":"<p>HDF5 (Hierarchical Data Format version 5) is a popular format for storing scientific data. HDF is widely used in high-performance computing.</p> <ul> <li> <p>Similarities</p> <p>Icechunk and HDF5 share the same data model: multidimensional arrays and metadata organized into a hierarchical tree structure. This data model can accommodate a wide range of different use cases and workflows.</p> <p>Both Icechunk and HDF5 use the concept of \"chunking\" to split large arrays into smaller storage units.</p> </li> <li> <p>Differences</p> <p>HDF5 is a monolithic file format designed first and foremost for POSIX filesystems. All of the chunks in an HDF5 dataset live within a single file. The size of an HDF5 dataset is limited to the size of a single file. HDF5 relies on the filesystem for consistency and is not designed for multiple concurrent yet uncoordinated readers and writers.</p> <p>Icechunk spreads chunks over many files and is designed first and foremost for cloud object storage. Icechunk can accommodate datasets of arbitrary size. Icechunk's optimistic concurrency design allows for safe concurrent access for uncoordinated readers and writers.</p> </li> </ul>"},{"location":"faq/#netcdf","title":"NetCDF","text":"<p>NetCDF (Network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.</p> <p>NetCDF4 uses HDF5 as its underlying file format. Therefore, the similarities and differences with Icechunk are fundamentally the same.</p> <p>Icechunk can accommodate the NetCDF data model. It's possible to write NetCDF compliant data in Icechunk using Xarray.</p>"},{"location":"faq/#zarr","title":"Zarr","text":"<p>Icechunk works together with Zarr. (See What is Icechunk's relationship to Zarr? for more detail.)</p> <p>Compared to regular Zarr (without Icechunk), Icechunk offers many benefits, including</p> <ul> <li>Serializable isolation of updates via transactions</li> <li>Data version control (snapshots, branches, tags)</li> <li>Ability to store references to chunks in external datasets (HDF5, NetCDF, GRIB, etc.)</li> <li>A Rust-optimized I/O layer for communicating with object storage</li> </ul>"},{"location":"faq/#cloud-optimized-geotiff-cog","title":"Cloud Optimized GeoTiff (CoG)","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing \u200bHTTP GET range requests to ask for just the parts of a file they need.</p> <p>CoG has become very popular in the geospatial community as a cloud-native format for raster data. A CoG file contains a single image (possibly with multiple bands), sharded into chunks of an appropriate size. A CoG also contains \"overviews,\" lower resolution versions of the same data. Finally, a CoG contains relevant geospatial metadata regarding projection, CRS, etc. which allow georeferencing of the data.</p> <p>Data identical to what is found in a CoG can be stored in the Zarr data model and therefore in an Icechunk repo. Furthermore, Zarr / Icechunk can accommodate rasters of arbitrarily large size and facilitate massive-scale concurrent writes (in addition to reads); A CoG, in contrast, is limited to a single file and thus has limitations on scale and write concurrency.</p> <p>However, Zarr and Icechunk currently do not offer the same level of broad geospatial interoperability that CoG does. The GeoZarr project aims to change that.</p>"},{"location":"faq/#tiledb-embedded","title":"TileDB Embedded","text":"<p>TileDB Embedded is an innovative array storage format that bears many similarities to both Zarr and Icechunk. Like TileDB Embedded, Icechunk aims to provide database-style features on top of the array data model. Both technologies use an embedded / serverless architecture, where client processes interact directly with data files in storage, rather than through a database server. However, there are a number of difference, enumerated below.</p> <p>The following table compares Zarr + Icechunk with TileDB Embedded in a few key areas</p> feature Zarr + Icechunk TileDB Embedded Comment atomicity atomic updates can span multiple arrays and groups array fragments limited to a single array Icechunk's model allows a writer to stage many updates across interrelated arrays into a single transaction. concurrency and isolation serializable isolation of transactions eventual consistency While both formats enable lock-free concurrent reading and writing, Icechunk can catch (and potentially reject) inconsistent, out-of order updates. versioning snapshots, branches, tags linear version history Icechunk's data versioning model is closer to Git's. unit of storage chunk tile (basically the same thing) minimum write chunk cell TileDB allows atomic updates to individual cells, while Zarr requires writing an entire chunk. sparse arrays Zarr + Icechunk do not currently support sparse arrays. virtual chunk references Icechunk enables references to chunks in other file formats (HDF5, NetCDF, GRIB, etc.), while TileDB does not. <p>Beyond this list, there are numerous differences in the design, file layout, and implementation of Icechunk and TileDB embedded which may lead to differences in suitability and performance for different workfloads.</p>"},{"location":"faq/#safetensors","title":"SafeTensors","text":"<p>SafeTensors is a format developed by HuggingFace for storing tensors (arrays) safely, in contrast to Python pickle objects.</p> <p>By the same criteria Icechunk and Zarr are also \"safe\", in that it is impossible to trigger arbitrary code execution when reading data.</p> <p>SafeTensors is a single-file format, like HDF5, SafeTensors optimizes for a simple on-disk layout that facilitates mem-map-based zero-copy reading in ML training pipelines, assuming that the data are being read from a local POSIX filesystem Zarr and Icechunk instead allow for flexible chunking and compression to optimize I/O against object storage.</p>"},{"location":"faq/#tabular-formats","title":"Tabular Formats","text":"<p>Tabular formats are for storing tabular data. Tabular formats are extremely prevalent in general-purpose data analytics but are less widely used in scientific domains. The tabular data model is different from Icechunk's multidimensional array data model, and so a direct comparison is not always apt. However, Icechunk is inspired by many tabular file formats, and there are some notable similarities.</p>"},{"location":"faq/#apache-parquet","title":"Apache Parquet","text":"<p>Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming language and analytics tools.</p> <p>Parquet employs many of the same core technological concepts used in Zarr + Icechunk such as chunking, compression, and efficient metadata access in a cloud context. Both formats support a range of different numerical data types. Both are \"columnar\" in the sense that different columns / variables / arrays can be queried efficiently without having to fetch unwanted data from other columns. Both also support attaching arbitrary key-value metadata to variables. Parquet supports \"nested\" types like variable-length lists, dicts, etc. that are currently unsupported in Zarr (but may be possible in the future).</p> <p>In general, Parquet and other tabular formats can't be substituted for Zarr / Icechunk, due to the lack of multidimensional array support. On the other hand, tabular data can be modeled in Zarr / Icechunk in a relatively straightforward way: each column as a 1D array, and a table / dataframe as a group of same-sized 1D arrays.</p>"},{"location":"faq/#apache-iceberg","title":"Apache Iceberg","text":"<p>Iceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time.</p> <p>Iceberg is commonly used to manage many Parquet files as a single table in object storage.</p> <p>Iceberg was influential in the design of Icechunk. Many of the spec core requirements are similar to Iceberg. Specifically, both formats share the following properties:</p> <ul> <li>Files written to object storage immutably</li> <li>All data and metadata files are tracked explicitly by manifests</li> <li>Similar mechanism for staging snapshots and committing transactions</li> <li>Support for branches and tags</li> </ul> <p>However, unlike Iceberg, Icechunk does not require an external catalog to commit transactions; it relies solely on the consistency of the object store.</p>"},{"location":"faq/#delta-lake","title":"Delta Lake","text":"<p>Delta is another popular table format based on a log of updates to the table state. Its functionality and design is quite similar to Iceberg, as is its comparison to Icechunk.</p>"},{"location":"faq/#lance","title":"Lance","text":"<p>Lance is a modern columnar data format that is optimized for ML workflows and datasets.</p> <p>Despite its focus on multimodal data, as a columnar format, Lance can't accommodate large arrays / tensors chunked over arbitrary dimensions, making it fundamentally different from Icechunk.</p> <p>However, the modern design of Lance was very influential on Icechunk. Icechunk's commit and conflict resolution mechanism is partly inspired by Lance.</p>"},{"location":"faq/#other-related-projects","title":"Other Related projects","text":""},{"location":"faq/#xarray","title":"Xarray","text":"<p>Xarray is an open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, which allows for more intuitive, more concise, and less error-prone user experience.</p> <p>Xarray includes a large and growing library of domain-agnostic functions for advanced analytics and visualization with these data structures.</p> <p>Xarray and Zarr / Icechunk work great together! Xarray is the recommended way to read and write Icechunk data for Python users in geospatial, weather, climate, and similar domains.</p>"},{"location":"faq/#kerchunk","title":"Kerchunk","text":"<p>Kerchunk is a library that provides a unified way to represent a variety of chunked, compressed data formats (e.g. NetCDF/HDF5, GRIB2, TIFF, \u2026), allowing efficient access to the data from traditional file systems or cloud object storage. It also provides a flexible way to create virtual datasets from multiple files. It does this by extracting the byte ranges, compression information and other information about the data and storing this metadata in a new, separate object. This means that you can create a virtual aggregate dataset over potentially many source files, for efficient, parallel and cloud-friendly in-situ access without having to copy or translate the originals. It is a gateway to in-the-cloud massive data processing while the data providers still insist on using legacy formats for archival storage</p> <p>Kerchunk emerged from the Pangeo community as an experimental way of reading archival files, allowing those files to be accessed \"virtually\" using the Zarr protocol. Kerchunk pioneered the concept of a \"chunk manifest\", a file containing references to compressed binary chunks in other files in the form of the tuple <code>(uri, offset, size)</code>. Kerchunk has experimented with different ways of serializing chunk manifests, including JSON and Parquet.</p> <p>Icechunk provides a highly efficient and scalable mechanism for storing and tracking the references generated by Kerchunk. Kerchunk and Icechunk are highly complimentary.</p>"},{"location":"faq/#virtualizarr","title":"VirtualiZarr","text":"<p>VirtualiZarr creates virtual Zarr stores for cloud-friendly access to archival data, using familiar Xarray syntax.</p> <p>VirtualiZarr provides another way of generating and manipulating Kerchunk-style references. VirtualiZarr first uses Kerchunk to generate virtual references, but then provides a simple Xarray-based interface for manipulating those references. As VirtualiZarr can also write virtual references into an Icechunk Store directly, together they form a complete pipeline for generating and storing references to multiple pre-existing files.</p>"},{"location":"faq/#lakefs","title":"LakeFS","text":"<p>LakeFS is a solution git-style version control on top of cloud object storage. LakeFS enables git-style commits, tags, and branches representing the state of an entire object storage bucket.</p> <p>LakeFS is format agnostic and can accommodate any type of data, including Zarr. LakeFS can therefore be used to create a versioned Zarr store, similar to Icechunk.</p> <p>Icechunk, however, is designed specifically for array data, based on the Zarr data model. This specialization enables numerous optimizations and user-experience enhancements not possible with LakeFS.</p> <p>LakeFS also requires a server to operate. Icechunk, in contrast, works with just object storage.</p>"},{"location":"faq/#tensorstore","title":"TensorStore","text":"<p>TensorStore is a library for efficiently reading and writing large multi-dimensional arrays.</p> <p>TensorStore can read and write a variety of different array formats, including Zarr.</p> <p>While TensorStore is not yet compatible with Icechunk, it should be possible to implement Icechunk support in TensorStore.</p> <p>TensorStore implements an ocdbt:</p> <p>The ocdbt driver implements an Optionally-Cooperative Distributed B+Tree (OCDBT) on top of a base key-value store.</p> <p>Ocdbt implements a transactional, versioned key-value store suitable for storing Zarr data, thereby supporting some of the same features as Icechunk. Unlike Icechunk, the ocdbt key-value store is not specialized to Zarr, does not differentiate between chunk or metadata keys, and does not store any metadata about chunks.</p>"},{"location":"icechunk-rust/","title":"Rust","text":"<p>Home / icechunk-rust</p>"},{"location":"icechunk-rust/#icechunk-rust","title":"Icechunk Rust","text":"<p>The Icechunk rust library is used internally by Icechunk Python. It is currently not designed to be used in standalone form.</p> <ul> <li>Icechunk Rust Documentation at docs.rs</li> </ul> <p>We welcome contributors interested in implementing more Rust functionality! In particular, we would love to integrate Icechunk with zarrs, a new Zarr Rust library.</p>"},{"location":"overview/","title":"Overview","text":"<p>Home / overview</p>"},{"location":"overview/#icechunk","title":"Icechunk","text":"<p>Icechunk is an open-source (Apache 2.0), transactional storage engine for tensor / ND-array data designed for use on cloud object storage. Icechunk works together with Zarr, augmenting the Zarr core data model with features that enhance performance, collaboration, and safety in a cloud-computing context.</p>"},{"location":"overview/#docs-organization","title":"Docs Organization","text":"<p>This is the Icechunk documentation. It's organized into the following parts.</p> <ul> <li>This page: a general overview of the project's goals and components.</li> <li>Frequently Asked Questions</li> <li>Documentation for Icechunk Python, the main user-facing   library</li> <li>Documentation for the Icechunk Rust Crate</li> <li>The Icechunk Spec</li> </ul>"},{"location":"overview/#icechunk-overview","title":"Icechunk Overview","text":"<p>Let's break down what \"transactional storage engine for Zarr\" actually means:</p> <ul> <li>Zarr is an open source specification for the storage of multidimensional array (a.k.a. tensor) data.   Zarr defines the metadata for describing arrays (shape, dtype, etc.) and the way these arrays are chunked, compressed, and converted to raw bytes for storage. Zarr can store its data in any key-value store.   There are many different implementations of Zarr in different languages. Right now, Icechunk only supports   Zarr Python.   If you're interested in implementing Icehcunk support, please open an issue so we can help you.</li> <li>Storage engine - Icechunk exposes a key-value interface to Zarr and manages all of the actual I/O for getting, setting, and updating both metadata and chunk data in cloud object storage.   Zarr libraries don't have to know exactly how icechunk works under the hood in order to use it.</li> <li>Transactional - The key improvement that Icechunk brings on top of regular Zarr is to provide consistent serializable isolation between transactions.   This means that Icechunk data are safe to read and write in parallel from multiple uncoordinated processes.   This allows Zarr to be used more like a database.</li> </ul> <p>The core entity in Icechunk is a repository or repo. A repo is defined as a Zarr hierarchy containing one or more Arrays and Groups, and a repo functions as self-contained Zarr Store. The most common scenario is for an Icechunk repo to contain a single Zarr group with multiple arrays, each corresponding to different physical variables but sharing common spatiotemporal coordinates. However, formally a repo can be any valid Zarr hierarchy, from a single Array to a deeply nested structure of Groups and Arrays. Users of Icechunk should aim to scope their repos only to related arrays and groups that require consistent transactional updates.</p> <p>Icechunk supports the following core requirements:</p> <ol> <li>Object storage - the format is designed around the consistency features and performance characteristics available in modern cloud object storage. No external database or catalog is required to maintain a repo. (It also works with file storage.)</li> <li>Serializable isolation - Reads are isolated from concurrent writes and always use a committed snapshot of a repo. Writes are committed atomically and are never partially visible. No locks are required for reading.</li> <li>Time travel - Previous snapshots of a repo remain accessible after new ones have been written.</li> <li>Data version control - Repos support both tags (immutable references to snapshots) and branches (mutable references to snapshots).</li> <li>Chunk shardings - Chunk storage is decoupled from specific file names. Multiple chunks can be packed into a single object (sharding).</li> <li>Chunk references - Zarr-compatible chunks within other file formats (e.g. HDF5, NetCDF) can be referenced.</li> <li>Schema evolution - Arrays and Groups can be added, renamed, and removed from the hierarchy with minimal overhead.</li> </ol>"},{"location":"overview/#key-concepts","title":"Key Concepts","text":""},{"location":"overview/#groups-arrays-and-chunks","title":"Groups, Arrays, and Chunks","text":"<p>Icechunk is designed around the Zarr data model, widely used in scientific computing, data science, and AI / ML. (The Zarr high-level data model is effectively the same as HDF5.) The core data structure in this data model is the array. Arrays have two fundamental properties:</p> <ul> <li>shape - a tuple of integers which specify the dimensions of each axis of the array. A 10 x 10 square array would have shape (10, 10)</li> <li>data type - a specification of what type of data is found in each element, e.g. integer, float, etc. Different data types have different precision (e.g. 16-bit integer, 64-bit float, etc.)</li> </ul> <p>In Zarr / Icechunk, arrays are split into chunks, A chunk is the minimum unit of data that must be read / written from storage, and thus choices about chunking have strong implications for performance. Zarr leaves this completely up to the user. Chunk shape should be chosen based on the anticipated data access pattern for each array An Icechunk array is not bounded by an individual file and is effectively unlimited in size.</p> <p>For further organization of data, Icechunk supports groups within a single repo. Group are like folders which contain multiple arrays and or other groups. Groups enable data to be organized into hierarchical trees. A common usage pattern is to store multiple arrays in a group representing a NetCDF-style dataset.</p> <p>Arbitrary JSON-style key-value metadata can be attached to both arrays and groups.</p>"},{"location":"overview/#snapshots","title":"Snapshots","text":"<p>Every update to an Icechunk store creates a new snapshot with a unique ID. Icechunk users must organize their updates into groups of related operations called transactions. For example, appending a new time slice to multiple arrays should be done as a single transaction, comprising the following steps 1. Update the array metadata to resize the array to accommodate the new elements. 2. Write new chunks for each array in the group.</p> <p>While the transaction is in progress, none of these changes will be visible to other users of the store. Once the transaction is committed, a new snapshot is generated. Readers can only see and use committed snapshots.</p>"},{"location":"overview/#branches-and-tags","title":"Branches and Tags","text":"<p>Additionally, snapshots occur in a specific linear (i.e. serializable) order within  branch. A branch is a mutable reference to a snapshot--a pointer that maps the branch name to a snapshot ID. The default branch is <code>main</code>. Every commit to the main branch updates this reference. Icechunk's design protects against the race condition in which two uncoordinated sessions attempt to update the branch at the same time; only one can succeed.</p> <p>Icechunk also defines tags--immutable references to snapshot. Tags are appropriate for publishing specific releases of a repository or for any application which requires a persistent, immutable identifier to the store state.</p>"},{"location":"overview/#chunk-references","title":"Chunk References","text":"<p>Chunk references are \"pointers\" to chunks that exist in other files--HDF5, NetCDF, GRIB, etc. Icechunk can store these references alongside native Zarr chunks as \"virtual datasets\". You can then can update these virtual datasets incrementally (overwrite chunks, change metadata, etc.) without touching the underlying files.</p>"},{"location":"overview/#how-does-it-work","title":"How Does It Work?","text":"<p>Note</p> <p>For more detailed explanation, have a look at the Icechunk spec</p> <p>Zarr itself works by storing both metadata and chunk data into a abstract store according to a specified system of \"keys\". For example, a 2D Zarr array called <code>myarray</code>, within a group called <code>mygroup</code>, would generate the following keys:</p> <pre><code>mygroup/zarr.json\nmygroup/myarray/zarr.json\nmygroup/myarray/c/0/0\nmygroup/myarray/c/0/1\n</code></pre> <p>In standard regular Zarr stores, these key map directly to filenames in a filesystem or object keys in an object storage system. When writing data, a Zarr implementation will create these keys and populate them with data. When modifying existing arrays or groups, a Zarr implementation will potentially overwrite existing keys with new data.</p> <p>This is generally not a problem, as long there is only one person or process coordinating access to the data. However, when multiple uncoordinated readers and writers attempt to access the same Zarr data at the same time, various consistency problems problems emerge. These consistency problems can occur in both file storage and object storage; they are particularly severe in a cloud setting where Zarr is being used as an active store for data that are frequently changed while also being read.</p> <p>With Icechunk, we keep the same core Zarr data model, but add a layer of indirection between the Zarr keys and the on-disk storage. The Icechunk library translates between the Zarr keys and the actual on-disk data given the particular context of the user's state. Icechunk defines a series of interconnected metadata and data files that together enable efficient isolated reading and writing of metadata and chunks. Once written, these files are immutable. Icechunk keeps track of every single chunk explicitly in a \"chunk manifest\".</p> <pre><code>flowchart TD\n    zarr-python[Zarr Library] &lt;-- key / value--&gt; icechunk[Icechunk Library]\n    icechunk &lt;-- data / metadata files --&gt; storage[(Object Storage)]\n</code></pre>"},{"location":"sample-datasets/","title":"Sample Datasets","text":"<p>Home / sample-datasets</p>"},{"location":"sample-datasets/#sample-datasets","title":"Sample Datasets","text":"<p>Warning</p> <p>This page is under construction. The listed datasets are outdated and will not work until the icechunk format is more stable.</p>"},{"location":"sample-datasets/#native-datasets","title":"Native Datasets","text":""},{"location":"sample-datasets/#virtual-datasets","title":"Virtual Datasets","text":""},{"location":"sample-datasets/#noaa-oisst-data","title":"NOAA OISST Data","text":"<p>The NOAA 1/4\u00b0 Daily Optimum Interpolation Sea Surface Temperature (OISST) is a long term Climate Data Record that incorporates observations from different platforms (satellites, ships, buoys and Argo floats) into a regular global grid</p> <p>Check out an example dataset built using all virtual references pointing to daily Sea Surface Temperature data from 2020 to 2024 on NOAA's S3 bucket using python:</p> <pre><code>import icechunk as ic\n\nstorage = ic.s3_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n    anonymous=True,\n)\n\nvirtual_credentials = ic.containers_credentials({\"s3\": ic.s3_credentials(anonymous=True)})\n\nrepo = ic.Repository.open(\n        storage=storage,\n        virtual_chunk_credentials=virtual_credentials)\n</code></pre> <p></p>"},{"location":"spec/","title":"Specification","text":"<p>Home / spec</p>"},{"location":"spec/#icechunk-specification","title":"Icechunk Specification","text":"<p>!!! Note:     The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p>"},{"location":"spec/#introduction","title":"Introduction","text":"<p>The Icechunk specification is a storage specification for Zarr data. Icechunk is inspired by Apache Iceberg and borrows many concepts and ideas from the Iceberg Spec.</p> <p>This specification describes a single Icechunk repository. A repository is defined as a Zarr store containing one or more Arrays and Groups. The most common scenario is for a repository to contain a single Zarr group with multiple arrays, each corresponding to different physical variables but sharing common spatiotemporal coordinates. However, formally a repository can be any valid Zarr hierarchy, from a single Array to a deeply nested structure of Groups and Arrays. Users of Icechunk should aim to scope their repository only to related arrays and groups that require consistent transactional updates.</p> <p>Icechunk defines a series of interconnected metadata and data files that together comprise the format. All the data and metadata for a repository are stored in a directory in object storage or file storage.</p>"},{"location":"spec/#goals","title":"Goals","text":"<p>The goals of the specification are as follows:</p> <ol> <li>Object storage - the format is designed around the consistency features and performance characteristics available in modern cloud object storage. No external database or catalog is required.</li> <li>Serializable isolation - Reads will be isolated from concurrent writes and always use a committed snapshot of a repository. Writes to repositories will be committed atomically and will not be partially visible. Readers will not acquire locks.</li> <li>Time travel - Previous snapshots of a repository remain accessible after new ones have been written.</li> <li>Chunk sharding and references - Chunk storage is decoupled from specific file names. Multiple chunks can be packed into a single object (sharding). Zarr-compatible chunks within other file formats (e.g. HDF5, NetCDF) can be referenced.</li> <li>Schema Evolution - Arrays and Groups can be added, renamed, and removed from the hierarchy with minimal overhead.</li> </ol>"},{"location":"spec/#non-goals","title":"Non Goals","text":"<ol> <li>Low Latency - Icechunk is designed to support analytical workloads for large repositories. We accept that the extra layers of metadata files and indirection will introduce additional cold-start latency compared to regular Zarr.</li> <li>No Catalog - The spec does not extend beyond a single repository or provide a way to organize multiple repositories into a hierarchy.</li> <li>Access Controls - Access control is the responsibility of the storage medium. The spec is not designed to enable fine-grained access restrictions (e.g. only read specific arrays) within a single repository.</li> </ol>"},{"location":"spec/#storage-operations","title":"Storage Operations","text":"<p>Icechunk requires that the storage system support the following operations:</p> <ul> <li>In-place write - Strong read-after-write and list-after-write consistency is expected. Files are not moved or altered once they are written.</li> <li>Conditional write if-not-exists - For the commit process to be safe and consistent, the storage system must guard against two files of the same name being created at the same time.</li> <li>Seekable reads - Chunk file formats may require seek support (e.g. shards).</li> <li>Deletes - Delete files that are no longer used (via a garbage-collection operation).</li> <li>Sorted List - The storage system must allow the listing of directories / prefixes in a consistent sorted order.</li> </ul> <p>These requirements are compatible with object stores, like S3, as well as with filesystems.</p> <p>The storage system is not required to support random-access writes. Once written, chunk and metadata files are immutable until they are deleted.</p>"},{"location":"spec/#specification","title":"Specification","text":""},{"location":"spec/#overview","title":"Overview","text":"<p>Icechunk uses a series of linked metadata files to describe the state of the repository.</p> <ul> <li>The Snapshot file records all of the different arrays and groups in the repository, plus their metadata. Every new commit creates a new snapshot file. The snapshot file contains pointers to one or more chunk manifest files and [optionally] attribute files.</li> <li>Chunk manifests store references to individual chunks. A single manifest may store references for multiple arrays or a subset of all the references for a single array.</li> <li>Attributes files provide a way to store additional user-defined attributes for arrays and groups outside of the structure file. This is important if attributes are very large, otherwise, they will be stored inline in the snapshot file.</li> <li>Chunk files store the actual compressed chunk data, potentially containing data for multiple chunks in a single file.</li> <li>Reference files track the state of branches and tags, containing a lightweight pointer to a snapshot file. Transactions on a branch are committed by creating the next branch file in a sequence.</li> </ul> <p>When reading from object store, the client opens the latest branch or tag file to obtain a pointer to the relevant snapshot file. The client then reads the snapshot file to determine the structure and hierarchy of the repository. When fetching data from an array, the client first examines the chunk manifest file[s] for that array and finally fetches the chunks referenced therein.</p> <p>When writing a new repository snapshot, the client first writes a new set of chunks and chunk manifests, and then generates a new snapshot file. Finally, in an atomic put-if-not-exists operation, to commit the transaction, it creates the next branch file in the sequence. This operation may fail if a different client has already committed the next snapshot. In this case, the client may attempt to resolve the conflicts and retry the commit.</p> <pre><code>flowchart TD\n    subgraph metadata[Metadata]\n    subgraph reference_files[Reference Files]\n    old_branch[Main Branch File 001]\n    branch[Main Branch File 002]\n    end\n    subgraph snapshots[Snapshots]\n    snapshot1[Snapshot File 1]\n    snapshot2[Snapshot File 2]\n    end\n    subgraph attributes[Attributes]\n    attrs[Attribute File]\n    end\n    subgraph manifests[Manifests]\n    manifestA[Chunk Manifest A]\n    manifestB[Chunk Manifest B]\n    end\n    end\n    subgraph data\n    chunk1[Chunk File 1]\n    chunk2[Chunk File 2]\n    chunk3[Chunk File 3]\n    chunk4[Chunk File 4]\n    end\n\n    branch -- snapshot ID --&gt; snapshot2\n    snapshot1 --&gt; attrs\n    snapshot1 --&gt; manifestA\n    snapshot2 --&gt; attrs\n    snapshot2 --&gt;manifestA\n    snapshot2 --&gt;manifestB\n    manifestA --&gt; chunk1\n    manifestA --&gt; chunk2\n    manifestB --&gt; chunk3\n    manifestB --&gt; chunk4\n\n</code></pre>"},{"location":"spec/#file-layout","title":"File Layout","text":"<p>All data and metadata files are stored within a root directory (typically a prefix within an object store) using the following directory structure.</p> <ul> <li><code>$ROOT</code> base URI (s3, gcs, local directory, etc.)</li> <li><code>$ROOT/refs/</code> reference files</li> <li><code>$ROOT/snapshots/</code> snapshot files</li> <li><code>$ROOT/attributes/</code> attribute files</li> <li><code>$ROOT/manifests/</code> chunk manifests</li> <li><code>$ROOT/chunks/</code> chunks</li> </ul>"},{"location":"spec/#file-formats","title":"File Formats","text":"<p>Warning</p> <p>The actual file formats used for each type of metadata file are in flux. The spec currently describes the data structures encoded in these files, rather than a specific file format.</p>"},{"location":"spec/#reference-files","title":"Reference Files","text":"<p>Similar to Git, Icechunk supports the concept of branches and tags. These references point to a specific snapshot of the repository.</p> <ul> <li>Branches are mutable references to a snapshot.   Repositories may have one or more branches.   The default branch name is <code>main</code>.   Repositories must always have a <code>main</code> branch, which is used to detect the existence of a valid repository in a given path.   After creation, branches may be updated to point to a different snapshot.</li> <li>Tags are immutable references to a snapshot.   A repository may contain zero or more tags.   After creation, tags may never be updated, unlike in Git.</li> </ul> <p>References are very important in the Icechunk design. Creating or updating references is the point at which consistency and atomicity of Icechunk transactions is enforced. Different client sessions may simultaneously create two inconsistent snapshots; however, only one session may successfully update a reference to point it to its snapshot.</p> <p>References (both branches and tags) are stored as JSON files, the content is a JSON object with:</p> <ul> <li>keys: a single key <code>\"snapshot\"</code>,</li> <li>value: a string representation of the snapshot id, using Base 32 Crockford encoding. The snapshot id is 12 byte random binary, so the encoded string has 20 characters.</li> </ul> <p>Here is an example of a JSON file corresponding to a tag or branch:</p> <pre><code>{\"snapshot\":\"VY76P925PRY57WFEK410\"}\n</code></pre>"},{"location":"spec/#creating-and-updating-branches","title":"Creating and Updating Branches","text":"<p>The process of creating and updating branches is designed to use the limited consistency guarantees offered by object storage to ensure transactional consistency. When a client checks out a branch, it obtains a specific snapshot ID and uses this snapshot as the basis for any changes it creates during its session. The client creates a new snapshot and then updates the branch reference to point to the new snapshot (a \"commit\"). However, when updating the branch reference, the client must detect whether a different session has updated the branch reference in the interim, possibly retrying or failing the commit if so. This is an \"optimistic concurrency\" strategy; the resolution mechanism can be expensive, but conflicts are expected to be infrequent.</p> <p>All popular object stores support a \"create if not exists\" operation. In other words, object stores can guard against the race condition which occurs when two sessions attempt to create the same file at the same time. This motivates the design of Icechunk's branch file naming convention.</p> <p>Each commit to an Icechunk branch augments a counter called the sequence number. The first commit creates sequence number 0. The next commit creates sequence number 1. Etc. This sequence number is encoded into the branch reference file name.</p> <p>When a client checks out a branch, it keeps track of its current sequence number N. When it tries to commit, it attempts to create the file corresponding to sequence number N + 1 in an atomic \"create if not exists\" operation. If this succeeds, the commit is successful. If this fails (because another client created that file already), the commit fails. At this point, the client may choose to retry its commit (possibly re-reading the updated data) and then create sequence number N + 2.</p> <p>Branch references are stored in the <code>refs/</code> directory within a subdirectory corresponding to the branch name prepended by the string <code>branch.</code>: <code>refs/branch.$BRANCH_NAME/</code>. Branch names may not contain the <code>/</code> character.</p> <p>To facilitate easy lookups of the latest branch reference, we use the following encoding for the sequence number: - subtract the sequence number from the integer <code>1099511627775</code> - encode the resulting integer as a string using Base 32 Crockford - left-padding the string with 0s to a length of 8 characters This produces a deterministic sequence of branch file names in which the latest sequence always appears first when sorted lexicographically, facilitating easy lookup by listing the object store.</p> <p>The full branch file name is then given by <code>refs/branch.$BRANCH_NAME/$ENCODED_SEQUENCE.json</code>.</p> <p>For example, the first main branch file is in a store, corresponding with sequence number 0, is always named <code>refs/branch.main/ZZZZZZZZ.json</code>. The branch file for sequence number 100 is <code>refs/branch.main/ZZZZZZWV.json</code>. The maximum number of commits allowed in an Icechunk repository is consequently <code>1099511627775</code>, corresponding to the state file <code>refs/branch.main/00000000.json</code>.</p>"},{"location":"spec/#tags","title":"Tags","text":"<p>Since tags are immutable, they are simpler than branches.</p> <p>Tag files follow the pattern <code>refs/tag.$TAG_NAME/ref.json</code>.</p> <p>Tag names may not contain the <code>/</code> character.</p> <p>When creating a new tag, the client attempts to create the tag file using a \"create if not exists\" operation. If successful, the tag is created successful. If not, that means another client has already created that tag.</p> <p>Tags cannot be deleted once created.</p>"},{"location":"spec/#snapshot-files","title":"Snapshot Files","text":"<p>The snapshot file fully describes the schema of the repository, including all arrays and groups.</p> <p>The snapshot file is currently encoded using MessagePack, but this may change before Icechunk version 1.0. Given the alpha status of this spec, the best way to understand the information stored in the snapshot file is through the data structure used internally by the Icechunk library for serialization. This data structure will most certainly change before the spec stabilization:</p> <pre><code>pub struct Snapshot {\n    pub icechunk_snapshot_format_version: IcechunkFormatVersion,\n    pub icechunk_snapshot_format_flags: BTreeMap&lt;String, rmpv::Value&gt;,\n\n    pub manifest_files: Vec&lt;ManifestFileInfo&gt;,\n    pub attribute_files: Vec&lt;AttributeFileInfo&gt;,\n\n    pub total_parents: u32,\n    pub short_term_parents: u16,\n    pub short_term_history: VecDeque&lt;SnapshotMetadata&gt;,\n\n    pub metadata: SnapshotMetadata,\n    pub started_at: DateTime&lt;Utc&gt;,\n    pub properties: SnapshotProperties,\n    nodes: BTreeMap&lt;Path, NodeSnapshot&gt;,\n}\n</code></pre> <p>To get full details on what each field contains, please refer to the Icechunk library code.</p>"},{"location":"spec/#attributes-files","title":"Attributes Files","text":"<p>Attribute files hold user-defined attributes separately from the snapshot file.</p> <p>Warning</p> <p>Attribute files have not been implemented.</p> <p>The on-disk format for attribute files has not been defined yet, but it will probably be a MessagePack serialization of the attributes map.</p>"},{"location":"spec/#chunk-manifest-files","title":"Chunk Manifest Files","text":"<p>A chunk manifest file stores chunk references. Chunk references from multiple arrays can be stored in the same chunk manifest. The chunks from a single array can also be spread across multiple manifests.</p> <p>Manifest files are currently encoded using MessagePack, but this may change before Icechunk version 1.0. Given the alpha status of this spec, the best way to understand the information stored in the snapshot file is through the data structure used internally by the Icechunk library. This data structure will most certainly change before the spec stabilization:</p> <pre><code>pub struct Manifest {\n    pub icechunk_manifest_format_version: IcechunkFormatVersion,\n    pub icechunk_manifest_format_flags: BTreeMap&lt;String, rmpv::Value&gt;,\n    chunks: BTreeMap&lt;(NodeId, ChunkIndices), ChunkPayload&gt;,\n}\n\npub enum ChunkPayload {\n    Inline(Bytes),\n    Virtual(VirtualChunkRef),\n    Ref(ChunkRef),\n}\n</code></pre> <p>The most important part to understand from the data structure is the fact that manifests can hold three types of references:</p> <ul> <li>Native (<code>Ref</code>), pointing to the id of a chunk within the Icechunk repository.</li> <li>Inline (<code>Inline</code>), an optimization for very small chunks that can be embedded directly in the manifest. Mostly used for coordinate arrays.</li> <li>Virtual (<code>Virtual</code>), pointing to a region of a file outside of the Icechunk repository, for example,   a chunk that is inside a NetCDF file in object store</li> </ul> <p>To get full details on what each field contains, please refer to the Icechunk library code.</p>"},{"location":"spec/#chunk-files","title":"Chunk Files","text":"<p>Chunk files contain the compressed binary chunks of a Zarr array. Icechunk permits quite a bit of flexibility about how chunks are stored. Chunk files can be:</p> <ul> <li>One chunk per chunk file (i.e. standard Zarr)</li> <li>Multiple contiguous chunks from the same array in a single chunk file (similar to Zarr V3 shards)</li> <li>Chunks from multiple different arrays in the same file</li> <li>Other file types (e.g. NetCDF, HDF5) which contain Zarr-compatible chunks</li> </ul> <p>Applications may choose to arrange chunks within files in different ways to optimize I/O patterns.</p>"},{"location":"spec/#algorithms","title":"Algorithms","text":""},{"location":"spec/#initialize-new-repository","title":"Initialize New Repository","text":"<p>A new repository is initialized by creating a new [possibly empty] snapshot file and then creating the first file in the main branch sequence.</p> <p>If another client attempts to initialize a repository in the same location, only one can succeed.</p>"},{"location":"spec/#read-from-repository","title":"Read from Repository","text":""},{"location":"spec/#from-snapshot-id","title":"From Snapshot ID","text":"<p>If the specific snapshot ID is known, a client can open it directly in read only mode.</p> <ol> <li>Use the specified snapshot ID to fetch the snapshot file.</li> <li>Fetch desired attributes and values from arrays.</li> </ol>"},{"location":"spec/#from-branch","title":"From Branch","text":"<p>Usually, a client will want to read from the latest branch (e.g. <code>main</code>).</p> <ol> <li>List the object store prefix <code>refs/branch.$BRANCH_NAME/</code> to obtain the latest branch file in the sequence. Due to the encoding of the sequence number, this should be the first file in lexicographical order.</li> <li>Read the branch file JSON contents to obtain the snapshot ID.</li> <li>Use the snapshot ID to fetch the snapshot file.</li> <li>Fetch desired attributes and values from arrays.</li> </ol>"},{"location":"spec/#from-tag","title":"From Tag","text":"<ol> <li>Read the tag file found at <code>refs/tag.$TAG_NAME/ref.json</code> to obtain the snapshot ID.</li> <li>Use the snapshot ID to fetch the snapshot file.</li> <li>Fetch desired attributes and values from arrays.</li> </ol>"},{"location":"spec/#write-new-snapshot","title":"Write New Snapshot","text":"<ol> <li>Open a repository at a specific branch as described above, keeping track of the sequence number and branch name in the session context.</li> <li>[optional] Write new chunk files.</li> <li>[optional] Write new chunk manifests.</li> <li>Write a new snapshot file.</li> <li>Attempt to write the next branch file in the sequence<ol> <li>If successful, the commit succeeded and the branch is updated.</li> <li>If unsuccessful, attempt to reconcile and retry the commit.</li> </ol> </li> </ol>"},{"location":"spec/#create-new-tag","title":"Create New Tag","text":"<p>A tag can be created from any snapshot.</p> <ol> <li>Open the repository at a specific snapshot.</li> <li>Attempt to create the tag file.    a. If successful, the tag was created.    b. If unsuccessful, the tag already exists.</li> </ol>"},{"location":"icechunk-python/","title":"Index","text":"<p>Home / icechunk-python</p>"},{"location":"icechunk-python/#index-of-icechunk-python","title":"Index of icechunk-python","text":"<ul> <li>quickstart</li> <li>configuration</li> <li>version control</li> <li>xarray</li> <li>concurrency</li> <li>dask</li> <li>virtual datasets</li> <li>API reference</li> </ul>"},{"location":"icechunk-python/concurrency/","title":"Concurrency","text":"<p>Home / icechunk-python / concurrency</p>"},{"location":"icechunk-python/concurrency/#concurrency","title":"Concurrency","text":"<p>TODO: describe the general approach to concurrency in Icechunk</p>"},{"location":"icechunk-python/concurrency/#built-in-concurrency","title":"Built-in concurrency","text":"<p>Describe the multi-threading and async concurrency in Icechunk / Zarr</p>"},{"location":"icechunk-python/concurrency/#distributed-concurrency-within-a-single-transaction","title":"Distributed concurrency within a single transaction","text":"<p>\"Cooperative\" concurrency</p>"},{"location":"icechunk-python/concurrency/#concurrency-across-uncoordinated-sessions","title":"Concurrency across uncoordinated sessions","text":""},{"location":"icechunk-python/concurrency/#conflict-detection","title":"Conflict detection","text":""},{"location":"icechunk-python/configuration/","title":"Configuration","text":"<p>Home / icechunk-python / configuration</p>"},{"location":"icechunk-python/configuration/#configuration","title":"Configuration","text":"<p>When creating and opening Icechunk repositories, there are a two different sets of configuration to be aware of:</p> <ul> <li><code>Storage</code> - for configuring access to the object store or filesystem</li> <li><code>RepositoryConfig</code> - for configuring the behavior of the Icechunk Repository itself</li> </ul>"},{"location":"icechunk-python/configuration/#storage","title":"Storage","text":"<p>Icechunk can be configured to work with both object storage and filesystem backends. The storage configuration defines the location of an Icechunk store, along with any options or information needed to access data from a given storage type.</p>"},{"location":"icechunk-python/configuration/#s3-storage","title":"S3 Storage","text":"<p>When using Icechunk with s3 compatible storage systems, credentials must be provided to allow access to the data on the given endpoint. Icechunk allows for creating the storage config for s3 in three ways:</p> From environmentProvide credentialsAnonymous <p>With this option, the credentials for connecting to S3 are detected automatically from your environment. This is usually the best choice if you are connecting from within an AWS environment (e.g. from EC2). See the API</p> <pre><code>icechunk.s3_storage(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\",\n    from_env=True\n)\n</code></pre> <p>With this option, you provide your credentials and other details explicitly. See the API</p> <pre><code>icechunk.s3_storage(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\",\n    region='us-east-1',\n    access_key_id='my-access-key',\n    secret_access_key='my-secret-key',\n    # session token is optional\n    session_token='my-token',\n    endpoint_url=None,  # if using a custom endpoint\n    allow_http=False,  # allow http connections (default is False)\n)\n</code></pre> <p>With this option, you connect to S3 anonymously (without credentials). This is suitable for public data. See the API</p> <pre><code>icechunk.s3_storage(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\",\n    region='us-east-1,\n    anonymous=True,\n)\n</code></pre>"},{"location":"icechunk-python/configuration/#filesystem-storage","title":"Filesystem Storage","text":"<p>Icechunk can also be used on a local filesystem by providing a path to the location of the store</p> Local filesystem <pre><code>icechunk.local_filesystem_storage(\"/path/to/my/dataset\")\n</code></pre>"},{"location":"icechunk-python/configuration/#repository-config","title":"Repository Config","text":"<p>Separate from the storage config, the Repository can also be configured with options which control its runtime behavior.</p> <p>Note</p> <p>This section is under construction and coming soon.</p>"},{"location":"icechunk-python/configuration/#creating-and-opening-repos","title":"Creating and Opening Repos","text":"<p>Now we can now create or open an Icechunk repo using our config.</p>"},{"location":"icechunk-python/configuration/#creating-a-new-repo","title":"Creating a new repo","text":"<p>Note</p> <p>Icechunk repos cannot be created in the same location where another store already exists.</p> Creating with S3 storageCreating with Google Cloud StorageCreating with Azure Blob StorageCreating with local filesystem <pre><code>storage = icechunk.s3_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.create(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.gcs_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.create(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.azure_storage(\n    container='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.create(\n    storage=storage,\n)\n</code></pre> <pre><code>repo = icechunk.Repository.create(\n    storage=icechunk.local_filesystem_storage(\"/path/to/my/dataset\"),\n)\n</code></pre> <p>If you are not sure if the repo exists yet, an <code>icechunk Repository</code> can created or opened if it already exists:</p> Open or creating with S3 storageOpen or creating with Google Cloud StorageOpen or creating with Azure Blob StorageOpen or creating with local filesystem <pre><code>storage = icechunk.s3_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.open_or_create(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.gcs_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.open_or_create(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.azure_storage(\n    container='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.open_or_create(\n    storage=storage,\n)\n</code></pre> <pre><code>repo = icechunk.Repository.open_or_create(\n    storage=icechunk.local_filesystem_storage(\"/path/to/my/dataset\"),\n)\n</code></pre>"},{"location":"icechunk-python/configuration/#opening-an-existing-repo","title":"Opening an existing repo","text":"Opening from S3 StorageOpening from Google Cloud StorageOpening from Azure Blob StorageOpening from local filesystem <pre><code>storage = icechunk.s3_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.open(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.gcs_storage(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.open(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.azure_storage(\n    container='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    from_env=True,\n)\n\nrepo = icechunk.Repository.open(\n    storage=storage,\n)\n</code></pre> <pre><code>storage = icechunk.local_filesystem_storage(\"/path/to/my/dataset\")\nstore = icechunk.IcechunkStore.open(\n    storage=storage,\n)\n</code></pre>"},{"location":"icechunk-python/dask/","title":"Dask","text":"<p>Home / icechunk-python / dask</p>"},{"location":"icechunk-python/dask/#distributed-writes-with-dask","title":"Distributed Writes with dask","text":"<p>You can use Icechunk in conjunction with Xarray and Dask to perform large-scale distributed writes from a multi-node cluster. However, because of how Icechunk works, it's not possible to use the existing <code>Dask.Array.to_zarr</code> or <code>Xarray.Dataset.to_zarr</code> functions with either the Dask multiprocessing or distributed schedulers. (It is fine with the multithreaded scheduler.)</p> <p>Instead, Icechunk provides its own specialized functions to make distributed writes with Dask and Xarray. This page explains how to use these specialized functions.</p> <p>Note</p> <p>Using Xarray, Dask, and Icechunk requires <code>icechunk&gt;=0.1.0a5</code>, <code>dask&gt;=2024.11.0</code>, and <code>xarray&gt;=2024.11.0</code>.</p> <p>First let's start a distributed Client and create an IcechunkStore.</p> <pre><code># initialize a distributed Client\nfrom distributed import Client\n\nclient = Client()\n\n# initialize the icechunk store\nimport icechunk\n\nstorage = icechunk.local_filesystem_storage(\"./icechunk-xarray\")\nicechunk_repo = icechunk.Repository.create(storage_config)\nicechunk_session = icechunk_repo.writable_session(\"main\")\n</code></pre>"},{"location":"icechunk-python/dask/#icechunk-dask","title":"Icechunk + Dask","text":"<p>Use <code>icechunk.dask.store_dask</code> to write a Dask array to an Icechunk store. The API follows that of <code>dask.array.store</code> without support for the <code>compute</code> kwarg.</p> <p>First create a dask array to write: <pre><code>shape = (100, 100)\ndask_chunks = (20, 20)\ndask_array = dask.array.random.random(shape, chunks=dask_chunks)\n</code></pre></p> <p>Now create the Zarr array you will write to. <pre><code>zarr_chunks = (10, 10)\ngroup = zarr.group(store=icechunk_sesion.store, overwrite=True)\n\nzarray = group.create_array(\n    \"array\",\n    shape=shape,\n    chunks=zarr_chunks,\n    dtype=\"f8\",\n    fill_value=float(\"nan\"),\n)\n</code></pre> Note that the chunks in the store are a divisor of the dask chunks. This means each individual write task is independent, and will not conflict. It is your responsibility to ensure that such conflicts are avoided.</p> <p>Now write <pre><code>import icechunk.dask\n\nicechunk.dask.store_dask(icechunk_session, sources=[dask_array], targets=[zarray])\n</code></pre></p> <p>Finally commit your changes! <pre><code>icechunk_session.commit(\"wrote a dask array!\")\n</code></pre></p>"},{"location":"icechunk-python/dask/#distributed","title":"Distributed","text":"<p>In distributed contexts where the Session, and Zarr Array objects are sent across the network, you must opt-in to successful pickling of a writable store.</p> <p><code>icechunk.dask.store_dask</code> takes care of the hard bit of merging Sessions but it is required that you opt-in to pickling prior to creating the target Zarr array objects.</p> <p>Here is an example: <pre><code>import icechunk.dask\n\nzarr_chunks = (10, 10)\nwith icechunk_session.allow_pickling():\n    group = zarr.group(store=icechunk_sesion.store, overwrite=True)\n\n    zarray = group.create_array(\n        \"array\",\n        shape=shape,\n        chunks=zarr_chunks,\n        dtype=\"f8\",\n        fill_value=float(\"nan\"),\n    )\n    icechunk.dask.store_dask(icechunk_session, sources=[dask_array], targets=[zarray])\nicechunk_session.commit(\"wrote a dask array!\")\n</code></pre></p>"},{"location":"icechunk-python/dask/#icechunk-dask-xarray","title":"Icechunk + Dask + Xarray","text":"<p>The <code>icechunk.xarray.to_icechunk</code> is functionally identical to Xarray's <code>Dataset.to_zarr</code>, including many of the same keyword arguments. Notably the <code>compute</code> kwarg is not supported.</p> <p>Warning</p> <p>When using Xarray, Icechunk in a Dask Distributed context, you must use <code>to_icechunk</code> so that the Session has a record of the writes that are executed remotely. Using <code>to_zarr</code> in such cases, will result in the local Session having no record of remote writes, and a meaningless commit.</p> <p>Now roundtrip an xarray dataset <pre><code>import icechunk.xarray\nimport xarray as xr\n\n# Assuming you have a valid writable Session named icechunk_session\ndataset = xr.tutorial.open_dataset(\"rasm\", chunks={\"time\": 1}).isel(time=slice(24))\n\nicechunk.xarray.to_icechunk(dataset, session)\n\nroundtripped = xr.open_zarr(icechunk_session.store, consolidated=False)\ndataset.identical(roundtripped)\n</code></pre></p> <p>Finally commit your changes! <pre><code>icechunk_session.commit(\"wrote an Xarray dataset!\")\n</code></pre></p>"},{"location":"icechunk-python/faq/","title":"FAQ","text":"<p>Home / icechunk-python / faq</p>"},{"location":"icechunk-python/faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Why do I have to opt-in to pickling an IcechunkStore or a Session?</p> <p>Icechunk is different from normal Zarr stores because it is stateful. In a distributed setting, you have to be careful to communicate back the Session objects from remote write tasks, merge them and commit them. The opt-in to pickle is a way for us to hint to the user that they need to be sure about what they are doing. We use pickling because these operations are only tricky once you cross a process boundary. More pragmatically, to_zarr(session.store) fails spectacularly in distributed contexts (e.g. this issue), and we do not want the user to be surprised.</p>"},{"location":"icechunk-python/parallel/","title":"Parallel Writes","text":"<p>Home / icechunk-python / parallel</p>"},{"location":"icechunk-python/parallel/#parallel-writes","title":"Parallel Writes","text":"<p>A common pattern with large distributed write jobs is to first initialize the dataset on a disk with all appropriate metadata, and any coordinate variables. Following this a large write job is kicked off in a distributed setting, where each worker is responsible for an independent \"region\" of the output.</p>"},{"location":"icechunk-python/parallel/#why-is-icechunk-different-from-any-other-zarr-store","title":"Why is Icechunk different from any other Zarr store?","text":"<p>The reason is that unlike Zarr, Icechunk is a \"stateful\" store. The Session object keeps a record of all writes, that is then bundled together in a commit. Thus <code>Session.commit</code> must be executed on a Session object that knows about all writes, including those executed remotely in a multi-processing or any other remote execution context.</p>"},{"location":"icechunk-python/parallel/#example","title":"Example","text":"<p>Here is how you can execute such writes with Icechunk, illustrate with a <code>ThreadPoolExecutor</code>. First read some example data, and create an Icechunk Repository. <pre><code>import xarray as xr\nimport tempfile\nfrom icechunk import Repository, local_filesystem_storage\n\nds = xr.tutorial.open_dataset(\"rasm\").isel(time=slice(24))\nrepo = Repository.create(local_filesystem_storage(tempfile.mkdtemp()))\nsession = repo.writable_session(\"main\")\n</code></pre> We will orchestrate so that each task writes one timestep. This is an arbitrary choice but determines what we set for the Zarr chunk size. <pre><code>chunks = {1 if dim == \"time\" else ds.sizes[dim] for dim in ds.Tair.dims}\n</code></pre></p> <p>Initialize the dataset using <code>Dataset.to_zarr</code> and <code>compute=False</code>, this will NOT write any chunked array data, but will write all array metadata, and any in-memory arrays (only <code>time</code> in this case). <pre><code>ds.to_zarr(session.store, compute=False, encoding={\"Tair\": {\"chunks\": chunks}}, mode=\"w\")\n# this commit is optional, but may be useful in your workflow\nsession.commit(\"initialize store\")\n</code></pre></p>"},{"location":"icechunk-python/parallel/#multi-threading","title":"Multi-threading","text":"<p>First define a function that constitutes one \"write task\". <pre><code>from icechunk import Session\n\ndef write_timestamp(*, itime: int, session: Session) -&gt; None:\n    # pass a list to isel to preserve the time dimension\n    ds = xr.tutorial.open_dataset(\"rasm\").isel(time=[itime])\n    # region=\"auto\" tells Xarray to infer which \"region\" of the output arrays to write to.\n    ds.to_zarr(session.store, region=\"auto\", consolidated=False)\n</code></pre></p> <p>Now execute the writes. <pre><code>from concurrent.futures import ThreadPoolExecutor, wait\nfrom icechunk.distributed import merge_sessions\n\nsession = repo.writable_session(\"main\")\nwith ThreadPoolExecutor() as executor:\n    # submit the writes\n    futures = [executor.submit(write_timestamp, itime=i, session=session) for i in range(ds.sizes[\"time\"])]\n    wait(futures)\n\nsession.commit(\"finished writes\")\n</code></pre></p> <p>Verify that the writes worked as expected: <pre><code>ondisk = xr.open_zarr(repo.readonly_session(branch=\"main\").store, consolidated=False)\nxr.testing.assert_identical(ds, ondisk)\n</code></pre></p>"},{"location":"icechunk-python/parallel/#distributed-writes","title":"Distributed writes","text":"<p>Any task execution framework (e.g. <code>ProcessPoolExecutor</code>, Joblib, Lithops, Dask Distributed, Ray, etc.) can be used instead of the <code>ThreadPoolExecutor</code>. However such workloads should account for Icehunk being a \"stateful\" store that records changes executed in a write session.</p> <p>There are three key points to keep in mind: 1. The <code>write_task</code> function must return the <code>Session</code>. It contains a record of the changes executed by this task.    These changes must be manually communicated back to the coordinating process, since each of the distributed processes    are working with their own independent <code>Session</code> instance. 2. Icechunk requires that users opt-in to pickling a writable <code>Session</code> using the <code>Session.allow_pickling()</code> context manager,    to remind the user that distributed writes with Icechunk require care. 3. The user must manually merge the Session objects to create a meaningful commit.</p> <p>First we modify <code>write_task</code> to return the <code>Session</code>: <pre><code>from icechunk import Session\n\ndef write_timestamp(*, itime: int, session: Session) -&gt; Session:\n    # pass a list to isel to preserve the time dimension\n    ds = xr.tutorial.open_dataset(\"rasm\").isel(time=[itime])\n    # region=\"auto\" tells Xarray to infer which \"region\" of the output arrays to write to.\n    ds.to_zarr(session.store, region=\"auto\", consolidated=False)\n    return session\n</code></pre></p> <p>Now we issue write tasks within the <code>session.allow_pickling()</code> context, gather the Sessions from individual tasks, merge them, and make a successful commit.</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\nfrom icechunk.distributed import merge_sessions\n\nsession = repo.writable_session(\"main\")\nwith ProcessPoolExecutor() as executor:\n    # opt-in to successful pickling of a writable session\n    with session.allow_pickling():\n        # submit the writes\n        futures = [\n            executor.submit(write_timestamp, itime=i, session=session)\n            for i in range(ds.sizes[\"time\"])\n        ]\n    # grab the Session objects from each individual write task\n    sessions = [f.result() for f in futures]\n\n# manually merge the remote sessions in to the local session\nsession = merge_sessions(session, *sessions)\nsession.commit(\"finished writes\")\n</code></pre> <p>Verify that the writes worked as expected: <pre><code>ondisk = xr.open_zarr(repo.readonly_session(branch=\"main\").store, consolidated=False)\nxr.testing.assert_identical(ds, ondisk)\n</code></pre></p>"},{"location":"icechunk-python/quickstart/","title":"Quickstart","text":"<p>Home / icechunk-python / quickstart</p>"},{"location":"icechunk-python/quickstart/#quickstart","title":"Quickstart","text":"<p>Icechunk is designed to be mostly in the background. As a Python user, you'll mostly be interacting with Zarr. If you're not familiar with Zarr, you may want to start with the Zarr Tutorial</p>"},{"location":"icechunk-python/quickstart/#installation","title":"Installation","text":"<p>Install Icechunk with pip</p> <pre><code>pip install icechunk\n</code></pre> <p>Note</p> <p>Icechunk is currently designed to support the Zarr V3 Specification. Using it today requires installing Zarr Python 3.</p>"},{"location":"icechunk-python/quickstart/#create-a-new-icechunk-repository","title":"Create a new Icechunk repository","text":"<p>To get started, let's create a new Icechunk repository. We recommend creating your repo on a cloud storage platform to get the most out of Icechunk's cloud-native design. However, you can also create a repo on your local filesystem.</p> S3 StorageGoogle Cloud StorageAzure Blob StorageLocal Storage <pre><code>storage = icechunk.s3_storage(bucket=\"my-bucket\", prefix=\"my-prefix\", from_env=True)\nrepo = icechunk.Repository.create(storage)\n</code></pre> <pre><code>storage = icechunk.gcs_storage(bucket=\"my-bucket\", prefix=\"my-prefix\", from_env=True)\nrepo = icechunk.Repository.create(storage)\n</code></pre> <pre><code>storage = icechunk.azure_storage(container=\"my-container\", prefix=\"my-prefix\", from_env=True)\nrepo = icechunk.Repository.create(storage)\n</code></pre> <pre><code>storage = icechunk.local_filesystem_storage(\"./icechunk-local\")\nrepo = icechunk.Repository.create(storage)\n</code></pre>"},{"location":"icechunk-python/quickstart/#accessing-the-icechunk-store","title":"Accessing the Icechunk store","text":"<p>Once the repository is created, we can use <code>Session</code>s to read and write data. Since there is no data in the repository yet, let's create a writable session on the default <code>main</code> branch.</p> <pre><code>session = repo.writable_session(\"main\")\n</code></pre> <p>Now that we have a session, we can access the <code>IcechunkStore</code> from it to interact with the underlying data using <code>zarr</code>:</p> <pre><code>store = session.store  # A zarr store\n</code></pre>"},{"location":"icechunk-python/quickstart/#write-some-data-and-commit","title":"Write some data and commit","text":"<p>We can now use our Icechunk <code>store</code> with Zarr. Let's first create a group and an array within it.</p> <pre><code>group = zarr.group(store)\narray = group.create(\"my_array\", shape=10, dtype='int32', chunks=(5,))\n</code></pre> <p>Now let's write some data</p> <pre><code>array[:] = 1\n</code></pre> <p>Now let's commit our update using the session</p> <pre><code>session.commit(\"first commit\")\n</code></pre> <p>\ud83c\udf89 Congratulations! You just made your first Icechunk snapshot.</p> <p>Note</p> <p>Once a writable <code>Session</code> has been successfully committed to, it becomes read only to ensure that all writing is done explicitly.</p>"},{"location":"icechunk-python/quickstart/#make-a-second-commit","title":"Make a second commit","text":"<p>At this point, we have already committed using our session, so we need to get a new session and store to make more changes.</p> <pre><code>session_2 = repo.writable_session(\"main\")\nstore_2 = session_2.store\ngroup = zarr.open_group(store_2)\narray = group[\"my_array\"]\n</code></pre> <p>Let's now put some new data into our array, overwriting the first five elements.</p> <pre><code>array[:5] = 2\n</code></pre> <p>...and commit the changes</p> <pre><code>snapshot_id_2 = session_2.commit(\"overwrite some values\")\n</code></pre>"},{"location":"icechunk-python/quickstart/#explore-version-history","title":"Explore version history","text":"<p>We can see the full version history of our repo:</p> <pre><code>hist = repo.ancestry(snapshot=snapshot_id_2)\nfor ancestor in hist:\n    print(ancestor.id, ancestor.message, ancestor.written_at)\n\n# Output:\n# AHC3TSP5ERXKTM4FCB5G overwrite some values 2024-10-14 14:07:27.328429+00:00\n# Q492CAPV7SF3T1BC0AA0 first commit 2024-10-14 14:07:26.152193+00:00\n# T7SMDT9C5DZ8MP83DNM0 Repository initialized 2024-10-14 14:07:22.338529+00:00\n</code></pre> <p>...and we can go back in time to the earlier version.</p> <pre><code># latest version\nassert array[0] == 2\n# check out earlier snapshot\nearlier_session = repo.readonly_session(snapshot=snapshot_id=hist[1].id)\nstore = earlier_session.store\n\n# get the array\ngroup = zarr.open_group(store, mode=\"r\")\narray = group[\"my_array]\n\n# verify data matches first version\nassert array[0] == 1\n</code></pre> <p>That's it! You now know how to use Icechunk! For your next step, dig deeper into configuration, explore the version control system, or learn how to use Icechunk with Xarray.</p>"},{"location":"icechunk-python/reference/","title":"API Reference","text":"<p>Home / icechunk-python / reference </p>"},{"location":"icechunk-python/reference/#icechunk.BasicConflictSolver","title":"<code>BasicConflictSolver</code>","text":"<p>               Bases: <code>ConflictSolver</code></p> <p>A basic conflict solver that allows for simple configuration of resolution behavior</p> <p>This conflict solver allows for simple configuration of resolution behavior for conflicts that may occur during a rebase operation. It will attempt to resolve a limited set of conflicts based on the configuration options provided.</p> <ul> <li>When a user attribute conflict is encountered, the behavior is determined by the <code>on_user_attributes_conflict</code> option</li> <li>When a chunk conflict is encountered, the behavior is determined by the <code>on_chunk_conflict</code> option</li> <li>When an array is deleted that has been updated, <code>fail_on_delete_of_updated_array</code> will determine whether to fail the rebase operation</li> <li>When a group is deleted that has been updated, <code>fail_on_delete_of_updated_group</code> will determine whether to fail the rebase operation</li> </ul> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class BasicConflictSolver(ConflictSolver):\n    \"\"\"A basic conflict solver that allows for simple configuration of resolution behavior\n\n    This conflict solver allows for simple configuration of resolution behavior for conflicts that may occur during a rebase operation.\n    It will attempt to resolve a limited set of conflicts based on the configuration options provided.\n\n    - When a user attribute conflict is encountered, the behavior is determined by the `on_user_attributes_conflict` option\n    - When a chunk conflict is encountered, the behavior is determined by the `on_chunk_conflict` option\n    - When an array is deleted that has been updated, `fail_on_delete_of_updated_array` will determine whether to fail the rebase operation\n    - When a group is deleted that has been updated, `fail_on_delete_of_updated_group` will determine whether to fail the rebase operation\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        on_user_attributes_conflict: VersionSelection = VersionSelection.UseOurs,\n        on_chunk_conflict: VersionSelection = VersionSelection.UseOurs,\n        fail_on_delete_of_updated_array: bool = False,\n        fail_on_delete_of_updated_group: bool = False,\n    ) -&gt; None:\n        \"\"\"Create a BasicConflictSolver object with the given configuration options\n        Parameters:\n        on_user_attributes_conflict: VersionSelection\n            The behavior to use when a user attribute conflict is encountered, by default VersionSelection.use_ours()\n        on_chunk_conflict: VersionSelection\n            The behavior to use when a chunk conflict is encountered, by default VersionSelection.use_theirs()\n        fail_on_delete_of_updated_array: bool\n            Whether to fail when a chunk is deleted that has been updated, by default False\n        fail_on_delete_of_updated_group: bool\n            Whether to fail when a group is deleted that has been updated, by default False\n        \"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.BasicConflictSolver.__init__","title":"<code>__init__(*, on_user_attributes_conflict=VersionSelection.UseOurs, on_chunk_conflict=VersionSelection.UseOurs, fail_on_delete_of_updated_array=False, fail_on_delete_of_updated_group=False)</code>","text":"<p>Create a BasicConflictSolver object with the given configuration options Parameters: on_user_attributes_conflict: VersionSelection     The behavior to use when a user attribute conflict is encountered, by default VersionSelection.use_ours() on_chunk_conflict: VersionSelection     The behavior to use when a chunk conflict is encountered, by default VersionSelection.use_theirs() fail_on_delete_of_updated_array: bool     Whether to fail when a chunk is deleted that has been updated, by default False fail_on_delete_of_updated_group: bool     Whether to fail when a group is deleted that has been updated, by default False</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>def __init__(\n    self,\n    *,\n    on_user_attributes_conflict: VersionSelection = VersionSelection.UseOurs,\n    on_chunk_conflict: VersionSelection = VersionSelection.UseOurs,\n    fail_on_delete_of_updated_array: bool = False,\n    fail_on_delete_of_updated_group: bool = False,\n) -&gt; None:\n    \"\"\"Create a BasicConflictSolver object with the given configuration options\n    Parameters:\n    on_user_attributes_conflict: VersionSelection\n        The behavior to use when a user attribute conflict is encountered, by default VersionSelection.use_ours()\n    on_chunk_conflict: VersionSelection\n        The behavior to use when a chunk conflict is encountered, by default VersionSelection.use_theirs()\n    fail_on_delete_of_updated_array: bool\n        Whether to fail when a chunk is deleted that has been updated, by default False\n    fail_on_delete_of_updated_group: bool\n        Whether to fail when a group is deleted that has been updated, by default False\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.CachingConfig","title":"<code>CachingConfig</code>","text":"<p>Configuration for how Icechunk caches its metadata files</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class CachingConfig:\n    \"\"\"Configuration for how Icechunk caches its metadata files\"\"\"\n\n    def __init__(\n        self,\n        num_snapshot_nodes: int | None = None,\n        num_chunk_refs: int | None = None,\n        num_transaction_changes: int | None = None,\n        num_bytes_attributes: int | None = None,\n        num_bytes_chunks: int | None = None,\n    ) -&gt; None: ...\n    @property\n    def num_snapshot_nodes(self) -&gt; int | None: ...\n    @num_snapshot_nodes.setter\n    def num_snapshot_nodes(self, value: int | None) -&gt; None: ...\n    @property\n    def num_chunk_refs(self) -&gt; int | None: ...\n    @num_chunk_refs.setter\n    def num_chunk_refs(self, value: int | None) -&gt; None: ...\n    @property\n    def num_transaction_changes(self) -&gt; int | None: ...\n    @num_transaction_changes.setter\n    def num_transaction_changes(self, value: int | None) -&gt; None: ...\n    @property\n    def num_bytes_attributes(self) -&gt; int | None: ...\n    @num_bytes_attributes.setter\n    def num_bytes_attributes(self, value: int | None) -&gt; None: ...\n    @property\n    def num_bytes_chunks(self) -&gt; int | None: ...\n    @num_bytes_chunks.setter\n    def num_bytes_chunks(self, value: int | None) -&gt; None: ...\n    @staticmethod\n    def default() -&gt; CachingConfig: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.CompressionAlgorithm","title":"<code>CompressionAlgorithm</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for selecting the compression algorithm used by Icechunk to write its metadata files</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class CompressionAlgorithm(Enum):\n    \"\"\"Enum for selecting the compression algorithm used by Icechunk to write its metadata files\"\"\"\n\n    Zstd = 0\n\n    def __init__(self) -&gt; None: ...\n    @staticmethod\n    def default() -&gt; CompressionAlgorithm: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.CompressionConfig","title":"<code>CompressionConfig</code>","text":"<p>Configuration for how Icechunk compresses its metadata files</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class CompressionConfig:\n    \"\"\"Configuration for how Icechunk compresses its metadata files\"\"\"\n\n    def __init__(\n        self, algorithm: CompressionAlgorithm | None = None, level: int | None = None\n    ) -&gt; None: ...\n    @property\n    def algorithm(self) -&gt; CompressionAlgorithm | None: ...\n    @algorithm.setter\n    def algorithm(self, value: CompressionAlgorithm | None) -&gt; None: ...\n    @property\n    def level(self) -&gt; int | None: ...\n    @level.setter\n    def level(self, value: int | None) -&gt; None: ...\n    @staticmethod\n    def default() -&gt; CompressionConfig: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Conflict","title":"<code>Conflict</code>","text":"<p>A conflict detected between snapshots</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class Conflict:\n    \"\"\"A conflict detected between snapshots\"\"\"\n\n    @property\n    def conflict_type(self) -&gt; ConflictType:\n        \"\"\"The type of conflict detected\"\"\"\n        ...\n\n    @property\n    def path(self) -&gt; str:\n        \"\"\"The path of the node that caused the conflict\"\"\"\n        ...\n\n    @property\n    def conflicted_chunks(self) -&gt; list[list[int]] | None:\n        \"\"\"If the conflict is a chunk conflict, this will return the list of chunk indices that are in conflict\"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Conflict.conflict_type","title":"<code>conflict_type: ConflictType</code>  <code>property</code>","text":"<p>The type of conflict detected</p>"},{"location":"icechunk-python/reference/#icechunk.Conflict.conflicted_chunks","title":"<code>conflicted_chunks: list[list[int]] | None</code>  <code>property</code>","text":"<p>If the conflict is a chunk conflict, this will return the list of chunk indices that are in conflict</p>"},{"location":"icechunk-python/reference/#icechunk.Conflict.path","title":"<code>path: str</code>  <code>property</code>","text":"<p>The path of the node that caused the conflict</p>"},{"location":"icechunk-python/reference/#icechunk.ConflictDetector","title":"<code>ConflictDetector</code>","text":"<p>               Bases: <code>ConflictSolver</code></p> <p>A conflict solver that can be used to detect conflicts between two stores, but does not resolve them</p> <p>Where the <code>BasicConflictSolver</code> will attempt to resolve conflicts, the <code>ConflictDetector</code> will only detect them. This means that during a rebase operation the <code>ConflictDetector</code> will raise a <code>RebaseFailed</code> error if any conflicts are detected, and allow the rebase operation to be retried with a different conflict resolution strategy. Otherwise, if no conflicts are detected the rebase operation will succeed.</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ConflictDetector(ConflictSolver):\n    \"\"\"A conflict solver that can be used to detect conflicts between two stores, but does not resolve them\n\n    Where the `BasicConflictSolver` will attempt to resolve conflicts, the `ConflictDetector` will only detect them. This means\n    that during a rebase operation the `ConflictDetector` will raise a `RebaseFailed` error if any conflicts are detected, and\n    allow the rebase operation to be retried with a different conflict resolution strategy. Otherwise, if no conflicts are detected\n    the rebase operation will succeed.\n    \"\"\"\n\n    def __init__(self) -&gt; None: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ConflictError","title":"<code>ConflictError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when a commit operation fails due to a conflict.</p> Source code in <code>icechunk/session.py</code> <pre><code>class ConflictError(Exception):\n    \"\"\"Error raised when a commit operation fails due to a conflict.\"\"\"\n\n    _error: ConflictErrorData\n\n    def __init__(self, error: PyConflictError) -&gt; None:\n        self._error = error.args[0]\n\n    def __str__(self) -&gt; str:\n        return str(self._error)\n\n    @property\n    def expected_parent(self) -&gt; str:\n        \"\"\"\n        The expected parent snapshot ID.\n\n        Returns\n        -------\n        str\n            The snapshot ID that the session was based on when the commit operation was called.\n        \"\"\"\n        return self._error.expected_parent\n\n    @property\n    def actual_parent(self) -&gt; str:\n        \"\"\"\n        The actual parent snapshot ID of the branch that the session attempted to commit to.\n\n        Returns\n        -------\n        str\n            The snapshot ID of the branch tip. If this error is raised, it means the branch was modified and committed by another session after the session was created.\n        \"\"\"\n        return self._error.actual_parent\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ConflictError.actual_parent","title":"<code>actual_parent: str</code>  <code>property</code>","text":"<p>The actual parent snapshot ID of the branch that the session attempted to commit to.</p> <p>Returns:</p> Type Description <code>str</code> <p>The snapshot ID of the branch tip. If this error is raised, it means the branch was modified and committed by another session after the session was created.</p>"},{"location":"icechunk-python/reference/#icechunk.ConflictError.expected_parent","title":"<code>expected_parent: str</code>  <code>property</code>","text":"<p>The expected parent snapshot ID.</p> <p>Returns:</p> Type Description <code>str</code> <p>The snapshot ID that the session was based on when the commit operation was called.</p>"},{"location":"icechunk-python/reference/#icechunk.ConflictErrorData","title":"<code>ConflictErrorData</code>","text":"<p>Data class for conflict errors. This describes the snapshot conflict detected when committing a session</p> <p>If this error is raised, it means the branch was modified and committed by another session after the session was created.</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ConflictErrorData:\n    \"\"\"Data class for conflict errors. This describes the snapshot conflict detected when committing a session\n\n    If this error is raised, it means the branch was modified and committed by another session after the session was created.\n    \"\"\"\n    @property\n    def expected_parent(self) -&gt; str:\n        \"\"\"The expected parent snapshot ID.\n\n        This is the snapshot ID that the session was based on when the\n        commit operation was called.\n        \"\"\"\n        ...\n    @property\n    def actual_parent(self) -&gt; str:\n        \"\"\"\n        The actual parent snapshot ID of the branch that the session attempted to commit to.\n\n        When the session is based on a branch, this is the snapshot ID of the branch tip. If this\n        error is raised, it means the branch was modified and committed by another session after\n        the session was created.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ConflictErrorData.actual_parent","title":"<code>actual_parent: str</code>  <code>property</code>","text":"<p>The actual parent snapshot ID of the branch that the session attempted to commit to.</p> <p>When the session is based on a branch, this is the snapshot ID of the branch tip. If this error is raised, it means the branch was modified and committed by another session after the session was created.</p>"},{"location":"icechunk-python/reference/#icechunk.ConflictErrorData.expected_parent","title":"<code>expected_parent: str</code>  <code>property</code>","text":"<p>The expected parent snapshot ID.</p> <p>This is the snapshot ID that the session was based on when the commit operation was called.</p>"},{"location":"icechunk-python/reference/#icechunk.ConflictSolver","title":"<code>ConflictSolver</code>","text":"<p>An abstract conflict solver that can be used to detect or resolve conflicts between two stores</p> <p>This should never be used directly, but should be subclassed to provide specific conflict resolution behavior</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ConflictSolver:\n    \"\"\"An abstract conflict solver that can be used to detect or resolve conflicts between two stores\n\n    This should never be used directly, but should be subclassed to provide specific conflict resolution behavior\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ConflictType","title":"<code>ConflictType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Type of conflict detected</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ConflictType(Enum):\n    \"\"\"Type of conflict detected\"\"\"\n\n    NewNodeConflictsWithExistingNode = 1\n    NewNodeInInvalidGroup = 2\n    ZarrMetadataDoubleUpdate = 3\n    ZarrMetadataUpdateOfDeletedArray = 4\n    UserAttributesDoubleUpdate = 5\n    UserAttributesUpdateOfDeletedNode = 6\n    ChunkDoubleUpdate = 7\n    ChunksUpdatedInDeletedArray = 8\n    ChunksUpdatedInUpdatedArray = 9\n    DeleteOfUpdatedArray = 10\n    DeleteOfUpdatedGroup = 11\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkError","title":"<code>IcechunkError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all Icechunk errors</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class IcechunkError(Exception):\n    \"\"\"Base class for all Icechunk errors\"\"\"\n\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore","title":"<code>IcechunkStore</code>","text":"<p>               Bases: <code>Store</code>, <code>SyncMixin</code></p> Source code in <code>icechunk/store.py</code> <pre><code>class IcechunkStore(Store, SyncMixin):\n    _store: PyStore\n    _allow_pickling: bool\n\n    def __init__(\n        self,\n        store: PyStore,\n        allow_pickling: bool,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        \"\"\"Create a new IcechunkStore.\n\n        This should not be called directly, instead use the `create`, `open_existing` or `open_or_create` class methods.\n        \"\"\"\n        super().__init__(read_only=store.read_only)\n        if store is None:\n            raise ValueError(\n                \"An IcechunkStore should not be created with the default constructor, instead use either the create or open_existing class methods.\"\n            )\n        self._store = store\n        self._is_open = True\n        self._allow_pickling = allow_pickling\n\n    def __eq__(self, value: object) -&gt; bool:\n        if not isinstance(value, IcechunkStore):\n            return False\n        return self._store == value._store\n\n    def __getstate__(self) -&gt; object:\n        # we serialize the Rust store as bytes\n        if not self._allow_pickling and not self._store.read_only:\n            raise ValueError(\n                \"You must opt in to pickling this *writable* store by using `Session.allow_pickling` context manager\"\n            )\n        d = self.__dict__.copy()\n        d[\"_store\"] = self._store.as_bytes()\n        return d\n\n    def __setstate__(self, state: Any) -&gt; None:\n        # we have to deserialize the bytes of the Rust store\n        store_repr = state[\"_store\"]\n        state[\"_store\"] = PyStore.from_bytes(store_repr)\n        state[\"_read_only\"] = state[\"_store\"].read_only\n        self.__dict__ = state\n\n    @property\n    def session(self) -&gt; \"Session\":\n        from icechunk import Session\n\n        return Session(self._store.session, self._allow_pickling)\n\n    async def clear(self) -&gt; None:\n        \"\"\"Clear the store.\n\n        This will remove all contents from the current session,\n        including all groups and all arrays. But it will not modify the repository history.\n        \"\"\"\n        return await self._store.clear()\n\n    def sync_clear(self) -&gt; None:\n        \"\"\"Clear the store.\n\n        This will remove all contents from the current session,\n        including all groups and all arrays. But it will not modify the repository history.\n        \"\"\"\n        return self._store.sync_clear()\n\n    async def is_empty(self, prefix: str) -&gt; bool:\n        \"\"\"\n        Check if the directory is empty.\n\n        Parameters\n        ----------\n        prefix : str\n            Prefix of keys to check.\n\n        Returns\n        -------\n        bool\n            True if the store is empty, False otherwise.\n        \"\"\"\n        return await self._store.is_empty(prefix)\n\n    async def get(\n        self,\n        key: str,\n        prototype: BufferPrototype,\n        byte_range: ByteRequest | None = None,\n    ) -&gt; Buffer | None:\n        \"\"\"Retrieve the value associated with a given key.\n\n        Parameters\n        ----------\n        key : str\n        byte_range : ByteRequest, optional\n\n            ByteRequest may be one of the following. If not provided, all data associated with the key is retrieved.\n\n            - RangeByteRequest(int, int): Request a specific range of bytes in the form (start, end). The end is exclusive. If the given range is zero-length or starts after the end of the object, an error will be returned. Additionally, if the range ends after the end of the object, the entire remainder of the object will be returned. Otherwise, the exact requested range will be returned.\n            - OffsetByteRequest(int): Request all bytes starting from a given byte offset. This is equivalent to bytes={int}- as an HTTP header.\n            - SuffixByteRequest(int): Request the last int bytes. Note that here, int is the size of the request, not the byte offset. This is equivalent to bytes=-{int} as an HTTP header.\n\n        Returns\n        -------\n        Buffer\n        \"\"\"\n\n        try:\n            result = await self._store.get(key, _byte_request_to_tuple(byte_range))\n        except KeyError as _e:\n            # Zarr python expects None to be returned if the key does not exist\n            # but an IcechunkStore returns an error if the key does not exist\n            return None\n\n        return prototype.buffer.from_bytes(result)\n\n    async def get_partial_values(\n        self,\n        prototype: BufferPrototype,\n        key_ranges: Iterable[tuple[str, ByteRequest | None]],\n    ) -&gt; list[Buffer | None]:\n        \"\"\"Retrieve possibly partial values from given key_ranges.\n\n        Parameters\n        ----------\n        key_ranges : Iterable[tuple[str, tuple[int | None, int | None]]]\n            Ordered set of key, range pairs, a key may occur multiple times with different ranges\n\n        Returns\n        -------\n        list of values, in the order of the key_ranges, may contain null/none for missing keys\n        \"\"\"\n        # NOTE: pyo3 has not implicit conversion from an Iterable to a rust iterable. So we convert it\n        # to a list here first. Possible opportunity for optimization.\n        ranges = [(k[0], _byte_request_to_tuple(k[1])) for k in key_ranges]\n        result = await self._store.get_partial_values(list(ranges))\n        return [prototype.buffer.from_bytes(r) for r in result]\n\n    async def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if a key exists in the store.\n\n        Parameters\n        ----------\n        key : str\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return await self._store.exists(key)\n\n    @property\n    def supports_writes(self) -&gt; bool:\n        \"\"\"Does the store support writes?\"\"\"\n        return self._store.supports_writes\n\n    async def set(self, key: str, value: Buffer) -&gt; None:\n        \"\"\"Store a (key, value) pair.\n\n        Parameters\n        ----------\n        key : str\n        value : Buffer\n        \"\"\"\n        if not isinstance(value, Buffer):\n            raise TypeError(\n                f\"IcechunkStore.set(): `value` must be a Buffer instance. Got an instance of {type(value)} instead.\"\n            )\n        return await self._store.set(key, value.to_bytes())\n\n    async def set_if_not_exists(self, key: str, value: Buffer) -&gt; None:\n        \"\"\"\n        Store a key to ``value`` if the key is not already present.\n\n        Parameters\n        -----------\n        key : str\n        value : Buffer\n        \"\"\"\n        return await self._store.set_if_not_exists(key, value.to_bytes())\n\n    def set_virtual_ref(\n        self,\n        key: str,\n        location: str,\n        *,\n        offset: int,\n        length: int,\n        checksum: str | datetime | None = None,\n        validate_container: bool = False,\n    ) -&gt; None:\n        \"\"\"Store a virtual reference to a chunk.\n\n        Parameters\n        ----------\n        key : str\n            The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'\n        location : str\n            The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'\n        offset : int\n            The offset in bytes from the start of the file location in storage the chunk starts at\n        length : int\n            The length of the chunk in bytes, measured from the given offset\n        checksum : str | datetime | None\n            The etag or last_medified_at field of the object\n        validate_container: bool\n            If set to true, fail for locations that don't match any existing virtual chunk container\n        \"\"\"\n        return self._store.set_virtual_ref(\n            key, location, offset, length, checksum, validate_container\n        )\n\n    async def delete(self, key: str) -&gt; None:\n        \"\"\"Remove a key from the store\n\n        Parameters\n        ----------\n        key : str\n        \"\"\"\n        return await self._store.delete(key)\n\n    async def delete_dir(self, prefix: str) -&gt; None:\n        \"\"\"Delete a prefix\n\n        Parameters\n        ----------\n        key : str\n        \"\"\"\n        return await self._store.delete_dir(prefix)\n\n    @property\n    def supports_partial_writes(self) -&gt; bool:\n        \"\"\"Does the store support partial writes?\"\"\"\n        return self._store.supports_partial_writes\n\n    async def set_partial_values(\n        self, key_start_values: Iterable[tuple[str, int, BytesLike]]\n    ) -&gt; None:\n        \"\"\"Store values at a given key, starting at byte range_start.\n\n        Parameters\n        ----------\n        key_start_values : list[tuple[str, int, BytesLike]]\n            set of key, range_start, values triples, a key may occur multiple times with different\n            range_starts, range_starts (considering the length of the respective values) must not\n            specify overlapping ranges for the same key\n        \"\"\"\n        # NOTE: pyo3 does not implicit conversion from an Iterable to a rust iterable. So we convert it\n        # to a list here first. Possible opportunity for optimization.\n        return await self._store.set_partial_values(list(key_start_values))\n\n    @property\n    def supports_listing(self) -&gt; bool:\n        \"\"\"Does the store support listing?\"\"\"\n        return self._store.supports_listing\n\n    @property\n    def supports_deletes(self) -&gt; bool:\n        return self._store.supports_deletes\n\n    def list(self) -&gt; AsyncIterator[str]:\n        \"\"\"Retrieve all keys in the store.\n\n        Returns\n        -------\n        AsyncIterator[str, None]\n        \"\"\"\n        # This method should be async, like overridden methods in child classes.\n        # However, that's not straightforward:\n        # https://stackoverflow.com/questions/68905848\n\n        # The zarr spec specefies that that this and other\n        # listing methods should not be async, so we need to\n        # wrap the async method in a sync method.\n        return self._store.list()\n\n    def list_prefix(self, prefix: str) -&gt; AsyncIterator[str]:\n        \"\"\"Retrieve all keys in the store that begin with a given prefix. Keys are returned relative\n        to the root of the store.\n\n        Parameters\n        ----------\n        prefix : str\n\n        Returns\n        -------\n        AsyncIterator[str, None]\n        \"\"\"\n        # The zarr spec specefies that that this and other\n        # listing methods should not be async, so we need to\n        # wrap the async method in a sync method.\n        return self._store.list_prefix(prefix)\n\n    def list_dir(self, prefix: str) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Retrieve all keys and prefixes with a given prefix and which do not contain the character\n        \u201c/\u201d after the given prefix.\n\n        Parameters\n        ----------\n        prefix : str\n\n        Returns\n        -------\n        AsyncIterator[str, None]\n        \"\"\"\n        # The zarr spec specefies that that this and other\n        # listing methods should not be async, so we need to\n        # wrap the async method in a sync method.\n        return self._store.list_dir(prefix)\n\n    async def getsize(self, key: str) -&gt; int:\n        return await self._store.getsize(key)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.supports_listing","title":"<code>supports_listing: bool</code>  <code>property</code>","text":"<p>Does the store support listing?</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.supports_partial_writes","title":"<code>supports_partial_writes: bool</code>  <code>property</code>","text":"<p>Does the store support partial writes?</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.supports_writes","title":"<code>supports_writes: bool</code>  <code>property</code>","text":"<p>Does the store support writes?</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.__init__","title":"<code>__init__(store, allow_pickling, *args, **kwargs)</code>","text":"<p>Create a new IcechunkStore.</p> <p>This should not be called directly, instead use the <code>create</code>, <code>open_existing</code> or <code>open_or_create</code> class methods.</p> Source code in <code>icechunk/store.py</code> <pre><code>def __init__(\n    self,\n    store: PyStore,\n    allow_pickling: bool,\n    *args: Any,\n    **kwargs: Any,\n):\n    \"\"\"Create a new IcechunkStore.\n\n    This should not be called directly, instead use the `create`, `open_existing` or `open_or_create` class methods.\n    \"\"\"\n    super().__init__(read_only=store.read_only)\n    if store is None:\n        raise ValueError(\n            \"An IcechunkStore should not be created with the default constructor, instead use either the create or open_existing class methods.\"\n        )\n    self._store = store\n    self._is_open = True\n    self._allow_pickling = allow_pickling\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Clear the store.</p> <p>This will remove all contents from the current session, including all groups and all arrays. But it will not modify the repository history.</p> Source code in <code>icechunk/store.py</code> <pre><code>async def clear(self) -&gt; None:\n    \"\"\"Clear the store.\n\n    This will remove all contents from the current session,\n    including all groups and all arrays. But it will not modify the repository history.\n    \"\"\"\n    return await self._store.clear()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Remove a key from the store</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required Source code in <code>icechunk/store.py</code> <pre><code>async def delete(self, key: str) -&gt; None:\n    \"\"\"Remove a key from the store\n\n    Parameters\n    ----------\n    key : str\n    \"\"\"\n    return await self._store.delete(key)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.delete_dir","title":"<code>delete_dir(prefix)</code>  <code>async</code>","text":"<p>Delete a prefix</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required Source code in <code>icechunk/store.py</code> <pre><code>async def delete_dir(self, prefix: str) -&gt; None:\n    \"\"\"Delete a prefix\n\n    Parameters\n    ----------\n    key : str\n    \"\"\"\n    return await self._store.delete_dir(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Check if a key exists in the store.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>icechunk/store.py</code> <pre><code>async def exists(self, key: str) -&gt; bool:\n    \"\"\"Check if a key exists in the store.\n\n    Parameters\n    ----------\n    key : str\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return await self._store.exists(key)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get","title":"<code>get(key, prototype, byte_range=None)</code>  <code>async</code>","text":"<p>Retrieve the value associated with a given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <code>byte_range</code> <code>ByteRequest</code> <p>ByteRequest may be one of the following. If not provided, all data associated with the key is retrieved.</p> <ul> <li>RangeByteRequest(int, int): Request a specific range of bytes in the form (start, end). The end is exclusive. If the given range is zero-length or starts after the end of the object, an error will be returned. Additionally, if the range ends after the end of the object, the entire remainder of the object will be returned. Otherwise, the exact requested range will be returned.</li> <li>OffsetByteRequest(int): Request all bytes starting from a given byte offset. This is equivalent to bytes={int}- as an HTTP header.</li> <li>SuffixByteRequest(int): Request the last int bytes. Note that here, int is the size of the request, not the byte offset. This is equivalent to bytes=-{int} as an HTTP header.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Buffer</code> Source code in <code>icechunk/store.py</code> <pre><code>async def get(\n    self,\n    key: str,\n    prototype: BufferPrototype,\n    byte_range: ByteRequest | None = None,\n) -&gt; Buffer | None:\n    \"\"\"Retrieve the value associated with a given key.\n\n    Parameters\n    ----------\n    key : str\n    byte_range : ByteRequest, optional\n\n        ByteRequest may be one of the following. If not provided, all data associated with the key is retrieved.\n\n        - RangeByteRequest(int, int): Request a specific range of bytes in the form (start, end). The end is exclusive. If the given range is zero-length or starts after the end of the object, an error will be returned. Additionally, if the range ends after the end of the object, the entire remainder of the object will be returned. Otherwise, the exact requested range will be returned.\n        - OffsetByteRequest(int): Request all bytes starting from a given byte offset. This is equivalent to bytes={int}- as an HTTP header.\n        - SuffixByteRequest(int): Request the last int bytes. Note that here, int is the size of the request, not the byte offset. This is equivalent to bytes=-{int} as an HTTP header.\n\n    Returns\n    -------\n    Buffer\n    \"\"\"\n\n    try:\n        result = await self._store.get(key, _byte_request_to_tuple(byte_range))\n    except KeyError as _e:\n        # Zarr python expects None to be returned if the key does not exist\n        # but an IcechunkStore returns an error if the key does not exist\n        return None\n\n    return prototype.buffer.from_bytes(result)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get_partial_values","title":"<code>get_partial_values(prototype, key_ranges)</code>  <code>async</code>","text":"<p>Retrieve possibly partial values from given key_ranges.</p> <p>Parameters:</p> Name Type Description Default <code>key_ranges</code> <code>Iterable[tuple[str, tuple[int | None, int | None]]]</code> <p>Ordered set of key, range pairs, a key may occur multiple times with different ranges</p> required <p>Returns:</p> Type Description <code>list of values, in the order of the key_ranges, may contain null/none for missing keys</code> Source code in <code>icechunk/store.py</code> <pre><code>async def get_partial_values(\n    self,\n    prototype: BufferPrototype,\n    key_ranges: Iterable[tuple[str, ByteRequest | None]],\n) -&gt; list[Buffer | None]:\n    \"\"\"Retrieve possibly partial values from given key_ranges.\n\n    Parameters\n    ----------\n    key_ranges : Iterable[tuple[str, tuple[int | None, int | None]]]\n        Ordered set of key, range pairs, a key may occur multiple times with different ranges\n\n    Returns\n    -------\n    list of values, in the order of the key_ranges, may contain null/none for missing keys\n    \"\"\"\n    # NOTE: pyo3 has not implicit conversion from an Iterable to a rust iterable. So we convert it\n    # to a list here first. Possible opportunity for optimization.\n    ranges = [(k[0], _byte_request_to_tuple(k[1])) for k in key_ranges]\n    result = await self._store.get_partial_values(list(ranges))\n    return [prototype.buffer.from_bytes(r) for r in result]\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.is_empty","title":"<code>is_empty(prefix)</code>  <code>async</code>","text":"<p>Check if the directory is empty.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix of keys to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the store is empty, False otherwise.</p> Source code in <code>icechunk/store.py</code> <pre><code>async def is_empty(self, prefix: str) -&gt; bool:\n    \"\"\"\n    Check if the directory is empty.\n\n    Parameters\n    ----------\n    prefix : str\n        Prefix of keys to check.\n\n    Returns\n    -------\n    bool\n        True if the store is empty, False otherwise.\n    \"\"\"\n    return await self._store.is_empty(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list","title":"<code>list()</code>","text":"<p>Retrieve all keys in the store.</p> <p>Returns:</p> Type Description <code>AsyncIterator[str, None]</code> Source code in <code>icechunk/store.py</code> <pre><code>def list(self) -&gt; AsyncIterator[str]:\n    \"\"\"Retrieve all keys in the store.\n\n    Returns\n    -------\n    AsyncIterator[str, None]\n    \"\"\"\n    # This method should be async, like overridden methods in child classes.\n    # However, that's not straightforward:\n    # https://stackoverflow.com/questions/68905848\n\n    # The zarr spec specefies that that this and other\n    # listing methods should not be async, so we need to\n    # wrap the async method in a sync method.\n    return self._store.list()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_dir","title":"<code>list_dir(prefix)</code>","text":"<p>Retrieve all keys and prefixes with a given prefix and which do not contain the character \u201c/\u201d after the given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> required <p>Returns:</p> Type Description <code>AsyncIterator[str, None]</code> Source code in <code>icechunk/store.py</code> <pre><code>def list_dir(self, prefix: str) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Retrieve all keys and prefixes with a given prefix and which do not contain the character\n    \u201c/\u201d after the given prefix.\n\n    Parameters\n    ----------\n    prefix : str\n\n    Returns\n    -------\n    AsyncIterator[str, None]\n    \"\"\"\n    # The zarr spec specefies that that this and other\n    # listing methods should not be async, so we need to\n    # wrap the async method in a sync method.\n    return self._store.list_dir(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_prefix","title":"<code>list_prefix(prefix)</code>","text":"<p>Retrieve all keys in the store that begin with a given prefix. Keys are returned relative to the root of the store.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> required <p>Returns:</p> Type Description <code>AsyncIterator[str, None]</code> Source code in <code>icechunk/store.py</code> <pre><code>def list_prefix(self, prefix: str) -&gt; AsyncIterator[str]:\n    \"\"\"Retrieve all keys in the store that begin with a given prefix. Keys are returned relative\n    to the root of the store.\n\n    Parameters\n    ----------\n    prefix : str\n\n    Returns\n    -------\n    AsyncIterator[str, None]\n    \"\"\"\n    # The zarr spec specefies that that this and other\n    # listing methods should not be async, so we need to\n    # wrap the async method in a sync method.\n    return self._store.list_prefix(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set","title":"<code>set(key, value)</code>  <code>async</code>","text":"<p>Store a (key, value) pair.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <code>value</code> <code>Buffer</code> required Source code in <code>icechunk/store.py</code> <pre><code>async def set(self, key: str, value: Buffer) -&gt; None:\n    \"\"\"Store a (key, value) pair.\n\n    Parameters\n    ----------\n    key : str\n    value : Buffer\n    \"\"\"\n    if not isinstance(value, Buffer):\n        raise TypeError(\n            f\"IcechunkStore.set(): `value` must be a Buffer instance. Got an instance of {type(value)} instead.\"\n        )\n    return await self._store.set(key, value.to_bytes())\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_if_not_exists","title":"<code>set_if_not_exists(key, value)</code>  <code>async</code>","text":"<p>Store a key to <code>value</code> if the key is not already present.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <code>value</code> <code>Buffer</code> required Source code in <code>icechunk/store.py</code> <pre><code>async def set_if_not_exists(self, key: str, value: Buffer) -&gt; None:\n    \"\"\"\n    Store a key to ``value`` if the key is not already present.\n\n    Parameters\n    -----------\n    key : str\n    value : Buffer\n    \"\"\"\n    return await self._store.set_if_not_exists(key, value.to_bytes())\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_partial_values","title":"<code>set_partial_values(key_start_values)</code>  <code>async</code>","text":"<p>Store values at a given key, starting at byte range_start.</p> <p>Parameters:</p> Name Type Description Default <code>key_start_values</code> <code>list[tuple[str, int, BytesLike]]</code> <p>set of key, range_start, values triples, a key may occur multiple times with different range_starts, range_starts (considering the length of the respective values) must not specify overlapping ranges for the same key</p> required Source code in <code>icechunk/store.py</code> <pre><code>async def set_partial_values(\n    self, key_start_values: Iterable[tuple[str, int, BytesLike]]\n) -&gt; None:\n    \"\"\"Store values at a given key, starting at byte range_start.\n\n    Parameters\n    ----------\n    key_start_values : list[tuple[str, int, BytesLike]]\n        set of key, range_start, values triples, a key may occur multiple times with different\n        range_starts, range_starts (considering the length of the respective values) must not\n        specify overlapping ranges for the same key\n    \"\"\"\n    # NOTE: pyo3 does not implicit conversion from an Iterable to a rust iterable. So we convert it\n    # to a list here first. Possible opportunity for optimization.\n    return await self._store.set_partial_values(list(key_start_values))\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_virtual_ref","title":"<code>set_virtual_ref(key, location, *, offset, length, checksum=None, validate_container=False)</code>","text":"<p>Store a virtual reference to a chunk.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'</p> required <code>location</code> <code>str</code> <p>The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'</p> required <code>offset</code> <code>int</code> <p>The offset in bytes from the start of the file location in storage the chunk starts at</p> required <code>length</code> <code>int</code> <p>The length of the chunk in bytes, measured from the given offset</p> required <code>checksum</code> <code>str | datetime | None</code> <p>The etag or last_medified_at field of the object</p> <code>None</code> <code>validate_container</code> <code>bool</code> <p>If set to true, fail for locations that don't match any existing virtual chunk container</p> <code>False</code> Source code in <code>icechunk/store.py</code> <pre><code>def set_virtual_ref(\n    self,\n    key: str,\n    location: str,\n    *,\n    offset: int,\n    length: int,\n    checksum: str | datetime | None = None,\n    validate_container: bool = False,\n) -&gt; None:\n    \"\"\"Store a virtual reference to a chunk.\n\n    Parameters\n    ----------\n    key : str\n        The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'\n    location : str\n        The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'\n    offset : int\n        The offset in bytes from the start of the file location in storage the chunk starts at\n    length : int\n        The length of the chunk in bytes, measured from the given offset\n    checksum : str | datetime | None\n        The etag or last_medified_at field of the object\n    validate_container: bool\n        If set to true, fail for locations that don't match any existing virtual chunk container\n    \"\"\"\n    return self._store.set_virtual_ref(\n        key, location, offset, length, checksum, validate_container\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.sync_clear","title":"<code>sync_clear()</code>","text":"<p>Clear the store.</p> <p>This will remove all contents from the current session, including all groups and all arrays. But it will not modify the repository history.</p> Source code in <code>icechunk/store.py</code> <pre><code>def sync_clear(self) -&gt; None:\n    \"\"\"Clear the store.\n\n    This will remove all contents from the current session,\n    including all groups and all arrays. But it will not modify the repository history.\n    \"\"\"\n    return self._store.sync_clear()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestConfig","title":"<code>ManifestConfig</code>","text":"<p>Configuration for how Icechunk manifests</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ManifestConfig:\n    \"\"\"Configuration for how Icechunk manifests\"\"\"\n\n    def __init__(\n        self,\n        preload: ManifestPreloadConfig | None = None,\n    ) -&gt; None: ...\n    @property\n    def preload(self) -&gt; ManifestPreloadConfig | None: ...\n    @preload.setter\n    def preload(self, value: ManifestPreloadConfig | None) -&gt; None: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition","title":"<code>ManifestPreloadCondition</code>","text":"<p>Configuration for conditions under which manifests will preload on session creation</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ManifestPreloadCondition:\n    \"\"\"Configuration for conditions under which manifests will preload on session creation\"\"\"\n\n    @staticmethod\n    def or_conditions(\n        conditions: list[ManifestPreloadCondition],\n    ) -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that matches if any of `conditions` matches\"\"\"\n        ...\n    @staticmethod\n    def and_conditions(\n        conditions: list[ManifestPreloadCondition],\n    ) -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that matches only if all passed `conditions` match\"\"\"\n        ...\n    @staticmethod\n    def path_matches(regex: str) -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that matches if the full path to the array matches the passed regex.\n\n        Array paths are absolute, as in `/path/to/my/array`\n        \"\"\"\n        ...\n    @staticmethod\n    def name_matches(regex: str) -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that matches if the array's name matches the passed regex.\n\n        Example, for an array  `/model/outputs/temperature`, the following will match:\n        ```\n        name_matches(\".*temp.*\")\n        ```\n        \"\"\"\n        ...\n    @staticmethod\n    def num_refs(from_refs: int | None, to_refs: int | None) -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that matches only if the number of chunk references in the manifest is within the given range.\n\n        from_refs is inclusive, to_refs is exclusive.\n        \"\"\"\n        ...\n    @staticmethod\n    def true() -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that always matches any manifest\"\"\"\n        ...\n    @staticmethod\n    def false() -&gt; ManifestPreloadCondition:\n        \"\"\"Create a preload condition that never matches any manifests\"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.and_conditions","title":"<code>and_conditions(conditions)</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that matches only if all passed <code>conditions</code> match</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef and_conditions(\n    conditions: list[ManifestPreloadCondition],\n) -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that matches only if all passed `conditions` match\"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.false","title":"<code>false()</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that never matches any manifests</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef false() -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that never matches any manifests\"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.name_matches","title":"<code>name_matches(regex)</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that matches if the array's name matches the passed regex.</p> <p>Example, for an array  <code>/model/outputs/temperature</code>, the following will match: <pre><code>name_matches(\".*temp.*\")\n</code></pre></p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef name_matches(regex: str) -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that matches if the array's name matches the passed regex.\n\n    Example, for an array  `/model/outputs/temperature`, the following will match:\n    ```\n    name_matches(\".*temp.*\")\n    ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.num_refs","title":"<code>num_refs(from_refs, to_refs)</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that matches only if the number of chunk references in the manifest is within the given range.</p> <p>from_refs is inclusive, to_refs is exclusive.</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef num_refs(from_refs: int | None, to_refs: int | None) -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that matches only if the number of chunk references in the manifest is within the given range.\n\n    from_refs is inclusive, to_refs is exclusive.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.or_conditions","title":"<code>or_conditions(conditions)</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that matches if any of <code>conditions</code> matches</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef or_conditions(\n    conditions: list[ManifestPreloadCondition],\n) -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that matches if any of `conditions` matches\"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.path_matches","title":"<code>path_matches(regex)</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that matches if the full path to the array matches the passed regex.</p> <p>Array paths are absolute, as in <code>/path/to/my/array</code></p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef path_matches(regex: str) -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that matches if the full path to the array matches the passed regex.\n\n    Array paths are absolute, as in `/path/to/my/array`\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadCondition.true","title":"<code>true()</code>  <code>staticmethod</code>","text":"<p>Create a preload condition that always matches any manifest</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@staticmethod\ndef true() -&gt; ManifestPreloadCondition:\n    \"\"\"Create a preload condition that always matches any manifest\"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.ManifestPreloadConfig","title":"<code>ManifestPreloadConfig</code>","text":"<p>Configuration for how Icechunk manifest preload on session creation</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class ManifestPreloadConfig:\n    \"\"\"Configuration for how Icechunk manifest preload on session creation\"\"\"\n\n    def __init__(\n        self,\n        max_total_refs: int | None = None,\n        preload_if: ManifestPreloadCondition | None = None,\n    ) -&gt; None: ...\n    @property\n    def max_total_refs(self) -&gt; int | None: ...\n    @max_total_refs.setter\n    def max_total_refs(self, value: int | None) -&gt; None: ...\n    @property\n    def preload_if(self) -&gt; ManifestPreloadCondition | None: ...\n    @preload_if.setter\n    def preload_if(self, value: ManifestPreloadCondition | None) -&gt; None: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.RebaseFailedData","title":"<code>RebaseFailedData</code>","text":"<p>Data class for rebase failed errors. This describes the error that occurred when rebasing a session</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class RebaseFailedData:\n    \"\"\"Data class for rebase failed errors. This describes the error that occurred when rebasing a session\"\"\"\n\n    @property\n    def snapshot(self) -&gt; str:\n        \"\"\"The snapshot ID that the session was rebased to\"\"\"\n        ...\n\n    @property\n    def conflicts(self) -&gt; list[Conflict]:\n        \"\"\"The conflicts that occurred during the rebase operation\"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.RebaseFailedData.conflicts","title":"<code>conflicts: list[Conflict]</code>  <code>property</code>","text":"<p>The conflicts that occurred during the rebase operation</p>"},{"location":"icechunk-python/reference/#icechunk.RebaseFailedData.snapshot","title":"<code>snapshot: str</code>  <code>property</code>","text":"<p>The snapshot ID that the session was rebased to</p>"},{"location":"icechunk-python/reference/#icechunk.RebaseFailedError","title":"<code>RebaseFailedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when a rebase operation fails.</p> Source code in <code>icechunk/session.py</code> <pre><code>class RebaseFailedError(Exception):\n    \"\"\"Error raised when a rebase operation fails.\"\"\"\n\n    _error: RebaseFailedData\n\n    def __init__(self, error: PyRebaseFailedError) -&gt; None:\n        self._error = error.args[0]\n\n    def __str__(self) -&gt; str:\n        return str(self._error)\n\n    @property\n    def snapshot_id(self) -&gt; str:\n        \"\"\"\n        The snapshot ID that the rebase operation failed on.\n\n        Returns\n        -------\n        str\n            The snapshot ID that the rebase operation failed on.\n        \"\"\"\n        return self._error.snapshot\n\n    @property\n    def conflicts(self) -&gt; list[Conflict]:\n        \"\"\"\n        List of conflicts that occurred during the rebase operation.\n\n        Returns\n        -------\n        list of Conflict\n            List of conflicts that occurred during the rebase operation.\n        \"\"\"\n        return self._error.conflicts\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.RebaseFailedError.conflicts","title":"<code>conflicts: list[Conflict]</code>  <code>property</code>","text":"<p>List of conflicts that occurred during the rebase operation.</p> <p>Returns:</p> Type Description <code>list of Conflict</code> <p>List of conflicts that occurred during the rebase operation.</p>"},{"location":"icechunk-python/reference/#icechunk.RebaseFailedError.snapshot_id","title":"<code>snapshot_id: str</code>  <code>property</code>","text":"<p>The snapshot ID that the rebase operation failed on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The snapshot ID that the rebase operation failed on.</p>"},{"location":"icechunk-python/reference/#icechunk.Repository","title":"<code>Repository</code>","text":"<p>An Icechunk repository.</p> Source code in <code>icechunk/repository.py</code> <pre><code>class Repository:\n    \"\"\"An Icechunk repository.\"\"\"\n\n    _repository: PyRepository\n\n    def __init__(self, repository: PyRepository):\n        self._repository = repository\n\n    @classmethod\n    def create(\n        cls,\n        storage: Storage,\n        config: RepositoryConfig | None = None,\n        virtual_chunk_credentials: dict[str, AnyCredential] | None = None,\n    ) -&gt; Self:\n        \"\"\"\n        Create a new Icechunk repository.\n        If one already exists at the given store location, an error will be raised.\n\n        Parameters\n        ----------\n        storage : Storage\n            The storage configuration for the repository.\n        config : RepositoryConfig, optional\n            The repository configuration. If not provided, a default configuration will be used.\n        virtual_chunk_credentials : dict[str, AnyCredential], optional\n            Credentials for virtual chunks.\n\n        Returns\n        -------\n        Self\n            An instance of the Repository class.\n        \"\"\"\n        return cls(\n            PyRepository.create(\n                storage,\n                config=config,\n                virtual_chunk_credentials=virtual_chunk_credentials,\n            )\n        )\n\n    @classmethod\n    def open(\n        cls,\n        storage: Storage,\n        config: RepositoryConfig | None = None,\n        virtual_chunk_credentials: dict[str, AnyCredential] | None = None,\n    ) -&gt; Self:\n        \"\"\"\n        Open an existing Icechunk repository.\n\n        If no repository exists at the given storage location, an error will be raised.\n\n        Parameters\n        ----------\n        storage : Storage\n            The storage configuration for the repository.\n        config : RepositoryConfig, optional\n            The repository settings. If not provided, a default configuration will be\n            loaded from the repository.\n        virtual_chunk_credentials : dict[str, AnyCredential], optional\n            Credentials for virtual chunks.\n\n        Returns\n        -------\n        Self\n            An instance of the Repository class.\n        \"\"\"\n        return cls(\n            PyRepository.open(\n                storage,\n                config=config,\n                virtual_chunk_credentials=virtual_chunk_credentials,\n            )\n        )\n\n    @classmethod\n    def open_or_create(\n        cls,\n        storage: Storage,\n        config: RepositoryConfig | None = None,\n        virtual_chunk_credentials: dict[str, AnyCredential] | None = None,\n    ) -&gt; Self:\n        \"\"\"\n        Open an existing Icechunk repository or create a new one if it does not exist.\n\n        Parameters\n        ----------\n        storage : Storage\n            The storage configuration for the repository.\n        config : RepositoryConfig, optional\n            The repository settings. If not provided, a default configuration will be\n            loaded from the repository.\n        virtual_chunk_credentials : dict[str, AnyCredential], optional\n            Credentials for virtual chunks.\n\n        Returns\n        -------\n        Self\n            An instance of the Repository class.\n        \"\"\"\n        return cls(\n            PyRepository.open_or_create(\n                storage,\n                config=config,\n                virtual_chunk_credentials=virtual_chunk_credentials,\n            )\n        )\n\n    @staticmethod\n    def exists(storage: Storage) -&gt; bool:\n        \"\"\"\n        Check if a repository exists at the given storage location.\n\n        Parameters\n        ----------\n        storage : Storage\n            The storage configuration for the repository.\n\n        Returns\n        -------\n        bool\n            True if the repository exists, False otherwise.\n        \"\"\"\n        return PyRepository.exists(storage)\n\n    @staticmethod\n    def fetch_config(storage: Storage) -&gt; RepositoryConfig | None:\n        \"\"\"\n        Fetch the configuration for the repository saved in storage.\n\n        Parameters\n        ----------\n        storage : Storage\n            The storage configuration for the repository.\n\n        Returns\n        -------\n        RepositoryConfig | None\n            The repository configuration if it exists, None otherwise.\n        \"\"\"\n        return PyRepository.fetch_config(storage)\n\n    def save_config(self) -&gt; None:\n        \"\"\"\n        Save the repository configuration to storage, this configuration will be used in future calls to Repository.open.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        return self._repository.save_config()\n\n    @property\n    def config(self) -&gt; RepositoryConfig:\n        \"\"\"\n        Get a copy of this repository's config.\n\n        Returns\n        -------\n        RepositoryConfig\n            The repository configuration.\n        \"\"\"\n        return self._repository.config()\n\n    @property\n    def storage(self) -&gt; Storage:\n        \"\"\"\n        Get a copy of this repository's Storage instance.\n\n        Returns\n        -------\n        Storage\n            The repository storage instance.\n        \"\"\"\n        return self._repository.storage()\n\n    def ancestry(\n        self,\n        *,\n        branch: str | None = None,\n        tag: str | None = None,\n        snapshot: str | None = None,\n    ) -&gt; list[SnapshotInfo]:\n        \"\"\"\n        Get the ancestry of a snapshot.\n\n        Parameters\n        ----------\n        branch : str, optional\n            The branch to get the ancestry of.\n        tag : str, optional\n            The tag to get the ancestry of.\n        snapshot : str, optional\n            The snapshot ID to get the ancestry of.\n\n        Returns\n        -------\n        list[SnapshotInfo]\n            The ancestry of the snapshot, listing out the snapshots and their metadata.\n\n        Notes\n        -----\n        Only one of the arguments can be specified.\n        \"\"\"\n        return self._repository.ancestry(branch=branch, tag=tag, snapshot=snapshot)\n\n    def async_ancestry(\n        self,\n        *,\n        branch: str | None = None,\n        tag: str | None = None,\n        snapshot: str | None = None,\n    ) -&gt; AsyncIterator[SnapshotInfo]:\n        \"\"\"\n        Get the ancestry of a snapshot.\n\n        Parameters\n        ----------\n        branch : str, optional\n            The branch to get the ancestry of.\n        tag : str, optional\n            The tag to get the ancestry of.\n        snapshot : str, optional\n            The snapshot ID to get the ancestry of.\n\n        Returns\n        -------\n        list[SnapshotInfo]\n            The ancestry of the snapshot, listing out the snapshots and their metadata.\n\n        Notes\n        -----\n        Only one of the arguments can be specified.\n        \"\"\"\n        return self._repository.async_ancestry(branch=branch, tag=tag, snapshot=snapshot)\n\n    def create_branch(self, branch: str, snapshot_id: str) -&gt; None:\n        \"\"\"\n        Create a new branch at the given snapshot.\n\n        Parameters\n        ----------\n        branch : str\n            The name of the branch to create.\n        snapshot_id : str\n            The snapshot ID to create the branch at.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self._repository.create_branch(branch, snapshot_id)\n\n    def list_branches(self) -&gt; set[str]:\n        \"\"\"\n        List the branches in the repository.\n\n        Returns\n        -------\n        set[str]\n            A set of branch names.\n        \"\"\"\n        return self._repository.list_branches()\n\n    def lookup_branch(self, branch: str) -&gt; str:\n        \"\"\"\n        Get the tip snapshot ID of a branch.\n\n        Parameters\n        ----------\n        branch : str\n            The branch to get the tip of.\n\n        Returns\n        -------\n        str\n            The snapshot ID of the tip of the branch.\n        \"\"\"\n        return self._repository.lookup_branch(branch)\n\n    def reset_branch(self, branch: str, snapshot_id: str) -&gt; None:\n        \"\"\"\n        Reset a branch to a specific snapshot.\n\n        This will permanently alter the history of the branch such that the tip of\n        the branch is the specified snapshot.\n\n        Parameters\n        ----------\n        branch : str\n            The branch to reset.\n        snapshot_id : str\n            The snapshot ID to reset the branch to.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self._repository.reset_branch(branch, snapshot_id)\n\n    def delete_branch(self, branch: str) -&gt; None:\n        \"\"\"\n        Delete a branch.\n\n        Parameters\n        ----------\n        branch : str\n            The branch to delete.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self._repository.delete_branch(branch)\n\n    def delete_tag(self, branch: str) -&gt; None:\n        \"\"\"\n        Delete a tag.\n\n        Parameters\n        ----------\n        tag : str\n            The tag to delete.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self._repository.delete_tag(branch)\n\n    def create_tag(self, tag: str, snapshot_id: str) -&gt; None:\n        \"\"\"\n        Create a new tag at the given snapshot.\n\n        Parameters\n        ----------\n        tag : str\n            The name of the tag to create.\n        snapshot_id : str\n            The snapshot ID to create the tag at.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self._repository.create_tag(tag, snapshot_id)\n\n    def list_tags(self) -&gt; set[str]:\n        \"\"\"\n        List the tags in the repository.\n\n        Returns\n        -------\n        set[str]\n            A set of tag names.\n        \"\"\"\n        return self._repository.list_tags()\n\n    def lookup_tag(self, tag: str) -&gt; str:\n        \"\"\"\n        Get the snapshot ID of a tag.\n\n        Parameters\n        ----------\n        tag : str\n            The tag to get the snapshot ID of.\n\n        Returns\n        -------\n        str\n            The snapshot ID of the tag.\n        \"\"\"\n        return self._repository.lookup_tag(tag)\n\n    def readonly_session(\n        self,\n        *,\n        branch: str | None = None,\n        tag: str | None = None,\n        snapshot: str | None = None,\n    ) -&gt; Session:\n        \"\"\"\n        Create a read-only session.\n\n        This can be thought of as a read-only checkout of the repository at a given snapshot.\n        When branch or tag are provided, the session will be based on the tip of the branch or\n        the snapshot ID of the tag.\n\n        Parameters\n        ----------\n        branch : str, optional\n            If provided, the branch to create the session on.\n        tag : str, optional\n            If provided, the tag to create the session on.\n        snapshot : str, optional\n            If provided, the snapshot ID to create the session on.\n\n        Returns\n        -------\n        Session\n            The read-only session, pointing to the specified snapshot, tag, or branch.\n\n        Notes\n        -----\n        Only one of the arguments can be specified.\n        \"\"\"\n        return Session(\n            self._repository.readonly_session(branch=branch, tag=tag, snapshot=snapshot)\n        )\n\n    def writable_session(self, branch: str) -&gt; Session:\n        \"\"\"\n        Create a writable session on a branch.\n\n        Like the read-only session, this can be thought of as a checkout of the repository at the\n        tip of the branch. However, this session is writable and can be used to make changes to the\n        repository. When ready, the changes can be committed to the branch, after which the session will\n        become a read-only session on the new snapshot.\n\n        Parameters\n        ----------\n        branch : str\n            The branch to create the session on.\n\n        Returns\n        -------\n        Session\n            The writable session on the branch.\n        \"\"\"\n        return Session(self._repository.writable_session(branch))\n\n    def expire_snapshots(self, older_than: datetime.datetime) -&gt; set[str]:\n        \"\"\"Expire all snapshots older than a threshold.\n\n        This processes snapshots found by navigating all references in\n        the repo, tags first, branches leter, both in lexicographical order.\n\n        Returns the ids of all snapshots considered expired and skipped\n        from history. Notice that this snapshot are not necessarily\n        available for garbage collection, they could still be pointed by\n        ether refs.\n\n        Warning: this is an administrative operation, it should be run\n        carefully. The repository can still operate concurrently while\n        `expire_snapshots` runs, but other readers can get inconsistent\n        views of the repository history.\n        \"\"\"\n\n        return self._repository.expire_snapshots(older_than)\n\n    def garbage_collect(self, delete_object_older_than: datetime.datetime) -&gt; GCSummary:\n        \"\"\"Delete any objects no longer accessible from any branches or tags.\n\n        Warning: this is an administrative operation, it should be run\n        carefully. The repository can still operate concurrently while\n        `garbage_collect` runs, but other reades can get inconsistent\n        views if they are trying to access the expired snapshots.\n        \"\"\"\n\n        return self._repository.garbage_collect(delete_object_older_than)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.config","title":"<code>config: RepositoryConfig</code>  <code>property</code>","text":"<p>Get a copy of this repository's config.</p> <p>Returns:</p> Type Description <code>RepositoryConfig</code> <p>The repository configuration.</p>"},{"location":"icechunk-python/reference/#icechunk.Repository.storage","title":"<code>storage: Storage</code>  <code>property</code>","text":"<p>Get a copy of this repository's Storage instance.</p> <p>Returns:</p> Type Description <code>Storage</code> <p>The repository storage instance.</p>"},{"location":"icechunk-python/reference/#icechunk.Repository.ancestry","title":"<code>ancestry(*, branch=None, tag=None, snapshot=None)</code>","text":"<p>Get the ancestry of a snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to get the ancestry of.</p> <code>None</code> <code>tag</code> <code>str</code> <p>The tag to get the ancestry of.</p> <code>None</code> <code>snapshot</code> <code>str</code> <p>The snapshot ID to get the ancestry of.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SnapshotInfo]</code> <p>The ancestry of the snapshot, listing out the snapshots and their metadata.</p> Notes <p>Only one of the arguments can be specified.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def ancestry(\n    self,\n    *,\n    branch: str | None = None,\n    tag: str | None = None,\n    snapshot: str | None = None,\n) -&gt; list[SnapshotInfo]:\n    \"\"\"\n    Get the ancestry of a snapshot.\n\n    Parameters\n    ----------\n    branch : str, optional\n        The branch to get the ancestry of.\n    tag : str, optional\n        The tag to get the ancestry of.\n    snapshot : str, optional\n        The snapshot ID to get the ancestry of.\n\n    Returns\n    -------\n    list[SnapshotInfo]\n        The ancestry of the snapshot, listing out the snapshots and their metadata.\n\n    Notes\n    -----\n    Only one of the arguments can be specified.\n    \"\"\"\n    return self._repository.ancestry(branch=branch, tag=tag, snapshot=snapshot)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.async_ancestry","title":"<code>async_ancestry(*, branch=None, tag=None, snapshot=None)</code>","text":"<p>Get the ancestry of a snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to get the ancestry of.</p> <code>None</code> <code>tag</code> <code>str</code> <p>The tag to get the ancestry of.</p> <code>None</code> <code>snapshot</code> <code>str</code> <p>The snapshot ID to get the ancestry of.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SnapshotInfo]</code> <p>The ancestry of the snapshot, listing out the snapshots and their metadata.</p> Notes <p>Only one of the arguments can be specified.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def async_ancestry(\n    self,\n    *,\n    branch: str | None = None,\n    tag: str | None = None,\n    snapshot: str | None = None,\n) -&gt; AsyncIterator[SnapshotInfo]:\n    \"\"\"\n    Get the ancestry of a snapshot.\n\n    Parameters\n    ----------\n    branch : str, optional\n        The branch to get the ancestry of.\n    tag : str, optional\n        The tag to get the ancestry of.\n    snapshot : str, optional\n        The snapshot ID to get the ancestry of.\n\n    Returns\n    -------\n    list[SnapshotInfo]\n        The ancestry of the snapshot, listing out the snapshots and their metadata.\n\n    Notes\n    -----\n    Only one of the arguments can be specified.\n    \"\"\"\n    return self._repository.async_ancestry(branch=branch, tag=tag, snapshot=snapshot)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.create","title":"<code>create(storage, config=None, virtual_chunk_credentials=None)</code>  <code>classmethod</code>","text":"<p>Create a new Icechunk repository. If one already exists at the given store location, an error will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>Storage</code> <p>The storage configuration for the repository.</p> required <code>config</code> <code>RepositoryConfig</code> <p>The repository configuration. If not provided, a default configuration will be used.</p> <code>None</code> <code>virtual_chunk_credentials</code> <code>dict[str, AnyCredential]</code> <p>Credentials for virtual chunks.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the Repository class.</p> Source code in <code>icechunk/repository.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    storage: Storage,\n    config: RepositoryConfig | None = None,\n    virtual_chunk_credentials: dict[str, AnyCredential] | None = None,\n) -&gt; Self:\n    \"\"\"\n    Create a new Icechunk repository.\n    If one already exists at the given store location, an error will be raised.\n\n    Parameters\n    ----------\n    storage : Storage\n        The storage configuration for the repository.\n    config : RepositoryConfig, optional\n        The repository configuration. If not provided, a default configuration will be used.\n    virtual_chunk_credentials : dict[str, AnyCredential], optional\n        Credentials for virtual chunks.\n\n    Returns\n    -------\n    Self\n        An instance of the Repository class.\n    \"\"\"\n    return cls(\n        PyRepository.create(\n            storage,\n            config=config,\n            virtual_chunk_credentials=virtual_chunk_credentials,\n        )\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.create_branch","title":"<code>create_branch(branch, snapshot_id)</code>","text":"<p>Create a new branch at the given snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The name of the branch to create.</p> required <code>snapshot_id</code> <code>str</code> <p>The snapshot ID to create the branch at.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/repository.py</code> <pre><code>def create_branch(self, branch: str, snapshot_id: str) -&gt; None:\n    \"\"\"\n    Create a new branch at the given snapshot.\n\n    Parameters\n    ----------\n    branch : str\n        The name of the branch to create.\n    snapshot_id : str\n        The snapshot ID to create the branch at.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    self._repository.create_branch(branch, snapshot_id)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.create_tag","title":"<code>create_tag(tag, snapshot_id)</code>","text":"<p>Create a new tag at the given snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>The name of the tag to create.</p> required <code>snapshot_id</code> <code>str</code> <p>The snapshot ID to create the tag at.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/repository.py</code> <pre><code>def create_tag(self, tag: str, snapshot_id: str) -&gt; None:\n    \"\"\"\n    Create a new tag at the given snapshot.\n\n    Parameters\n    ----------\n    tag : str\n        The name of the tag to create.\n    snapshot_id : str\n        The snapshot ID to create the tag at.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    self._repository.create_tag(tag, snapshot_id)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.delete_branch","title":"<code>delete_branch(branch)</code>","text":"<p>Delete a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to delete.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/repository.py</code> <pre><code>def delete_branch(self, branch: str) -&gt; None:\n    \"\"\"\n    Delete a branch.\n\n    Parameters\n    ----------\n    branch : str\n        The branch to delete.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    self._repository.delete_branch(branch)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.delete_tag","title":"<code>delete_tag(branch)</code>","text":"<p>Delete a tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>The tag to delete.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/repository.py</code> <pre><code>def delete_tag(self, branch: str) -&gt; None:\n    \"\"\"\n    Delete a tag.\n\n    Parameters\n    ----------\n    tag : str\n        The tag to delete.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    self._repository.delete_tag(branch)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.exists","title":"<code>exists(storage)</code>  <code>staticmethod</code>","text":"<p>Check if a repository exists at the given storage location.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>Storage</code> <p>The storage configuration for the repository.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the repository exists, False otherwise.</p> Source code in <code>icechunk/repository.py</code> <pre><code>@staticmethod\ndef exists(storage: Storage) -&gt; bool:\n    \"\"\"\n    Check if a repository exists at the given storage location.\n\n    Parameters\n    ----------\n    storage : Storage\n        The storage configuration for the repository.\n\n    Returns\n    -------\n    bool\n        True if the repository exists, False otherwise.\n    \"\"\"\n    return PyRepository.exists(storage)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.expire_snapshots","title":"<code>expire_snapshots(older_than)</code>","text":"<p>Expire all snapshots older than a threshold.</p> <p>This processes snapshots found by navigating all references in the repo, tags first, branches leter, both in lexicographical order.</p> <p>Returns the ids of all snapshots considered expired and skipped from history. Notice that this snapshot are not necessarily available for garbage collection, they could still be pointed by ether refs.</p> <p>Warning: this is an administrative operation, it should be run carefully. The repository can still operate concurrently while <code>expire_snapshots</code> runs, but other readers can get inconsistent views of the repository history.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def expire_snapshots(self, older_than: datetime.datetime) -&gt; set[str]:\n    \"\"\"Expire all snapshots older than a threshold.\n\n    This processes snapshots found by navigating all references in\n    the repo, tags first, branches leter, both in lexicographical order.\n\n    Returns the ids of all snapshots considered expired and skipped\n    from history. Notice that this snapshot are not necessarily\n    available for garbage collection, they could still be pointed by\n    ether refs.\n\n    Warning: this is an administrative operation, it should be run\n    carefully. The repository can still operate concurrently while\n    `expire_snapshots` runs, but other readers can get inconsistent\n    views of the repository history.\n    \"\"\"\n\n    return self._repository.expire_snapshots(older_than)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.fetch_config","title":"<code>fetch_config(storage)</code>  <code>staticmethod</code>","text":"<p>Fetch the configuration for the repository saved in storage.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>Storage</code> <p>The storage configuration for the repository.</p> required <p>Returns:</p> Type Description <code>RepositoryConfig | None</code> <p>The repository configuration if it exists, None otherwise.</p> Source code in <code>icechunk/repository.py</code> <pre><code>@staticmethod\ndef fetch_config(storage: Storage) -&gt; RepositoryConfig | None:\n    \"\"\"\n    Fetch the configuration for the repository saved in storage.\n\n    Parameters\n    ----------\n    storage : Storage\n        The storage configuration for the repository.\n\n    Returns\n    -------\n    RepositoryConfig | None\n        The repository configuration if it exists, None otherwise.\n    \"\"\"\n    return PyRepository.fetch_config(storage)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.garbage_collect","title":"<code>garbage_collect(delete_object_older_than)</code>","text":"<p>Delete any objects no longer accessible from any branches or tags.</p> <p>Warning: this is an administrative operation, it should be run carefully. The repository can still operate concurrently while <code>garbage_collect</code> runs, but other reades can get inconsistent views if they are trying to access the expired snapshots.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def garbage_collect(self, delete_object_older_than: datetime.datetime) -&gt; GCSummary:\n    \"\"\"Delete any objects no longer accessible from any branches or tags.\n\n    Warning: this is an administrative operation, it should be run\n    carefully. The repository can still operate concurrently while\n    `garbage_collect` runs, but other reades can get inconsistent\n    views if they are trying to access the expired snapshots.\n    \"\"\"\n\n    return self._repository.garbage_collect(delete_object_older_than)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.list_branches","title":"<code>list_branches()</code>","text":"<p>List the branches in the repository.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of branch names.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def list_branches(self) -&gt; set[str]:\n    \"\"\"\n    List the branches in the repository.\n\n    Returns\n    -------\n    set[str]\n        A set of branch names.\n    \"\"\"\n    return self._repository.list_branches()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.list_tags","title":"<code>list_tags()</code>","text":"<p>List the tags in the repository.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of tag names.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def list_tags(self) -&gt; set[str]:\n    \"\"\"\n    List the tags in the repository.\n\n    Returns\n    -------\n    set[str]\n        A set of tag names.\n    \"\"\"\n    return self._repository.list_tags()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.lookup_branch","title":"<code>lookup_branch(branch)</code>","text":"<p>Get the tip snapshot ID of a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to get the tip of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The snapshot ID of the tip of the branch.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def lookup_branch(self, branch: str) -&gt; str:\n    \"\"\"\n    Get the tip snapshot ID of a branch.\n\n    Parameters\n    ----------\n    branch : str\n        The branch to get the tip of.\n\n    Returns\n    -------\n    str\n        The snapshot ID of the tip of the branch.\n    \"\"\"\n    return self._repository.lookup_branch(branch)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.lookup_tag","title":"<code>lookup_tag(tag)</code>","text":"<p>Get the snapshot ID of a tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>The tag to get the snapshot ID of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The snapshot ID of the tag.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def lookup_tag(self, tag: str) -&gt; str:\n    \"\"\"\n    Get the snapshot ID of a tag.\n\n    Parameters\n    ----------\n    tag : str\n        The tag to get the snapshot ID of.\n\n    Returns\n    -------\n    str\n        The snapshot ID of the tag.\n    \"\"\"\n    return self._repository.lookup_tag(tag)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.open","title":"<code>open(storage, config=None, virtual_chunk_credentials=None)</code>  <code>classmethod</code>","text":"<p>Open an existing Icechunk repository.</p> <p>If no repository exists at the given storage location, an error will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>Storage</code> <p>The storage configuration for the repository.</p> required <code>config</code> <code>RepositoryConfig</code> <p>The repository settings. If not provided, a default configuration will be loaded from the repository.</p> <code>None</code> <code>virtual_chunk_credentials</code> <code>dict[str, AnyCredential]</code> <p>Credentials for virtual chunks.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the Repository class.</p> Source code in <code>icechunk/repository.py</code> <pre><code>@classmethod\ndef open(\n    cls,\n    storage: Storage,\n    config: RepositoryConfig | None = None,\n    virtual_chunk_credentials: dict[str, AnyCredential] | None = None,\n) -&gt; Self:\n    \"\"\"\n    Open an existing Icechunk repository.\n\n    If no repository exists at the given storage location, an error will be raised.\n\n    Parameters\n    ----------\n    storage : Storage\n        The storage configuration for the repository.\n    config : RepositoryConfig, optional\n        The repository settings. If not provided, a default configuration will be\n        loaded from the repository.\n    virtual_chunk_credentials : dict[str, AnyCredential], optional\n        Credentials for virtual chunks.\n\n    Returns\n    -------\n    Self\n        An instance of the Repository class.\n    \"\"\"\n    return cls(\n        PyRepository.open(\n            storage,\n            config=config,\n            virtual_chunk_credentials=virtual_chunk_credentials,\n        )\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.open_or_create","title":"<code>open_or_create(storage, config=None, virtual_chunk_credentials=None)</code>  <code>classmethod</code>","text":"<p>Open an existing Icechunk repository or create a new one if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>Storage</code> <p>The storage configuration for the repository.</p> required <code>config</code> <code>RepositoryConfig</code> <p>The repository settings. If not provided, a default configuration will be loaded from the repository.</p> <code>None</code> <code>virtual_chunk_credentials</code> <code>dict[str, AnyCredential]</code> <p>Credentials for virtual chunks.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>An instance of the Repository class.</p> Source code in <code>icechunk/repository.py</code> <pre><code>@classmethod\ndef open_or_create(\n    cls,\n    storage: Storage,\n    config: RepositoryConfig | None = None,\n    virtual_chunk_credentials: dict[str, AnyCredential] | None = None,\n) -&gt; Self:\n    \"\"\"\n    Open an existing Icechunk repository or create a new one if it does not exist.\n\n    Parameters\n    ----------\n    storage : Storage\n        The storage configuration for the repository.\n    config : RepositoryConfig, optional\n        The repository settings. If not provided, a default configuration will be\n        loaded from the repository.\n    virtual_chunk_credentials : dict[str, AnyCredential], optional\n        Credentials for virtual chunks.\n\n    Returns\n    -------\n    Self\n        An instance of the Repository class.\n    \"\"\"\n    return cls(\n        PyRepository.open_or_create(\n            storage,\n            config=config,\n            virtual_chunk_credentials=virtual_chunk_credentials,\n        )\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.readonly_session","title":"<code>readonly_session(*, branch=None, tag=None, snapshot=None)</code>","text":"<p>Create a read-only session.</p> <p>This can be thought of as a read-only checkout of the repository at a given snapshot. When branch or tag are provided, the session will be based on the tip of the branch or the snapshot ID of the tag.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>If provided, the branch to create the session on.</p> <code>None</code> <code>tag</code> <code>str</code> <p>If provided, the tag to create the session on.</p> <code>None</code> <code>snapshot</code> <code>str</code> <p>If provided, the snapshot ID to create the session on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>The read-only session, pointing to the specified snapshot, tag, or branch.</p> Notes <p>Only one of the arguments can be specified.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def readonly_session(\n    self,\n    *,\n    branch: str | None = None,\n    tag: str | None = None,\n    snapshot: str | None = None,\n) -&gt; Session:\n    \"\"\"\n    Create a read-only session.\n\n    This can be thought of as a read-only checkout of the repository at a given snapshot.\n    When branch or tag are provided, the session will be based on the tip of the branch or\n    the snapshot ID of the tag.\n\n    Parameters\n    ----------\n    branch : str, optional\n        If provided, the branch to create the session on.\n    tag : str, optional\n        If provided, the tag to create the session on.\n    snapshot : str, optional\n        If provided, the snapshot ID to create the session on.\n\n    Returns\n    -------\n    Session\n        The read-only session, pointing to the specified snapshot, tag, or branch.\n\n    Notes\n    -----\n    Only one of the arguments can be specified.\n    \"\"\"\n    return Session(\n        self._repository.readonly_session(branch=branch, tag=tag, snapshot=snapshot)\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.reset_branch","title":"<code>reset_branch(branch, snapshot_id)</code>","text":"<p>Reset a branch to a specific snapshot.</p> <p>This will permanently alter the history of the branch such that the tip of the branch is the specified snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to reset.</p> required <code>snapshot_id</code> <code>str</code> <p>The snapshot ID to reset the branch to.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/repository.py</code> <pre><code>def reset_branch(self, branch: str, snapshot_id: str) -&gt; None:\n    \"\"\"\n    Reset a branch to a specific snapshot.\n\n    This will permanently alter the history of the branch such that the tip of\n    the branch is the specified snapshot.\n\n    Parameters\n    ----------\n    branch : str\n        The branch to reset.\n    snapshot_id : str\n        The snapshot ID to reset the branch to.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    self._repository.reset_branch(branch, snapshot_id)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.save_config","title":"<code>save_config()</code>","text":"<p>Save the repository configuration to storage, this configuration will be used in future calls to Repository.open.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/repository.py</code> <pre><code>def save_config(self) -&gt; None:\n    \"\"\"\n    Save the repository configuration to storage, this configuration will be used in future calls to Repository.open.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    return self._repository.save_config()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Repository.writable_session","title":"<code>writable_session(branch)</code>","text":"<p>Create a writable session on a branch.</p> <p>Like the read-only session, this can be thought of as a checkout of the repository at the tip of the branch. However, this session is writable and can be used to make changes to the repository. When ready, the changes can be committed to the branch, after which the session will become a read-only session on the new snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>branch</code> <code>str</code> <p>The branch to create the session on.</p> required <p>Returns:</p> Type Description <code>Session</code> <p>The writable session on the branch.</p> Source code in <code>icechunk/repository.py</code> <pre><code>def writable_session(self, branch: str) -&gt; Session:\n    \"\"\"\n    Create a writable session on a branch.\n\n    Like the read-only session, this can be thought of as a checkout of the repository at the\n    tip of the branch. However, this session is writable and can be used to make changes to the\n    repository. When ready, the changes can be committed to the branch, after which the session will\n    become a read-only session on the new snapshot.\n\n    Parameters\n    ----------\n    branch : str\n        The branch to create the session on.\n\n    Returns\n    -------\n    Session\n        The writable session on the branch.\n    \"\"\"\n    return Session(self._repository.writable_session(branch))\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.RepositoryConfig","title":"<code>RepositoryConfig</code>","text":"<p>Configuration for an Icechunk repository</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class RepositoryConfig:\n    \"\"\"Configuration for an Icechunk repository\"\"\"\n\n    def __init__(\n        self,\n        inline_chunk_threshold_bytes: int | None = None,\n        unsafe_overwrite_refs: bool | None = None,\n        get_partial_values_concurrency: int | None = None,\n        compression: CompressionConfig | None = None,\n        caching: CachingConfig | None = None,\n        storage: StorageSettings | None = None,\n        virtual_chunk_containers: dict[str, VirtualChunkContainer] | None = None,\n        manifest: ManifestConfig | None = None,\n    ) -&gt; None: ...\n    @staticmethod\n    def default() -&gt; RepositoryConfig: ...\n    @property\n    def inline_chunk_threshold_bytes(self) -&gt; int | None: ...\n    @inline_chunk_threshold_bytes.setter\n    def inline_chunk_threshold_bytes(self, value: int | None) -&gt; None: ...\n    @property\n    def unsafe_overwrite_refs(self) -&gt; bool | None: ...\n    @unsafe_overwrite_refs.setter\n    def unsafe_overwrite_refs(self, value: bool | None) -&gt; None: ...\n    @property\n    def get_partial_values_concurrency(self) -&gt; int | None: ...\n    @get_partial_values_concurrency.setter\n    def get_partial_values_concurrency(self, value: int | None) -&gt; None: ...\n    @property\n    def compression(self) -&gt; CompressionConfig | None: ...\n    @compression.setter\n    def compression(self, value: CompressionConfig | None) -&gt; None: ...\n    @property\n    def caching(self) -&gt; CachingConfig | None: ...\n    @caching.setter\n    def caching(self, value: CachingConfig | None) -&gt; None: ...\n    @property\n    def storage(self) -&gt; StorageSettings | None: ...\n    @storage.setter\n    def storage(self, value: StorageSettings | None) -&gt; None: ...\n    @property\n    def manifest(self) -&gt; ManifestConfig | None: ...\n    @manifest.setter\n    def manifest(self, value: ManifestConfig | None) -&gt; None: ...\n    @property\n    def virtual_chunk_containers(self) -&gt; dict[str, VirtualChunkContainer] | None: ...\n    def get_virtual_chunk_container(self, name: str) -&gt; VirtualChunkContainer | None: ...\n    def set_virtual_chunk_container(self, cont: VirtualChunkContainer) -&gt; None: ...\n    def clear_virtual_chunk_containers(self) -&gt; None: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session","title":"<code>Session</code>","text":"<p>A session object that allows for reading and writing data from an Icechunk repository.</p> Source code in <code>icechunk/session.py</code> <pre><code>class Session:\n    \"\"\"A session object that allows for reading and writing data from an Icechunk repository.\"\"\"\n\n    _session: PySession\n    _allow_pickling: bool\n\n    def __init__(self, session: PySession, _allow_pickling: bool = False):\n        self._session = session\n        self._allow_pickling = _allow_pickling\n\n    def __eq__(self, value: object) -&gt; bool:\n        if not isinstance(value, Session):\n            return False\n        return self._session == value._session\n\n    def __getstate__(self) -&gt; object:\n        if not self._allow_pickling and not self.read_only:\n            raise ValueError(\n                \"You must opt-in to pickle writable sessions in a distributed context \"\n                \"using the `Session.allow_pickling` context manager. \"\n                # link to docs\n                \"If you are using xarray's `Dataset.to_zarr` method with dask arrays, \"\n                \"please consider `icechunk.xarray.to_icechunk` instead.\"\n            )\n        state = {\n            \"_session\": self._session.as_bytes(),\n            \"_allow_pickling\": self._allow_pickling,\n        }\n        return state\n\n    def __setstate__(self, state: object) -&gt; None:\n        if not isinstance(state, dict):\n            raise ValueError(\"Invalid state\")\n        self._session = PySession.from_bytes(state[\"_session\"])\n        self._allow_pickling = state[\"_allow_pickling\"]\n\n    @contextlib.contextmanager\n    def allow_pickling(self) -&gt; Generator[None, None, None]:\n        \"\"\"\n        Context manager to allow unpickling this store if writable.\n        \"\"\"\n        try:\n            self._allow_pickling = True\n            yield\n        finally:\n            self._allow_pickling = False\n\n    @property\n    def read_only(self) -&gt; bool:\n        \"\"\"\n        Whether the session is read-only.\n\n        Returns\n        -------\n        bool\n            True if the session is read-only, False otherwise.\n        \"\"\"\n        return self._session.read_only\n\n    @property\n    def snapshot_id(self) -&gt; str:\n        \"\"\"\n        The base snapshot ID of the session.\n\n        Returns\n        -------\n        str\n            The base snapshot ID of the session.\n        \"\"\"\n        return self._session.snapshot_id\n\n    @property\n    def branch(self) -&gt; str | None:\n        \"\"\"\n        The branch that the session is based on. This is only set if the session is writable.\n\n        Returns\n        -------\n        str or None\n            The branch that the session is based on if the session is writable, None otherwise.\n        \"\"\"\n        return self._session.branch\n\n    @property\n    def has_uncommitted_changes(self) -&gt; bool:\n        \"\"\"\n        Whether the session has uncommitted changes. This is only possibly true if the session is writable.\n\n        Returns\n        -------\n        bool\n            True if the session has uncommitted changes, False otherwise.\n        \"\"\"\n        return self._session.has_uncommitted_changes\n\n    def discard_changes(self) -&gt; None:\n        \"\"\"\n        When the session is writable, discard any uncommitted changes.\n        \"\"\"\n        self._session.discard_changes()\n\n    @property\n    def store(self) -&gt; IcechunkStore:\n        \"\"\"\n        Get a zarr Store object for reading and writing data from the repository using zarr python.\n\n        Returns\n        -------\n        IcechunkStore\n            A zarr Store object for reading and writing data from the repository.\n        \"\"\"\n        return IcechunkStore(self._session.store, self._allow_pickling)\n\n    def all_virtual_chunk_locations(self) -&gt; list[str]:\n        \"\"\"\n        Return the location URLs of all virtual chunks.\n\n        Returns\n        -------\n        list of str\n            The location URLs of all virtual chunks.\n        \"\"\"\n        return self._session.all_virtual_chunk_locations()\n\n    async def chunk_coordinates(\n        self, array_path: str, batch_size: int = 1000\n    ) -&gt; AsyncIterator[tuple[int, ...]]:\n        \"\"\"\n        Return an async iterator to all initialized chunks for the array at array_path\n\n        Returns\n        -------\n        an async iterator to chunk coordinates as tuples\n        \"\"\"\n        # We do unbatching here to improve speed. Switching to rust to get\n        # a batch is much faster than switching for every element\n        async for batch in self._session.chunk_coordinates(array_path, batch_size):\n            for coord in batch:\n                yield tuple(coord)\n\n    def merge(self, other: Self) -&gt; None:\n        \"\"\"\n        Merge the changes for this session with the changes from another session.\n\n        Parameters\n        ----------\n        other : Self\n            The other session to merge changes from.\n        \"\"\"\n        self._session.merge(other._session)\n\n    def commit(self, message: str, metadata: dict[str, Any] | None = None) -&gt; str:\n        \"\"\"\n        Commit the changes in the session to the repository.\n\n        When successful, the writable session is completed and the session is now read-only and based on the new commit. The snapshot ID of the new commit is returned.\n\n        If the session is out of date, this will raise a ConflictError exception depicting the conflict that occurred. The session will need to be rebased before committing.\n\n        Parameters\n        ----------\n        message : str\n            The message to write with the commit.\n\n        Returns\n        -------\n        str\n            The snapshot ID of the new commit.\n\n        Raises\n        ------\n        ConflictError\n            If the session is out of date and a conflict occurs.\n        \"\"\"\n        try:\n            return self._session.commit(message, metadata)\n        except PyConflictError as e:\n            raise ConflictError(e) from None\n\n    def rebase(self, solver: ConflictSolver) -&gt; None:\n        \"\"\"\n        Rebase the session to the latest ancestry of the branch.\n\n        This method will iteratively crawl the ancestry of the branch and apply the changes from the branch to the session. If a conflict is detected, the conflict solver will be used to optionally resolve the conflict. When complete, the session will be based on the latest commit of the branch and the session will be ready to attempt another commit.\n\n        When a conflict is detected and a resolution is not possible with the provided solver, a RebaseFailed exception will be raised. This exception will contain the snapshot ID that the rebase failed on and a list of conflicts that occurred.\n\n        Parameters\n        ----------\n        solver : ConflictSolver\n            The conflict solver to use when a conflict is detected.\n\n        Raises\n        ------\n        RebaseFailedError\n            When a conflict is detected and the solver fails to resolve it.\n        \"\"\"\n        try:\n            self._session.rebase(solver)\n        except PyRebaseFailedError as e:\n            raise RebaseFailedError(e) from None\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.branch","title":"<code>branch: str | None</code>  <code>property</code>","text":"<p>The branch that the session is based on. This is only set if the session is writable.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The branch that the session is based on if the session is writable, None otherwise.</p>"},{"location":"icechunk-python/reference/#icechunk.Session.has_uncommitted_changes","title":"<code>has_uncommitted_changes: bool</code>  <code>property</code>","text":"<p>Whether the session has uncommitted changes. This is only possibly true if the session is writable.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the session has uncommitted changes, False otherwise.</p>"},{"location":"icechunk-python/reference/#icechunk.Session.read_only","title":"<code>read_only: bool</code>  <code>property</code>","text":"<p>Whether the session is read-only.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the session is read-only, False otherwise.</p>"},{"location":"icechunk-python/reference/#icechunk.Session.snapshot_id","title":"<code>snapshot_id: str</code>  <code>property</code>","text":"<p>The base snapshot ID of the session.</p> <p>Returns:</p> Type Description <code>str</code> <p>The base snapshot ID of the session.</p>"},{"location":"icechunk-python/reference/#icechunk.Session.store","title":"<code>store: IcechunkStore</code>  <code>property</code>","text":"<p>Get a zarr Store object for reading and writing data from the repository using zarr python.</p> <p>Returns:</p> Type Description <code>IcechunkStore</code> <p>A zarr Store object for reading and writing data from the repository.</p>"},{"location":"icechunk-python/reference/#icechunk.Session.all_virtual_chunk_locations","title":"<code>all_virtual_chunk_locations()</code>","text":"<p>Return the location URLs of all virtual chunks.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>The location URLs of all virtual chunks.</p> Source code in <code>icechunk/session.py</code> <pre><code>def all_virtual_chunk_locations(self) -&gt; list[str]:\n    \"\"\"\n    Return the location URLs of all virtual chunks.\n\n    Returns\n    -------\n    list of str\n        The location URLs of all virtual chunks.\n    \"\"\"\n    return self._session.all_virtual_chunk_locations()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.allow_pickling","title":"<code>allow_pickling()</code>","text":"<p>Context manager to allow unpickling this store if writable.</p> Source code in <code>icechunk/session.py</code> <pre><code>@contextlib.contextmanager\ndef allow_pickling(self) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager to allow unpickling this store if writable.\n    \"\"\"\n    try:\n        self._allow_pickling = True\n        yield\n    finally:\n        self._allow_pickling = False\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.chunk_coordinates","title":"<code>chunk_coordinates(array_path, batch_size=1000)</code>  <code>async</code>","text":"<p>Return an async iterator to all initialized chunks for the array at array_path</p> <p>Returns:</p> Type Description <code>an async iterator to chunk coordinates as tuples</code> Source code in <code>icechunk/session.py</code> <pre><code>async def chunk_coordinates(\n    self, array_path: str, batch_size: int = 1000\n) -&gt; AsyncIterator[tuple[int, ...]]:\n    \"\"\"\n    Return an async iterator to all initialized chunks for the array at array_path\n\n    Returns\n    -------\n    an async iterator to chunk coordinates as tuples\n    \"\"\"\n    # We do unbatching here to improve speed. Switching to rust to get\n    # a batch is much faster than switching for every element\n    async for batch in self._session.chunk_coordinates(array_path, batch_size):\n        for coord in batch:\n            yield tuple(coord)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.commit","title":"<code>commit(message, metadata=None)</code>","text":"<p>Commit the changes in the session to the repository.</p> <p>When successful, the writable session is completed and the session is now read-only and based on the new commit. The snapshot ID of the new commit is returned.</p> <p>If the session is out of date, this will raise a ConflictError exception depicting the conflict that occurred. The session will need to be rebased before committing.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to write with the commit.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The snapshot ID of the new commit.</p> <p>Raises:</p> Type Description <code>ConflictError</code> <p>If the session is out of date and a conflict occurs.</p> Source code in <code>icechunk/session.py</code> <pre><code>def commit(self, message: str, metadata: dict[str, Any] | None = None) -&gt; str:\n    \"\"\"\n    Commit the changes in the session to the repository.\n\n    When successful, the writable session is completed and the session is now read-only and based on the new commit. The snapshot ID of the new commit is returned.\n\n    If the session is out of date, this will raise a ConflictError exception depicting the conflict that occurred. The session will need to be rebased before committing.\n\n    Parameters\n    ----------\n    message : str\n        The message to write with the commit.\n\n    Returns\n    -------\n    str\n        The snapshot ID of the new commit.\n\n    Raises\n    ------\n    ConflictError\n        If the session is out of date and a conflict occurs.\n    \"\"\"\n    try:\n        return self._session.commit(message, metadata)\n    except PyConflictError as e:\n        raise ConflictError(e) from None\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.discard_changes","title":"<code>discard_changes()</code>","text":"<p>When the session is writable, discard any uncommitted changes.</p> Source code in <code>icechunk/session.py</code> <pre><code>def discard_changes(self) -&gt; None:\n    \"\"\"\n    When the session is writable, discard any uncommitted changes.\n    \"\"\"\n    self._session.discard_changes()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.merge","title":"<code>merge(other)</code>","text":"<p>Merge the changes for this session with the changes from another session.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>The other session to merge changes from.</p> required Source code in <code>icechunk/session.py</code> <pre><code>def merge(self, other: Self) -&gt; None:\n    \"\"\"\n    Merge the changes for this session with the changes from another session.\n\n    Parameters\n    ----------\n    other : Self\n        The other session to merge changes from.\n    \"\"\"\n    self._session.merge(other._session)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.Session.rebase","title":"<code>rebase(solver)</code>","text":"<p>Rebase the session to the latest ancestry of the branch.</p> <p>This method will iteratively crawl the ancestry of the branch and apply the changes from the branch to the session. If a conflict is detected, the conflict solver will be used to optionally resolve the conflict. When complete, the session will be based on the latest commit of the branch and the session will be ready to attempt another commit.</p> <p>When a conflict is detected and a resolution is not possible with the provided solver, a RebaseFailed exception will be raised. This exception will contain the snapshot ID that the rebase failed on and a list of conflicts that occurred.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>ConflictSolver</code> <p>The conflict solver to use when a conflict is detected.</p> required <p>Raises:</p> Type Description <code>RebaseFailedError</code> <p>When a conflict is detected and the solver fails to resolve it.</p> Source code in <code>icechunk/session.py</code> <pre><code>def rebase(self, solver: ConflictSolver) -&gt; None:\n    \"\"\"\n    Rebase the session to the latest ancestry of the branch.\n\n    This method will iteratively crawl the ancestry of the branch and apply the changes from the branch to the session. If a conflict is detected, the conflict solver will be used to optionally resolve the conflict. When complete, the session will be based on the latest commit of the branch and the session will be ready to attempt another commit.\n\n    When a conflict is detected and a resolution is not possible with the provided solver, a RebaseFailed exception will be raised. This exception will contain the snapshot ID that the rebase failed on and a list of conflicts that occurred.\n\n    Parameters\n    ----------\n    solver : ConflictSolver\n        The conflict solver to use when a conflict is detected.\n\n    Raises\n    ------\n    RebaseFailedError\n        When a conflict is detected and the solver fails to resolve it.\n    \"\"\"\n    try:\n        self._session.rebase(solver)\n    except PyRebaseFailedError as e:\n        raise RebaseFailedError(e) from None\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.SnapshotInfo","title":"<code>SnapshotInfo</code>","text":"<p>Metadata for a snapshot</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class SnapshotInfo:\n    \"\"\"Metadata for a snapshot\"\"\"\n    @property\n    def id(self) -&gt; str:\n        \"\"\"The snapshot ID\"\"\"\n        ...\n    @property\n    def parent_id(self) -&gt; str | None:\n        \"\"\"The snapshot ID\"\"\"\n        ...\n    @property\n    def written_at(self) -&gt; datetime.datetime:\n        \"\"\"\n        The timestamp when the snapshot was written\n        \"\"\"\n        ...\n    @property\n    def message(self) -&gt; str:\n        \"\"\"\n        The commit message of the snapshot\n        \"\"\"\n        ...\n    @property\n    def metadata(self) -&gt; dict[str, Any]:\n        \"\"\"\n        The metadata of the snapshot\n        \"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.SnapshotInfo.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>The snapshot ID</p>"},{"location":"icechunk-python/reference/#icechunk.SnapshotInfo.message","title":"<code>message: str</code>  <code>property</code>","text":"<p>The commit message of the snapshot</p>"},{"location":"icechunk-python/reference/#icechunk.SnapshotInfo.metadata","title":"<code>metadata: dict[str, Any]</code>  <code>property</code>","text":"<p>The metadata of the snapshot</p>"},{"location":"icechunk-python/reference/#icechunk.SnapshotInfo.parent_id","title":"<code>parent_id: str | None</code>  <code>property</code>","text":"<p>The snapshot ID</p>"},{"location":"icechunk-python/reference/#icechunk.SnapshotInfo.written_at","title":"<code>written_at: datetime.datetime</code>  <code>property</code>","text":"<p>The timestamp when the snapshot was written</p>"},{"location":"icechunk-python/reference/#icechunk.Storage","title":"<code>Storage</code>","text":"<p>Storage configuration for an IcechunkStore</p> <p>Currently supports memory, filesystem S3, azure blob, and google cloud storage backends. Use the following methods to create a Storage object with the desired backend.</p> <p>Ex: <pre><code>storage = icechunk.in_memory_storage()\nstorage = icechunk.local_filesystem_storage(\"/path/to/root\")\nstorage = icechunk.s3_storage(\"bucket\", \"prefix\", ...)\nstorage = icechunk.gcs_storage(\"bucket\", \"prefix\", ...)\nstorage = icechunk.azure_storage(\"container\", \"prefix\", ...)\n</code></pre></p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class Storage:\n    \"\"\"Storage configuration for an IcechunkStore\n\n    Currently supports memory, filesystem S3, azure blob, and google cloud storage backends.\n    Use the following methods to create a Storage object with the desired backend.\n\n    Ex:\n    ```\n    storage = icechunk.in_memory_storage()\n    storage = icechunk.local_filesystem_storage(\"/path/to/root\")\n    storage = icechunk.s3_storage(\"bucket\", \"prefix\", ...)\n    storage = icechunk.gcs_storage(\"bucket\", \"prefix\", ...)\n    storage = icechunk.azure_storage(\"container\", \"prefix\", ...)\n    ```\n    \"\"\"\n\n    @classmethod\n    def new_s3(\n        cls,\n        config: S3Options,\n        bucket: str,\n        prefix: str | None,\n        credentials: AnyS3Credential | None = None,\n    ) -&gt; Storage: ...\n    @classmethod\n    def new_tigris(\n        cls,\n        config: S3Options,\n        bucket: str,\n        prefix: str | None,\n        credentials: AnyS3Credential | None = None,\n    ) -&gt; Storage: ...\n    @classmethod\n    def new_in_memory(cls) -&gt; Storage: ...\n    @classmethod\n    def new_local_filesystem(cls, path: str) -&gt; Storage: ...\n    @classmethod\n    def new_gcs(\n        cls,\n        bucket: str,\n        prefix: str | None,\n        credentials: AnyGcsCredential | None = None,\n        *,\n        config: dict[str, str] | None = None,\n    ) -&gt; Storage: ...\n    @classmethod\n    def new_azure_blob(\n        cls,\n        container: str,\n        prefix: str,\n        credentials: AnyAzureCredential | None = None,\n        *,\n        config: dict[str, str] | None = None,\n    ) -&gt; Storage: ...\n    def default_settings(self) -&gt; StorageSettings: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConcurrencySettings","title":"<code>StorageConcurrencySettings</code>","text":"<p>Configuration for how Icechunk uses its Storage instance</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class StorageConcurrencySettings:\n    \"\"\"Configuration for how Icechunk uses its Storage instance\"\"\"\n\n    def __init__(\n        self,\n        max_concurrent_requests_for_object: int | None = None,\n        ideal_concurrent_request_size: int | None = None,\n    ) -&gt; None: ...\n    @property\n    def max_concurrent_requests_for_object(self) -&gt; int | None: ...\n    @max_concurrent_requests_for_object.setter\n    def max_concurrent_requests_for_object(self, value: int | None) -&gt; None: ...\n    @property\n    def ideal_concurrent_request_size(self) -&gt; int | None: ...\n    @ideal_concurrent_request_size.setter\n    def ideal_concurrent_request_size(self, value: int | None) -&gt; None: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageSettings","title":"<code>StorageSettings</code>","text":"<p>Configuration for how Icechunk uses its Storage instance</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class StorageSettings:\n    \"\"\"Configuration for how Icechunk uses its Storage instance\"\"\"\n\n    def __init__(self, concurrency: StorageConcurrencySettings | None = None) -&gt; None: ...\n    @property\n    def concurrency(self) -&gt; StorageConcurrencySettings | None: ...\n    @concurrency.setter\n    def concurrency(self, value: StorageConcurrencySettings | None) -&gt; None: ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.VersionSelection","title":"<code>VersionSelection</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for selecting the which version of a conflict</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class VersionSelection(Enum):\n    \"\"\"Enum for selecting the which version of a conflict\"\"\"\n\n    Fail = 0\n    UseOurs = 1\n    UseTheirs = 2\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.azure_credentials","title":"<code>azure_credentials(*, access_key=None, sas_token=None, bearer_token=None, from_env=None)</code>","text":"<p>Create credentials Azure Blob Storage object store.</p> <p>If all arguments are None, credentials are fetched from the operative system environment.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def azure_credentials(\n    *,\n    access_key: str | None = None,\n    sas_token: str | None = None,\n    bearer_token: str | None = None,\n    from_env: bool | None = None,\n) -&gt; AnyAzureCredential:\n    \"\"\"Create credentials Azure Blob Storage object store.\n\n    If all arguments are None, credentials are fetched from the operative system environment.\n    \"\"\"\n    if (from_env is None or from_env) and (\n        access_key is None and sas_token is None and bearer_token is None\n    ):\n        return azure_from_env_credentials()\n\n    if (access_key is not None or sas_token is not None or bearer_token is not None) and (\n        from_env is None or not from_env\n    ):\n        return AzureCredentials.Static(\n            azure_static_credentials(\n                access_key=access_key,\n                sas_token=sas_token,\n                bearer_token=bearer_token,\n            )\n        )\n\n    raise ValueError(\"Conflicting arguments to azure_credentials function\")\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.azure_from_env_credentials","title":"<code>azure_from_env_credentials()</code>","text":"<p>Instruct Azure Blob Storage object store to fetch credentials from the operative system environment.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def azure_from_env_credentials() -&gt; AzureCredentials.FromEnv:\n    \"\"\"Instruct Azure Blob Storage object store to fetch credentials from the operative system environment.\"\"\"\n    return AzureCredentials.FromEnv()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.azure_static_credentials","title":"<code>azure_static_credentials(*, access_key=None, sas_token=None, bearer_token=None)</code>","text":"<p>Create static credentials Azure Blob Storage object store.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def azure_static_credentials(\n    *,\n    access_key: str | None = None,\n    sas_token: str | None = None,\n    bearer_token: str | None = None,\n) -&gt; AnyAzureStaticCredential:\n    \"\"\"Create static credentials Azure Blob Storage object store.\"\"\"\n    if [access_key, sas_token, bearer_token].count(None) != 2:\n        raise ValueError(\"Conflicting arguments to azure_static_credentials function\")\n    if access_key is not None:\n        return AzureStaticCredentials.AccessKey(access_key)\n    if sas_token is not None:\n        return AzureStaticCredentials.SasToken(sas_token)\n    if bearer_token is not None:\n        return AzureStaticCredentials.BearerToken(bearer_token)\n    raise ValueError(\n        \"No valid static credential provided for Azure Blob Storage object store\"\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.azure_storage","title":"<code>azure_storage(*, container, prefix, access_key=None, sas_token=None, bearer_token=None, from_env=None, config=None)</code>","text":"<p>Create a Storage instance that saves data in Azure Blob Storage object store.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>str</code> <p>The container where the repository will store its data</p> required <code>prefix</code> <code>str</code> <p>The prefix within the container that is the root directory of the repository</p> required <code>access_key</code> <code>str | None</code> <p>Azure Blob Storage credential access key</p> <code>None</code> <code>sas_token</code> <code>str | None</code> <p>Azure Blob Storage credential SAS token</p> <code>None</code> <code>bearer_token</code> <code>str | None</code> <p>Azure Blob Storage credential bearer token</p> <code>None</code> <code>from_env</code> <code>bool | None</code> <p>Fetch credentials from the operative system environment</p> <code>None</code> Source code in <code>icechunk/storage.py</code> <pre><code>def azure_storage(\n    *,\n    container: str,\n    prefix: str,\n    access_key: str | None = None,\n    sas_token: str | None = None,\n    bearer_token: str | None = None,\n    from_env: bool | None = None,\n    config: dict[str, str] | None = None,\n) -&gt; Storage:\n    \"\"\"Create a Storage instance that saves data in Azure Blob Storage object store.\n\n    Parameters\n    ----------\n    container: str\n        The container where the repository will store its data\n    prefix: str\n        The prefix within the container that is the root directory of the repository\n    access_key: str | None\n        Azure Blob Storage credential access key\n    sas_token: str | None\n        Azure Blob Storage credential SAS token\n    bearer_token: str | None\n        Azure Blob Storage credential bearer token\n    from_env: bool | None\n        Fetch credentials from the operative system environment\n    \"\"\"\n    credentials = azure_credentials(\n        access_key=access_key,\n        sas_token=sas_token,\n        bearer_token=bearer_token,\n        from_env=from_env,\n    )\n    return Storage.new_azure_blob(\n        container=container,\n        prefix=prefix,\n        credentials=credentials,\n        config=config,\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.containers_credentials","title":"<code>containers_credentials(m={}, **kwargs)</code>","text":"<p>Build a map of credentials for virtual chunk containers.</p> <p>Example usage: <pre><code>import icechunk as ic\n\nconfig = ic.RepositoryConfig.default()\nconfig.inline_chunk_threshold_bytes = 512\n\nvirtual_store_config = ic.s3_store(\n    region=\"us-east-1\",\n    endpoint_url=\"http://localhost:9000\",\n    allow_http=True,\n    s3_compatible=True,\n)\ncontainer = ic.VirtualChunkContainer(\"s3\", \"s3://\", virtual_store_config)\nconfig.set_virtual_chunk_container(container)\ncredentials = ic.containers_credentials(\n    s3=ic.s3_credentials(access_key_id=\"ACCESS_KEY\", secret_access_key=\"SECRET\")\n)\n\nrepo = ic.Repository.create(\n    storage=ic.local_filesystem_storage(store_path),\n    config=config,\n    virtual_chunk_credentials=credentials,\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Mapping[str, AnyS3Credential]</code> <p>A mapping from container name to credentials.</p> <code>{}</code> Source code in <code>icechunk/credentials.py</code> <pre><code>def containers_credentials(\n    m: Mapping[str, AnyS3Credential] = {}, **kwargs: AnyS3Credential\n) -&gt; dict[str, Credentials.S3]:\n    \"\"\"Build a map of credentials for virtual chunk containers.\n\n    Example usage:\n    ```\n    import icechunk as ic\n\n    config = ic.RepositoryConfig.default()\n    config.inline_chunk_threshold_bytes = 512\n\n    virtual_store_config = ic.s3_store(\n        region=\"us-east-1\",\n        endpoint_url=\"http://localhost:9000\",\n        allow_http=True,\n        s3_compatible=True,\n    )\n    container = ic.VirtualChunkContainer(\"s3\", \"s3://\", virtual_store_config)\n    config.set_virtual_chunk_container(container)\n    credentials = ic.containers_credentials(\n        s3=ic.s3_credentials(access_key_id=\"ACCESS_KEY\", secret_access_key=\"SECRET\")\n    )\n\n    repo = ic.Repository.create(\n        storage=ic.local_filesystem_storage(store_path),\n        config=config,\n        virtual_chunk_credentials=credentials,\n    )\n    ```\n\n    Parameters\n    ----------\n    m: Mapping[str, AnyS3Credential]\n        A mapping from container name to credentials.\n    \"\"\"\n    res = {}\n    for name, cred in {**m, **kwargs}.items():\n        if isinstance(cred, AnyS3Credential):\n            res[name] = Credentials.S3(cred)\n        else:\n            raise ValueError(f\"Unknown credential type {type(cred)}\")\n    return res\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.gcs_credentials","title":"<code>gcs_credentials(*, service_account_file=None, service_account_key=None, application_credentials=None, from_env=None)</code>","text":"<p>Create credentials Google Cloud Storage object store.</p> <p>If all arguments are None, credentials are fetched from the operative system environment.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def gcs_credentials(\n    *,\n    service_account_file: str | None = None,\n    service_account_key: str | None = None,\n    application_credentials: str | None = None,\n    from_env: bool | None = None,\n) -&gt; AnyGcsCredential:\n    \"\"\"Create credentials Google Cloud Storage object store.\n\n    If all arguments are None, credentials are fetched from the operative system environment.\n    \"\"\"\n    if (from_env is None or from_env) and (\n        service_account_file is None\n        and service_account_key is None\n        and application_credentials is None\n    ):\n        return gcs_from_env_credentials()\n\n    if (\n        service_account_file is not None\n        or service_account_key is not None\n        or application_credentials is not None\n    ) and (from_env is None or not from_env):\n        return GcsCredentials.Static(\n            gcs_static_credentials(\n                service_account_file=service_account_file,\n                service_account_key=service_account_key,\n                application_credentials=application_credentials,\n            )\n        )\n\n    raise ValueError(\"Conflicting arguments to gcs_credentials function\")\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.gcs_from_env_credentials","title":"<code>gcs_from_env_credentials()</code>","text":"<p>Instruct Google Cloud Storage object store to fetch credentials from the operative system environment.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def gcs_from_env_credentials() -&gt; GcsCredentials.FromEnv:\n    \"\"\"Instruct Google Cloud Storage object store to fetch credentials from the operative system environment.\"\"\"\n    return GcsCredentials.FromEnv()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.gcs_static_credentials","title":"<code>gcs_static_credentials(*, service_account_file=None, service_account_key=None, application_credentials=None)</code>","text":"<p>Create static credentials Google Cloud Storage object store.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def gcs_static_credentials(\n    *,\n    service_account_file: str | None = None,\n    service_account_key: str | None = None,\n    application_credentials: str | None = None,\n) -&gt; AnyGcsStaticCredential:\n    \"\"\"Create static credentials Google Cloud Storage object store.\"\"\"\n    if service_account_file is not None:\n        return GcsStaticCredentials.ServiceAccount(service_account_file)\n    if service_account_key is not None:\n        return GcsStaticCredentials.ServiceAccountKey(service_account_key)\n    if application_credentials is not None:\n        return GcsStaticCredentials.ApplicationCredentials(application_credentials)\n    raise ValueError(\"Conflicting arguments to gcs_static_credentials function\")\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.gcs_storage","title":"<code>gcs_storage(*, bucket, prefix, service_account_file=None, service_account_key=None, application_credentials=None, from_env=None, config=None)</code>","text":"<p>Create a Storage instance that saves data in Google Cloud Storage object store.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket where the repository will store its data</p> required <code>prefix</code> <code>str | None</code> <p>The prefix within the bucket that is the root directory of the repository</p> required <code>from_env</code> <code>bool | None</code> <p>Fetch credentials from the operative system environment</p> <code>None</code> Source code in <code>icechunk/storage.py</code> <pre><code>def gcs_storage(\n    *,\n    bucket: str,\n    prefix: str | None,\n    service_account_file: str | None = None,\n    service_account_key: str | None = None,\n    application_credentials: str | None = None,\n    from_env: bool | None = None,\n    config: dict[str, str] | None = None,\n) -&gt; Storage:\n    \"\"\"Create a Storage instance that saves data in Google Cloud Storage object store.\n\n    Parameters\n    ----------\n    bucket: str\n        The bucket where the repository will store its data\n    prefix: str | None\n        The prefix within the bucket that is the root directory of the repository\n    from_env: bool | None\n        Fetch credentials from the operative system environment\n    \"\"\"\n    credentials = gcs_credentials(\n        service_account_file=service_account_file,\n        service_account_key=service_account_key,\n        application_credentials=application_credentials,\n        from_env=from_env,\n    )\n    return Storage.new_gcs(\n        bucket=bucket,\n        prefix=prefix,\n        credentials=credentials,\n        config=config,\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.in_memory_storage","title":"<code>in_memory_storage()</code>","text":"<p>Create a Storage instance that saves data in memory.</p> <p>This Storage implementation is used for tests. Data will be lost after the process finishes, and can only be accesses through the Storage instance returned. Different instances don't share data.</p> Source code in <code>icechunk/storage.py</code> <pre><code>def in_memory_storage() -&gt; Storage:\n    \"\"\"Create a Storage instance that saves data in memory.\n\n    This Storage implementation is used for tests. Data will be lost after the process finishes, and can only be accesses through the Storage instance returned. Different instances don't share data.\"\"\"\n    return Storage.new_in_memory()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.local_filesystem_storage","title":"<code>local_filesystem_storage(path)</code>","text":"<p>Create a Storage instance that saves data in the local file system.</p> <p>This Storage instance is not recommended for production data</p> Source code in <code>icechunk/storage.py</code> <pre><code>def local_filesystem_storage(path: str) -&gt; Storage:\n    \"\"\"Create a Storage instance that saves data in the local file system.\n\n    This Storage instance is not recommended for production data\n    \"\"\"\n    return Storage.new_local_filesystem(path)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_anonymous_credentials","title":"<code>s3_anonymous_credentials()</code>","text":"<p>Create no-signature credentials for S3 and S3 compatible object stores.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def s3_anonymous_credentials() -&gt; S3Credentials.Anonymous:\n    \"\"\"Create no-signature credentials for S3 and S3 compatible object stores.\"\"\"\n    return S3Credentials.Anonymous()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_credentials","title":"<code>s3_credentials(*, access_key_id=None, secret_access_key=None, session_token=None, expires_after=None, anonymous=None, from_env=None, get_credentials=None)</code>","text":"<p>Create credentials for S3 and S3 compatible object stores.</p> <p>If all arguments are None, credentials are fetched from the environment.</p> <p>Parameters:</p> Name Type Description Default <code>access_key_id</code> <code>str | None</code> <p>S3 credential access key</p> <code>None</code> <code>secret_access_key</code> <code>str | None</code> <p>S3 credential secret access key</p> <code>None</code> <code>session_token</code> <code>str | None</code> <p>Optional S3 credential session token</p> <code>None</code> <code>expires_after</code> <code>datetime | None</code> <p>Optional expiration for the object store credentials</p> <code>None</code> <code>anonymous</code> <code>bool | None</code> <p>If set to True requests to the object store will not be signed</p> <code>None</code> <code>from_env</code> <code>bool | None</code> <p>Fetch credentials from the operative system environment</p> <code>None</code> <code>get_credentials</code> <code>Callable[[], S3StaticCredentials] | None</code> <p>Use this function to get and refresh object store credentials</p> <code>None</code> Source code in <code>icechunk/credentials.py</code> <pre><code>def s3_credentials(\n    *,\n    access_key_id: str | None = None,\n    secret_access_key: str | None = None,\n    session_token: str | None = None,\n    expires_after: datetime | None = None,\n    anonymous: bool | None = None,\n    from_env: bool | None = None,\n    get_credentials: Callable[[], S3StaticCredentials] | None = None,\n) -&gt; AnyS3Credential:\n    \"\"\"Create credentials for S3 and S3 compatible object stores.\n\n    If all arguments are None, credentials are fetched from the environment.\n\n    Parameters\n    ----------\n    access_key_id: str | None\n        S3 credential access key\n    secret_access_key: str | None\n        S3 credential secret access key\n    session_token: str | None\n        Optional S3 credential session token\n    expires_after: datetime | None\n        Optional expiration for the object store credentials\n    anonymous: bool | None\n        If set to True requests to the object store will not be signed\n    from_env: bool | None\n        Fetch credentials from the operative system environment\n    get_credentials: Callable[[], S3StaticCredentials] | None\n        Use this function to get and refresh object store credentials\n    \"\"\"\n    if (\n        (from_env is None or from_env)\n        and access_key_id is None\n        and secret_access_key is None\n        and session_token is None\n        and expires_after is None\n        and not anonymous\n        and get_credentials is None\n    ):\n        return s3_from_env_credentials()\n\n    if (\n        anonymous\n        and access_key_id is None\n        and secret_access_key is None\n        and session_token is None\n        and expires_after is None\n        and not from_env\n        and get_credentials is None\n    ):\n        return s3_anonymous_credentials()\n\n    if (\n        get_credentials is not None\n        and access_key_id is None\n        and secret_access_key is None\n        and session_token is None\n        and expires_after is None\n        and not from_env\n        and not anonymous\n    ):\n        return s3_refreshable_credentials(get_credentials)\n\n    if (\n        access_key_id\n        and secret_access_key\n        and not from_env\n        and not anonymous\n        and get_credentials is None\n    ):\n        return s3_static_credentials(\n            access_key_id=access_key_id,\n            secret_access_key=secret_access_key,\n            session_token=session_token,\n            expires_after=expires_after,\n        )\n\n    raise ValueError(\"Conflicting arguments to s3_credentials function\")\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_from_env_credentials","title":"<code>s3_from_env_credentials()</code>","text":"<p>Instruct S3 and S3 compatible object stores to gather credentials from the operative system environment.</p> Source code in <code>icechunk/credentials.py</code> <pre><code>def s3_from_env_credentials() -&gt; S3Credentials.FromEnv:\n    \"\"\"Instruct S3 and S3 compatible object stores to gather credentials from the operative system environment.\"\"\"\n    return S3Credentials.FromEnv()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_refreshable_credentials","title":"<code>s3_refreshable_credentials(get_credentials)</code>","text":"<p>Create refreshable credentials for S3 and S3 compatible object stores.</p> <p>Parameters:</p> Name Type Description Default <code>get_credentials</code> <code>Callable[[], S3StaticCredentials]</code> <p>Use this function to get and refresh the credentials. The function must be pickable.</p> required Source code in <code>icechunk/credentials.py</code> <pre><code>def s3_refreshable_credentials(\n    get_credentials: Callable[[], S3StaticCredentials],\n) -&gt; S3Credentials.Refreshable:\n    \"\"\"Create refreshable credentials for S3 and S3 compatible object stores.\n\n\n    Parameters\n    ----------\n    get_credentials: Callable[[], S3StaticCredentials]\n        Use this function to get and refresh the credentials. The function must be pickable.\n    \"\"\"\n    return S3Credentials.Refreshable(pickle.dumps(get_credentials))\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_static_credentials","title":"<code>s3_static_credentials(*, access_key_id, secret_access_key, session_token=None, expires_after=None)</code>","text":"<p>Create static credentials for S3 and S3 compatible object stores.</p> <p>Parameters:</p> Name Type Description Default <code>access_key_id</code> <code>str</code> <p>S3 credential access key</p> required <code>secret_access_key</code> <code>str</code> <p>S3 credential secret access key</p> required <code>session_token</code> <code>str | None</code> <p>Optional S3 credential session token</p> <code>None</code> <code>expires_after</code> <code>datetime | None</code> <p>Optional expiration for the object store credentials</p> <code>None</code> Source code in <code>icechunk/credentials.py</code> <pre><code>def s3_static_credentials(\n    *,\n    access_key_id: str,\n    secret_access_key: str,\n    session_token: str | None = None,\n    expires_after: datetime | None = None,\n) -&gt; S3Credentials.Static:\n    \"\"\"Create static credentials for S3 and S3 compatible object stores.\n\n    Parameters\n    ----------\n    access_key_id: str | None\n        S3 credential access key\n    secret_access_key: str | None\n        S3 credential secret access key\n    session_token: str | None\n        Optional S3 credential session token\n    expires_after: datetime | None\n        Optional expiration for the object store credentials\n    \"\"\"\n    return S3Credentials.Static(\n        S3StaticCredentials(\n            access_key_id=access_key_id,\n            secret_access_key=secret_access_key,\n            session_token=session_token,\n            expires_after=expires_after,\n        )\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_storage","title":"<code>s3_storage(*, bucket, prefix, region=None, endpoint_url=None, allow_http=False, access_key_id=None, secret_access_key=None, session_token=None, expires_after=None, anonymous=None, from_env=None, get_credentials=None)</code>","text":"<p>Create a Storage instance that saves data in S3 or S3 compatible object stores.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket where the repository will store its data</p> required <code>prefix</code> <code>str | None</code> <p>The prefix within the bucket that is the root directory of the repository</p> required <code>region</code> <code>str | None</code> <p>The region to use in the object store, if <code>None</code> a default region will be used</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>Optional endpoint where the object store serves data, example: http://localhost:9000</p> <code>None</code> <code>allow_http</code> <code>bool</code> <p>If the object store can be accessed using http protocol instead of https</p> <code>False</code> <code>access_key_id</code> <code>str | None</code> <p>S3 credential access key</p> <code>None</code> <code>secret_access_key</code> <code>str | None</code> <p>S3 credential secret access key</p> <code>None</code> <code>session_token</code> <code>str | None</code> <p>Optional S3 credential session token</p> <code>None</code> <code>expires_after</code> <code>datetime | None</code> <p>Optional expiration for the object store credentials</p> <code>None</code> <code>anonymous</code> <code>bool | None</code> <p>If set to True requests to the object store will not be signed</p> <code>None</code> <code>from_env</code> <code>bool | None</code> <p>Fetch credentials from the operative system environment</p> <code>None</code> <code>get_credentials</code> <code>Callable[[], S3StaticCredentials] | None</code> <p>Use this function to get and refresh object store credentials</p> <code>None</code> Source code in <code>icechunk/storage.py</code> <pre><code>def s3_storage(\n    *,\n    bucket: str,\n    prefix: str | None,\n    region: str | None = None,\n    endpoint_url: str | None = None,\n    allow_http: bool = False,\n    access_key_id: str | None = None,\n    secret_access_key: str | None = None,\n    session_token: str | None = None,\n    expires_after: datetime | None = None,\n    anonymous: bool | None = None,\n    from_env: bool | None = None,\n    get_credentials: Callable[[], S3StaticCredentials] | None = None,\n) -&gt; Storage:\n    \"\"\"Create a Storage instance that saves data in S3 or S3 compatible object stores.\n\n    Parameters\n    ----------\n    bucket: str\n        The bucket where the repository will store its data\n    prefix: str | None\n        The prefix within the bucket that is the root directory of the repository\n    region: str | None\n        The region to use in the object store, if `None` a default region will be used\n    endpoint_url: str | None\n        Optional endpoint where the object store serves data, example: http://localhost:9000\n    allow_http: bool\n        If the object store can be accessed using http protocol instead of https\n    access_key_id: str | None\n        S3 credential access key\n    secret_access_key: str | None\n        S3 credential secret access key\n    session_token: str | None\n        Optional S3 credential session token\n    expires_after: datetime | None\n        Optional expiration for the object store credentials\n    anonymous: bool | None\n        If set to True requests to the object store will not be signed\n    from_env: bool | None\n        Fetch credentials from the operative system environment\n    get_credentials: Callable[[], S3StaticCredentials] | None\n        Use this function to get and refresh object store credentials\n    \"\"\"\n    credentials = s3_credentials(\n        access_key_id=access_key_id,\n        secret_access_key=secret_access_key,\n        session_token=session_token,\n        expires_after=expires_after,\n        anonymous=anonymous,\n        from_env=from_env,\n        get_credentials=get_credentials,\n    )\n    options = S3Options(region=region, endpoint_url=endpoint_url, allow_http=allow_http)\n    return Storage.new_s3(\n        config=options,\n        bucket=bucket,\n        prefix=prefix,\n        credentials=credentials,\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.s3_store","title":"<code>s3_store(region=None, endpoint_url=None, allow_http=False, anonymous=False, s3_compatible=False)</code>","text":"<p>Build an ObjectStoreConfig instance for S3 or S3 compatible object stores.</p> Source code in <code>icechunk/storage.py</code> <pre><code>def s3_store(\n    region: str | None = None,\n    endpoint_url: str | None = None,\n    allow_http: bool = False,\n    anonymous: bool = False,\n    s3_compatible: bool = False,\n) -&gt; ObjectStoreConfig.S3Compatible | ObjectStoreConfig.S3:\n    \"\"\"Build an ObjectStoreConfig instance for S3 or S3 compatible object stores.\"\"\"\n    options = S3Options(region=region, endpoint_url=endpoint_url, allow_http=allow_http)\n    return (\n        ObjectStoreConfig.S3Compatible(options)\n        if s3_compatible\n        else ObjectStoreConfig.S3(options)\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.tigris_storage","title":"<code>tigris_storage(*, bucket, prefix, region=None, endpoint_url=None, allow_http=False, access_key_id=None, secret_access_key=None, session_token=None, expires_after=None, anonymous=None, from_env=None, get_credentials=None)</code>","text":"<p>Create a Storage instance that saves data in Tigris object store.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket where the repository will store its data</p> required <code>prefix</code> <code>str | None</code> <p>The prefix within the bucket that is the root directory of the repository</p> required <code>region</code> <code>str | None</code> <p>The region to use in the object store, if <code>None</code> a default region will be used</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>Optional endpoint where the object store serves data, example: http://localhost:9000</p> <code>None</code> <code>allow_http</code> <code>bool</code> <p>If the object store can be accessed using http protocol instead of https</p> <code>False</code> <code>access_key_id</code> <code>str | None</code> <p>S3 credential access key</p> <code>None</code> <code>secret_access_key</code> <code>str | None</code> <p>S3 credential secret access key</p> <code>None</code> <code>session_token</code> <code>str | None</code> <p>Optional S3 credential session token</p> <code>None</code> <code>expires_after</code> <code>datetime | None</code> <p>Optional expiration for the object store credentials</p> <code>None</code> <code>anonymous</code> <code>bool | None</code> <p>If set to True requests to the object store will not be signed</p> <code>None</code> <code>from_env</code> <code>bool | None</code> <p>Fetch credentials from the operative system environment</p> <code>None</code> <code>get_credentials</code> <code>Callable[[], S3StaticCredentials] | None</code> <p>Use this function to get and refresh object store credentials</p> <code>None</code> Source code in <code>icechunk/storage.py</code> <pre><code>def tigris_storage(\n    *,\n    bucket: str,\n    prefix: str | None,\n    region: str | None = None,\n    endpoint_url: str | None = None,\n    allow_http: bool = False,\n    access_key_id: str | None = None,\n    secret_access_key: str | None = None,\n    session_token: str | None = None,\n    expires_after: datetime | None = None,\n    anonymous: bool | None = None,\n    from_env: bool | None = None,\n    get_credentials: Callable[[], S3StaticCredentials] | None = None,\n) -&gt; Storage:\n    \"\"\"Create a Storage instance that saves data in Tigris object store.\n\n    Parameters\n    ----------\n    bucket: str\n        The bucket where the repository will store its data\n    prefix: str | None\n        The prefix within the bucket that is the root directory of the repository\n    region: str | None\n        The region to use in the object store, if `None` a default region will be used\n    endpoint_url: str | None\n        Optional endpoint where the object store serves data, example: http://localhost:9000\n    allow_http: bool\n        If the object store can be accessed using http protocol instead of https\n    access_key_id: str | None\n        S3 credential access key\n    secret_access_key: str | None\n        S3 credential secret access key\n    session_token: str | None\n        Optional S3 credential session token\n    expires_after: datetime | None\n        Optional expiration for the object store credentials\n    anonymous: bool | None\n        If set to True requests to the object store will not be signed\n    from_env: bool | None\n        Fetch credentials from the operative system environment\n    get_credentials: Callable[[], S3StaticCredentials] | None\n        Use this function to get and refresh object store credentials\n    \"\"\"\n    credentials = s3_credentials(\n        access_key_id=access_key_id,\n        secret_access_key=secret_access_key,\n        session_token=session_token,\n        expires_after=expires_after,\n        anonymous=anonymous,\n        from_env=from_env,\n        get_credentials=get_credentials,\n    )\n    options = S3Options(region=region, endpoint_url=endpoint_url, allow_http=allow_http)\n    return Storage.new_tigris(\n        config=options,\n        bucket=bucket,\n        prefix=prefix,\n        credentials=credentials,\n    )\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.xarray.XarrayDatasetWriter","title":"<code>XarrayDatasetWriter</code>  <code>dataclass</code>","text":"<p>Write Xarray Datasets to a group in an Icechunk store.</p> <p>This class is private API. Please do not use it.</p> Source code in <code>icechunk/xarray.py</code> <pre><code>@dataclass\nclass XarrayDatasetWriter:\n    \"\"\"\n    Write Xarray Datasets to a group in an Icechunk store.\n\n    This class is private API. Please do not use it.\n    \"\"\"\n\n    dataset: Dataset = field(repr=False)\n    store: IcechunkStore = field(kw_only=True)\n\n    safe_chunks: bool = field(kw_only=True, default=True)\n\n    _initialized: bool = field(default=False, repr=False)\n\n    xarray_store: ZarrStore = field(init=False, repr=False)\n    writer: LazyArrayWriter = field(init=False, repr=False)\n\n    def __post_init__(self) -&gt; None:\n        if not isinstance(self.store, IcechunkStore):\n            raise ValueError(\n                f\"Please pass in an icechunk.Session. Received {type(self.store)!r} instead.\"\n            )\n\n    def _open_group(\n        self,\n        *,\n        group: str | None,\n        mode: ZarrWriteModes | None,\n        append_dim: Hashable | None,\n        region: Region,\n    ) -&gt; None:\n        concrete_mode: ZarrWriteModes = _choose_default_mode(\n            mode=mode, append_dim=append_dim, region=region\n        )\n\n        self.xarray_store = ZarrStore.open_group(\n            store=self.store,\n            group=group,\n            mode=concrete_mode,\n            zarr_format=3,\n            append_dim=append_dim,\n            write_region=region,\n            safe_chunks=self.safe_chunks,\n            synchronizer=None,\n            consolidated=False,\n            consolidate_on_close=False,\n            zarr_version=None,\n        )\n        self.dataset = self.xarray_store._validate_and_autodetect_region(self.dataset)\n\n    def write_metadata(self, encoding: Mapping[Any, Any] | None = None) -&gt; None:\n        \"\"\"\n        This method creates new Zarr arrays when necessary, writes attributes,\n        and any in-memory arrays.\n        \"\"\"\n        from xarray.backends.api import _validate_dataset_names, dump_to_store\n\n        # validate Dataset keys, DataArray names\n        _validate_dataset_names(self.dataset)\n\n        if encoding is None:\n            encoding = {}\n        self.xarray_store._validate_encoding(encoding)\n\n        # This writes the metadata (zarr.json) for all arrays\n        # This also will resize arrays for any appends\n        self.writer = LazyArrayWriter()\n        dump_to_store(self.dataset, self.xarray_store, self.writer, encoding=encoding)  # type: ignore[no-untyped-call]\n\n        self._initialized = True\n\n    def write_eager(self) -&gt; None:\n        \"\"\"\n        Write in-memory variables to store.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if not self._initialized:\n            raise ValueError(\"Please call `write_metadata` first.\")\n        self.writer.write_eager()\n\n    def write_lazy(\n        self,\n        chunkmanager_store_kwargs: MutableMapping[Any, Any] | None = None,\n        split_every: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Write lazy arrays (e.g. dask) to store.\n        \"\"\"\n        if not self._initialized:\n            raise ValueError(\"Please call `write_metadata` first.\")\n\n        if not self.writer.sources:\n            return\n\n        chunkmanager_store_kwargs = chunkmanager_store_kwargs or {}\n        chunkmanager_store_kwargs[\"load_stored\"] = False\n        chunkmanager_store_kwargs[\"return_stored\"] = True\n\n        # This calls dask.array.store, and we receive a dask array where each chunk is a Zarr array\n        # each of those zarr.Array.store contains the changesets we need\n        stored_arrays = self.writer.sync(\n            compute=False, chunkmanager_store_kwargs=chunkmanager_store_kwargs\n        )  # type: ignore[no-untyped-call]\n\n        # Now we tree-reduce all changesets\n        merged_session = stateful_store_reduce(\n            stored_arrays,\n            prefix=\"ice-changeset\",\n            chunk=extract_session,\n            aggregate=merge_sessions,\n            split_every=split_every,\n            compute=True,\n            **chunkmanager_store_kwargs,\n        )\n        self.store.session.merge(merged_session)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.xarray.XarrayDatasetWriter.write_eager","title":"<code>write_eager()</code>","text":"<p>Write in-memory variables to store.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>icechunk/xarray.py</code> <pre><code>def write_eager(self) -&gt; None:\n    \"\"\"\n    Write in-memory variables to store.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not self._initialized:\n        raise ValueError(\"Please call `write_metadata` first.\")\n    self.writer.write_eager()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.xarray.XarrayDatasetWriter.write_lazy","title":"<code>write_lazy(chunkmanager_store_kwargs=None, split_every=None)</code>","text":"<p>Write lazy arrays (e.g. dask) to store.</p> Source code in <code>icechunk/xarray.py</code> <pre><code>def write_lazy(\n    self,\n    chunkmanager_store_kwargs: MutableMapping[Any, Any] | None = None,\n    split_every: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Write lazy arrays (e.g. dask) to store.\n    \"\"\"\n    if not self._initialized:\n        raise ValueError(\"Please call `write_metadata` first.\")\n\n    if not self.writer.sources:\n        return\n\n    chunkmanager_store_kwargs = chunkmanager_store_kwargs or {}\n    chunkmanager_store_kwargs[\"load_stored\"] = False\n    chunkmanager_store_kwargs[\"return_stored\"] = True\n\n    # This calls dask.array.store, and we receive a dask array where each chunk is a Zarr array\n    # each of those zarr.Array.store contains the changesets we need\n    stored_arrays = self.writer.sync(\n        compute=False, chunkmanager_store_kwargs=chunkmanager_store_kwargs\n    )  # type: ignore[no-untyped-call]\n\n    # Now we tree-reduce all changesets\n    merged_session = stateful_store_reduce(\n        stored_arrays,\n        prefix=\"ice-changeset\",\n        chunk=extract_session,\n        aggregate=merge_sessions,\n        split_every=split_every,\n        compute=True,\n        **chunkmanager_store_kwargs,\n    )\n    self.store.session.merge(merged_session)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.xarray.XarrayDatasetWriter.write_metadata","title":"<code>write_metadata(encoding=None)</code>","text":"<p>This method creates new Zarr arrays when necessary, writes attributes, and any in-memory arrays.</p> Source code in <code>icechunk/xarray.py</code> <pre><code>def write_metadata(self, encoding: Mapping[Any, Any] | None = None) -&gt; None:\n    \"\"\"\n    This method creates new Zarr arrays when necessary, writes attributes,\n    and any in-memory arrays.\n    \"\"\"\n    from xarray.backends.api import _validate_dataset_names, dump_to_store\n\n    # validate Dataset keys, DataArray names\n    _validate_dataset_names(self.dataset)\n\n    if encoding is None:\n        encoding = {}\n    self.xarray_store._validate_encoding(encoding)\n\n    # This writes the metadata (zarr.json) for all arrays\n    # This also will resize arrays for any appends\n    self.writer = LazyArrayWriter()\n    dump_to_store(self.dataset, self.xarray_store, self.writer, encoding=encoding)  # type: ignore[no-untyped-call]\n\n    self._initialized = True\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.xarray.make_dataset","title":"<code>make_dataset(obj)</code>","text":"<pre><code>make_dataset(obj: DataArray) -&gt; Dataset\n</code></pre><pre><code>make_dataset(obj: Dataset) -&gt; Dataset\n</code></pre> <p>Copied from DataArray.to_zarr</p> Source code in <code>icechunk/xarray.py</code> <pre><code>def make_dataset(obj: DataArray | Dataset) -&gt; Dataset:\n    \"\"\"Copied from DataArray.to_zarr\"\"\"\n    DATAARRAY_NAME = \"__xarray_dataarray_name__\"\n    DATAARRAY_VARIABLE = \"__xarray_dataarray_variable__\"\n\n    if isinstance(obj, Dataset):\n        return obj\n\n    assert isinstance(obj, DataArray)\n\n    if obj.name is None:\n        # If no name is set then use a generic xarray name\n        dataset = obj.to_dataset(name=DATAARRAY_VARIABLE)\n    elif obj.name in obj.coords or obj.name in obj.dims:\n        # The name is the same as one of the coords names, which the netCDF data model\n        # does not support, so rename it but keep track of the old name\n        dataset = obj.to_dataset(name=DATAARRAY_VARIABLE)\n        dataset.attrs[DATAARRAY_NAME] = obj.name\n    else:\n        # No problems with the name - so we're fine!\n        dataset = obj.to_dataset()\n    return dataset\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.xarray.to_icechunk","title":"<code>to_icechunk(obj, session, *, group=None, mode=None, safe_chunks=True, append_dim=None, region=None, encoding=None, chunkmanager_store_kwargs=None, split_every=None)</code>","text":"<p>Write an Xarray object to a group of an Icechunk store.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>DataArray | Dataset</code> <p>Xarray object to write</p> required <code>session</code> <code>Session</code> <p>Writable Icechunk Session</p> required <code>mode</code> <code>\"w\", \"w-\", \"a\", \"a-\", r+\", None</code> <p>Persistence mode: \"w\" means create (overwrite if exists); \"w-\" means create (fail if exists); \"a\" means override all existing variables including dimension coordinates (create if does not exist); \"a-\" means only append those variables that have <code>append_dim</code>. \"r+\" means modify existing array values only (raise an error if any metadata or shapes would change). The default mode is \"a\" if <code>append_dim</code> is set. Otherwise, it is \"r+\" if <code>region</code> is set and <code>w-</code> otherwise.</p> <code>\"w\"</code> <code>group</code> <code>str</code> <p>Group path. (a.k.a. <code>path</code> in zarr terminology.)</p> <code>None</code> <code>encoding</code> <code>dict</code> <p>Nested dictionary with variable names as keys and dictionaries of variable specific encodings as values, e.g., <code>{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}</code></p> <code>None</code> <code>append_dim</code> <code>hashable</code> <p>If set, the dimension along which the data will be appended. All other dimensions on overridden variables must remain the same size.</p> <code>None</code> <code>region</code> <code>dict or auto</code> <p>Optional mapping from dimension names to either a) <code>\"auto\"</code>, or b) integer slices, indicating the region of existing zarr array(s) in which to write this dataset's data.</p> <p>If <code>\"auto\"</code> is provided the existing store will be opened and the region inferred by matching indexes. <code>\"auto\"</code> can be used as a single string, which will automatically infer the region for all dimensions, or as dictionary values for specific dimensions mixed together with explicit slices for other dimensions.</p> <p>Alternatively integer slices can be provided; for example, <code>{'x': slice(0, 1000), 'y': slice(10000, 11000)}</code> would indicate that values should be written to the region <code>0:1000</code> along <code>x</code> and <code>10000:11000</code> along <code>y</code>.</p> <p>Users are expected to ensure that the specified region aligns with Zarr chunk boundaries, and that dask chunks are also aligned. Xarray makes limited checks that these multiple chunk boundaries line up. It is possible to write incomplete chunks and corrupt the data with this option if you are not careful.</p> <code>None</code> <code>safe_chunks</code> <code>bool</code> <p>If True, only allow writes to when there is a many-to-one relationship between Zarr chunks (specified in encoding) and Dask chunks. Set False to override this restriction; however, data may become corrupted if Zarr arrays are written in parallel. In addition to the many-to-one relationship validation, it also detects partial chunks writes when using the region parameter, these partial chunks are considered unsafe in the mode \"r+\" but safe in the mode \"a\". Note: Even with these validations it can still be unsafe to write two or more chunked arrays in the same location in parallel if they are not writing in independent regions.</p> <code>True</code> <code>chunkmanager_store_kwargs</code> <code>dict</code> <p>Additional keyword arguments passed on to the <code>ChunkManager.store</code> method used to store chunked arrays. For example for a dask array additional kwargs will be passed eventually to <code>dask.array.store()</code>. Experimental API that should not be relied upon.</p> <code>None</code> <code>split_every</code> <code>int | None</code> <p>Number of tasks to merge at every level of the tree reduction.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>Two restrictions apply to the use of <code>region</code>:</p> <ul> <li>If <code>region</code> is set, all variables in a dataset must have at     least one dimension in common with the region. Other variables     should be written in a separate single call to <code>to_icechunk()</code>.</li> <li>Dimensions cannot be included in both <code>region</code> and     <code>append_dim</code> at the same time. To create empty arrays to fill     in with <code>region</code>, use the <code>XarrayDatasetWriter</code> directly.</li> </ul> Source code in <code>icechunk/xarray.py</code> <pre><code>def to_icechunk(\n    obj: DataArray | Dataset,\n    session: Session,\n    *,\n    group: str | None = None,\n    mode: ZarrWriteModes | None = None,\n    safe_chunks: bool = True,\n    append_dim: Hashable | None = None,\n    region: Region = None,\n    encoding: Mapping[Any, Any] | None = None,\n    chunkmanager_store_kwargs: MutableMapping[Any, Any] | None = None,\n    split_every: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Write an Xarray object to a group of an Icechunk store.\n\n    Parameters\n    ----------\n    obj: DataArray or Dataset\n        Xarray object to write\n    session : icechunk.Session\n        Writable Icechunk Session\n    mode : {\"w\", \"w-\", \"a\", \"a-\", r+\", None}, optional\n        Persistence mode: \"w\" means create (overwrite if exists);\n        \"w-\" means create (fail if exists);\n        \"a\" means override all existing variables including dimension coordinates (create if does not exist);\n        \"a-\" means only append those variables that have ``append_dim``.\n        \"r+\" means modify existing array *values* only (raise an error if\n        any metadata or shapes would change).\n        The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n        \"r+\" if ``region`` is set and ``w-`` otherwise.\n    group : str, optional\n        Group path. (a.k.a. `path` in zarr terminology.)\n    encoding : dict, optional\n        Nested dictionary with variable names as keys and dictionaries of\n        variable specific encodings as values, e.g.,\n        ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n    append_dim : hashable, optional\n        If set, the dimension along which the data will be appended. All\n        other dimensions on overridden variables must remain the same size.\n    region : dict or \"auto\", optional\n        Optional mapping from dimension names to either a) ``\"auto\"``, or b) integer\n        slices, indicating the region of existing zarr array(s) in which to write\n        this dataset's data.\n\n        If ``\"auto\"`` is provided the existing store will be opened and the region\n        inferred by matching indexes. ``\"auto\"`` can be used as a single string,\n        which will automatically infer the region for all dimensions, or as\n        dictionary values for specific dimensions mixed together with explicit\n        slices for other dimensions.\n\n        Alternatively integer slices can be provided; for example, ``{'x': slice(0,\n        1000), 'y': slice(10000, 11000)}`` would indicate that values should be\n        written to the region ``0:1000`` along ``x`` and ``10000:11000`` along\n        ``y``.\n\n        Users are expected to ensure that the specified region aligns with\n        Zarr chunk boundaries, and that dask chunks are also aligned.\n        Xarray makes limited checks that these multiple chunk boundaries line up.\n        It is possible to write incomplete chunks and corrupt the data with this\n        option if you are not careful.\n    safe_chunks : bool, default: True\n        If True, only allow writes to when there is a many-to-one relationship\n        between Zarr chunks (specified in encoding) and Dask chunks.\n        Set False to override this restriction; however, data may become corrupted\n        if Zarr arrays are written in parallel.\n        In addition to the many-to-one relationship validation, it also detects partial\n        chunks writes when using the region parameter,\n        these partial chunks are considered unsafe in the mode \"r+\" but safe in\n        the mode \"a\".\n        Note: Even with these validations it can still be unsafe to write\n        two or more chunked arrays in the same location in parallel if they are\n        not writing in independent regions.\n    chunkmanager_store_kwargs : dict, optional\n        Additional keyword arguments passed on to the `ChunkManager.store` method used to store\n        chunked arrays. For example for a dask array additional kwargs will be passed eventually to\n        `dask.array.store()`. Experimental API that should not be relied upon.\n    split_every: int, optional\n        Number of tasks to merge at every level of the tree reduction.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Two restrictions apply to the use of ``region``:\n\n      - If ``region`` is set, _all_ variables in a dataset must have at\n        least one dimension in common with the region. Other variables\n        should be written in a separate single call to ``to_icechunk()``.\n      - Dimensions cannot be included in both ``region`` and\n        ``append_dim`` at the same time. To create empty arrays to fill\n        in with ``region``, use the `XarrayDatasetWriter` directly.\n    \"\"\"\n\n    as_dataset = make_dataset(obj)\n    with session.allow_pickling():\n        store = session.store\n        writer = XarrayDatasetWriter(as_dataset, store=store, safe_chunks=safe_chunks)\n\n        writer._open_group(group=group, mode=mode, append_dim=append_dim, region=region)\n\n        # write metadata\n        writer.write_metadata(encoding)\n        # write in-memory arrays\n        writer.write_eager()\n        # eagerly write dask arrays\n        writer.write_lazy(chunkmanager_store_kwargs=chunkmanager_store_kwargs)\n</code></pre>"},{"location":"icechunk-python/version-control/","title":"Version Control","text":"<p>Home / icechunk-python / version-control</p>"},{"location":"icechunk-python/version-control/#version-control","title":"Version Control","text":"<p>Icechunk carries over concepts from other version control software (e.g. Git) to multidimensional arrays. Doing so helps ease the burden of managing multiple versions of your data, and helps you be precise about which version of your dataset is being used for downstream purposes.</p> <p>Core concepts of Icechunk's version control system are:</p> <ul> <li>A snapshot bundles together related data and metadata changes in a single \"transaction\".</li> <li>A branch points to the latest snapshot in a series of snapshots. Multiple branches can co-exist at a given time, and multiple users can add snapshots to a single branch. One common pattern is to use dev, stage, and prod branches to separate versions of a dataset.</li> <li>A tag is an immutable reference to a snapshot, usually used to represent an \"important\" version of the dataset such as a release.</li> </ul> <p>Snapshots, branches, and tags all refer to specific versions of your dataset. You can time-travel/navigate back to any version of your data as referenced by a snapshot, a branch, or a tag using a snapshot ID, a branch name, or a tag name when creating a new <code>Session</code>.</p>"},{"location":"icechunk-python/version-control/#setup","title":"Setup","text":"<p>To get started, we can create a new <code>Repository</code>.</p> <p>Note</p> <p>This example uses an in-memory storage backend, but you can also use any other storage backend instead.</p> <pre><code>import icechunk\n\nrepo = icechunk.Repository.create(icechunk.in_memory_storage())\n</code></pre> <p>On creating a new <code>Repository</code>, it will automatically create a <code>main</code> branch with an initial snapshot. We can take a look at the ancestry of the <code>main</code> branch to confirm this.</p> <pre><code>repo.ancestry(branch=\"main\")\n\n# [SnapshotInfo(id=\"A840RMN5CF807CM66RY0\", parent_id=None, written_at=datetime.datetime(2025,1,30,19,52,41,592998, tzinfo=datetime.timezone.utc), message=\"Repository...\")]\n</code></pre> <p>Note</p> <p>The <code>ancestry</code> method can be used to inspect the ancestry of any branch, snapshot, or tag.</p> <p>We get back a list of <code>SnapshotInfo</code> objects, which contain information about the snapshot, including its ID, the ID of its parent snapshot, and the time it was written.</p>"},{"location":"icechunk-python/version-control/#creating-a-snapshot","title":"Creating a snapshot","text":"<p>Now that we have a <code>Repository</code> with a <code>main</code> branch, we can modify the data in the repository and create a new snapshot. First we need to create a writable from the <code>main</code> branch.</p> <p>Note</p> <p>Writable <code>Session</code> objects are required to create new snapshots, and can only be created from the tip of a branch. Checking out tags or other snapshots is read-only.</p> <pre><code>session = repo.writable_session(branch=\"main\")\n</code></pre> <p>We can now access the <code>zarr.Store</code> from the <code>Session</code> and create a new root group. Then we can modify the attributes of the root group and create a new snapshot.</p> <pre><code>import zarr\n\nroot = zarr.group(session.store)\nroot.attrs[\"foo\"] = \"bar\"\nsession.commit(message=\"Add foo attribute to root group\")\n\n# 'J1ZJHS4EEQW3ATKMV9TG'\n</code></pre> <p>Success! We've created a new snapshot with a new attribute on the root group.</p> <p>Once we've committed the snapshot, the <code>Session</code> will become read-only, and we can no longer modify the data using our existing <code>Session</code>. If we want to modify the data again, we need to create a new writable <code>Session</code> from the branch. Notice that we don't have to refresh the <code>Repository</code> to get the updates from the <code>main</code> branch. Instead, the <code>Repository</code> will automatically fetch the latest snapshot from the branch when we create a new writable <code>Session</code> from it.</p> <pre><code>session = repo.writable_session(branch=\"main\")\nroot = zarr.group(session.store)\nroot.attrs[\"foo\"] = \"baz\"\nsession.commit(message=\"Update foo attribute on root group\")\n\n# 'BZ9YP38SWPW2E784VAB0'\n</code></pre> <p>With a few snapshots committed, we can take a look at the ancestry of the <code>main</code> branch:</p> <pre><code>for snapshot in repo.ancestry(branch=\"main\"):\n    print(snapshot)\n\n# SnapshotInfo(id=\"BZ9YP38SWPW2E784VAB0\", parent_id=\"J1ZJHS4EEQW3ATKMV9TG\", written_at=datetime.datetime(2025,1,30,20,26,51,115330, tzinfo=datetime.timezone.utc), message=\"Update foo...\")\n# SnapshotInfo(id=\"J1ZJHS4EEQW3ATKMV9TG\", parent_id=\"A840RMN5CF807CM66RY0\", written_at=datetime.datetime(2025,1,30,20,26,50,9616, tzinfo=datetime.timezone.utc), message=\"Add foo at...\")\n# SnapshotInfo(id=\"A840RMN5CF807CM66RY0\", parent_id=None, written_at=datetime.datetime(2025,1,30,20,26,47,66157, tzinfo=datetime.timezone.utc), message=\"Repository...\")\n</code></pre> <p>Visually, this looks like below, where the arrows represent the parent-child relationship between snapshots.</p> <pre><code>gitGraph\n    commit id: \"A840RMN5\" type: NORMAL\n    commit id: \"J1ZJHS4\" type: NORMAL\n    commit id: \"BZ9YP38\" type: NORMAL\n</code></pre>"},{"location":"icechunk-python/version-control/#time-travel","title":"Time Travel","text":"<p>Now that we've created a new snapshot, we can time-travel back to the previous snapshot using the snapshot ID.</p> <p>Note</p> <p>It's important to note that because the <code>zarr Store</code> is read-only, we need to pass <code>mode=\"r\"</code> to the <code>zarr.open_group</code> function.</p> <pre><code>session = repo.readonly_session(snapshot_id=\"BSHY7B1AGAPWQC14Q18G\")\nroot = zarr.open_group(session.store, mode=\"r\")\nroot.attrs[\"foo\"]\n\n# 'bar'\n</code></pre>"},{"location":"icechunk-python/version-control/#branches","title":"Branches","text":"<p>If we want to modify the data from a previous snapshot, we can create a new branch from that snapshot with <code>create_branch</code>.</p> <pre><code>repo.create_branch(\"dev\", snapshot_id=main_branch_snapshot_id)\n</code></pre> <p>We can now create a new writable <code>Session</code> from the <code>dev</code> branch and modify the data.</p> <pre><code>session = repo.writable_session(branch=\"dev\")\nroot = zarr.group(session.store)\nroot.attrs[\"foo\"] = \"balogna\"\nsession.commit(message=\"Update foo attribute on root group\")\n\n# 'H1M3R93ZW19MYKCYASH0'\n</code></pre> <p>We can also create a new branch from the tip of the <code>main</code> branch if we want to modify our current working branch without modifying the <code>main</code> branch.</p> <pre><code>main_branch_snapshot_id = repo.lookup_branch(\"main\")\nrepo.create_branch(\"feature\", snapshot_id=main_branch_snapshot_id)\n\nsession = repo.writable_session(branch=\"feature\")\nroot = zarr.group(session.store)\nroot.attrs[\"foo\"] = \"cherry\"\nsession.commit(message=\"Update foo attribute on root group\")\n\n# 'S3QY2RDQQTRYFGJDTB6G'\n</code></pre> <p>With these branches created, the hierarchy of the repository now looks like below.</p> <pre><code>gitGraph\n    commit id: \"A840RMN5\" type: NORMAL\n    commit id: \"J1ZJHS4\" type: NORMAL\n    branch dev\n    checkout dev\n    commit id: \"H1M3R93\" type: NORMAL\n\n    checkout main\n    commit id: \"BZ9YP38\" type: NORMAL\n\n    checkout main\n    branch feature\n    commit id: \"S3QY2RD\" type: NORMAL\n</code></pre> <p>We can also list all branches in the repository.</p> <pre><code>repo.list_branches()\n\n# { 'dev', 'feature', 'main' }\n</code></pre> <p>If we need to find the snapshot that a branch is based on, we can use the <code>lookup_branch</code> method.</p> <pre><code>repo.lookup_branch(\"feature\")\n\n# 'J1ZJHS4EEQW3ATKMV9TG'\n</code></pre> <p>We can also delete a branch with <code>delete_branch</code>.</p> <pre><code>repo.delete_branch(\"feature\")\n</code></pre> <p>Finally, we can reset a branch to a previous snapshot with <code>reset_branch</code>. This immediately modifies the branch tip to the specified snapshot, changing the history of the branch.</p> <pre><code>repo.reset_branch(\"dev\", snapshot_id=\"J1ZJHS4EEQW3ATKMV9TG\")\n</code></pre>"},{"location":"icechunk-python/version-control/#tags","title":"Tags","text":"<p>Tags are immutable references to a snapshot. They are created with <code>create_tag</code>.</p> <pre><code>repo.create_tag(\"v1.0.0\", snapshot_id=\"J1ZJHS4EEQW3ATKMV9TG\")\n</code></pre> <p>Because tags are immutable, we need to use a readonly <code>Session</code> to access the data referenced by a tag.</p> <pre><code>session = repo.readonly_session(tag=\"v1.0.0\")\nroot = zarr.open_group(session.store, mode=\"r\")\nroot.attrs[\"foo\"]\n\n# 'bar'\n</code></pre> <pre><code>gitGraph\n    commit id: \"A840RMN5\" type: NORMAL\n    commit id: \"J1ZJHS4\" type: NORMAL\n    commit tag: \"v1.0.0\"\n    commit id: \"BZ9YP38\" type: NORMAL\n</code></pre> <p>We can also list all tags in the repository.</p> <pre><code>repo.list_tags()\n\n# { 'v1.0.0' }\n</code></pre> <p>and we can look up the snapshot that a tag is based on with <code>lookup_tag</code>.</p> <pre><code>repo.lookup_tag(\"v1.0.0\")\n\n# 'J1ZJHS4EEQW3ATKMV9TG'\n</code></pre> <p>And then finally delete a tag with <code>delete_tag</code>.</p> <p>Note</p> <p>Tags are immutable and once a tag is deleted, it can never be recreated.</p> <pre><code>repo.delete_tag(\"v1.0.0\")\n</code></pre>"},{"location":"icechunk-python/version-control/#conflict-resolution","title":"Conflict Resolution","text":"<p>Icechunk is a serverless distributed system, and as such, it is possible to have multiple users or processes modifying the same data at the same time. Icechunk relies on the consistency guarantees of the underlying storage backends to ensure that the data is always consistent. In situations where two users or processes attempt to modify the same data at the same time, Icechunk will detect the conflict and raise an exception at commit time. This can be illustrated with the following example.</p> <p>Let's create a fresh repository, add some attributes to the root group and create an array named <code>data</code>.</p> <pre><code>import icechunk\nimport numpy as np\nimport zarr\n\nrepo = icechunk.Repository.create(icechunk.in_memory_storage())\nsession = repo.writable_session(branch=\"main\")\nroot = zarr.group(session.store)\nroot.attrs[\"foo\"] = \"bar\"\nroot.create_dataset(\"data\", shape=(10, 10), chunks=(1, 1), dtype=np.int32)\nsession.commit(message=\"Add foo attribute and data array\")\n\n# 'BG0W943WSNFMMVD1FXJ0'\n</code></pre> <p>Lets try to modify the <code>data</code> array in two different sessions, created from the <code>main</code> branch.</p> <pre><code>session1 = repo.writable_session(branch=\"main\")\nsession2 = repo.writable_session(branch=\"main\")\n\nroot1 = zarr.group(session1.store)\nroot2 = zarr.group(session2.store)\n</code></pre> <p>First, we'll modify the attributes of the root group from both sessions.</p> <pre><code>root1.attrs[\"foo\"] = \"bar\"\nroot2.attrs[\"foo\"] = \"baz\"\n</code></pre> <p>and then try to commit the changes.</p> <pre><code>session1.commit(message=\"Update foo attribute on root group\")\nsession2.commit(message=\"Update foo attribute on root group\")\n\n# AE9XS2ZWXT861KD2JGHG\n# ---------------------------------------------------------------------------\n# ConflictError                             Traceback (most recent call last)\n# Cell In[7], line 11\n#      8 root2.attrs[\"foo\"] = \"baz\"\n#      10 print(session1.commit(message=\"Update foo attribute on root group\"))\n# ---&gt; 11 print(session2.commit(message=\"Update foo attribute on root group\"))\n\n# File ~/Developer/icechunk/icechunk-python/python/icechunk/session.py:224, in Session.commit(self, message, metadata)\n#     222     return self._session.commit(message, metadata)\n#     223 except PyConflictError as e:\n# --&gt; 224     raise ConflictError(e) from None\n\n# ConflictError: Failed to commit, expected parent: Some(\"BG0W943WSNFMMVD1FXJ0\"), actual parent: Some(\"AE9XS2ZWXT861KD2JGHG\")\n</code></pre> <p>The first session was able to commit successfully, but the second session failed with a <code>ConflictError</code>. When the second session was created, the changes made were relative to the tip of the <code>main</code> branch, but the tip of the <code>main</code> branch had been modified by the first session.</p> <p>To resolve this conflict, we can use the <code>rebase</code> functionality.</p>"},{"location":"icechunk-python/version-control/#rebasing","title":"Rebasing","text":"<p>To update the second session so it is based off the tip of the <code>main</code> branch, we can use the <code>rebase</code> method.</p> <p>First, we can try to rebase, without merging any conflicting changes:</p> <pre><code>session2.rebase(icechunk.ConflictDetector())\n\n# ---------------------------------------------------------------------------\n# RebaseFailedError                         Traceback (most recent call last)\n# Cell In[8], line 1\n# ----&gt; 1 session2.rebase(icechunk.ConflictDetector())\n\n# File ~/Developer/icechunk/icechunk-python/python/icechunk/session.py:247, in Session.rebase(self, solver)\n#     245     self._session.rebase(solver)\n#     246 except PyRebaseFailedError as e:\n# --&gt; 247     raise RebaseFailedError(e) from None\n\n# RebaseFailedError: Rebase failed on snapshot AE9XS2ZWXT861KD2JGHG: 1 conflicts found\n</code></pre> <p>This however fails because both sessions modified the <code>foo</code> attribute on the root group. We can use the <code>ConflictError</code> to get more information about the conflict.</p> <pre><code>try:\n    session2.rebase(icechunk.ConflictDetector())\nexcept icechunk.RebaseFailedError as e:\n    print(e.conflicts)\n\n# [Conflict(UserAttributesDoubleUpdate, path=/)]\n</code></pre> <p>This tells us that the conflict is caused by the two sessions modifying the user attributes of the root group (<code>/</code>). In this casewe have decided that second session set the <code>foo</code> attribute to the correct value, so we can now try to rebase by instructing the <code>rebase</code> method to use the second session's changes with the <code>BasicConflictSolver</code>.</p> <pre><code>session2.rebase(icechunk.BasicConflictSolver(on_user_attributes_conflict=icechunk.VersionSelection.UseOurs))\n</code></pre> <p>Success! We can now try and commit the changes again.</p> <pre><code>session2.commit(message=\"Update foo attribute on root group\")\n\n# 'SY4WRE8A9TVYMTJPEAHG'\n</code></pre> <p>This same process can be used to resolve conflicts with arrays. Let's try to modify the <code>data</code> array from both sessions.</p> <pre><code>session1 = repo.writable_session(branch=\"main\")\nsession2 = repo.writable_session(branch=\"main\")\n\nroot1 = zarr.group(session1.store)\nroot2 = zarr.group(session2.store)\n\nroot1[\"data\"][0,0] = 1\nroot2[\"data\"][0,:] = 2\n</code></pre> <p>We have now created a conflict, because the first session modified the first element of the <code>data</code> array, and the second session modified the first row of the <code>data</code> array. Let's commit the changes from the second session first, then see what conflicts are reported when we try to commit the changes from the first session.</p> <pre><code>print(session2.commit(message=\"Update first row of data array\"))\nprint(session1.commit(message=\"Update first element of data array\"))\n\n# ---------------------------------------------------------------------------\n# ConflictError                             Traceback (most recent call last)\n# Cell In[15], line 2\n#      1 print(session2.commit(message=\"Update first row of data array\"))\n# ----&gt; 2 print(session1.commit(message=\"Update first element of data array\"))\n\n# File ~/Developer/icechunk/icechunk-python/python/icechunk/session.py:224, in Session.commit(self, message, metadata)\n#     222     return self._session.commit(message, metadata)\n#     223 except PyConflictError as e:\n# --&gt; 224     raise ConflictError(e) from None\n\n# ConflictError: Failed to commit, expected parent: Some(\"SY4WRE8A9TVYMTJPEAHG\"), actual parent: Some(\"5XRDGZPSG747AMMRTWT0\")\n</code></pre> <p>Okay! We have a conflict. Lets see what conflicts are reported.</p> <pre><code>try:\n    session1.rebase(icechunk.ConflictDetector())\nexcept icechunk.RebaseFailedError as e:\n    for conflict in e.conflicts:\n        print(f\"Conflict at {conflict.path}: {conflict.conflicted_chunks}\")\n\n# Conflict at /data: [[0, 0]]\n</code></pre> <p>We get a clear indication of the conflict, and the chunks that are conflicting. In this case we have decided that the first session's changes are correct, so we can again use the <code>BasicConflictSolver</code> to resolve the conflict.</p> <pre><code>session1.rebase(icechunk.BasicConflictSolver(on_chunk_conflict=icechunk.VersionSelection.UseOurs))\nsession1.commit(message=\"Update first element of data array\")\n\n# 'R4WXW2CYNAZTQ3HXTNK0'\n</code></pre> <p>Success! We have now resolved the conflict and committed the changes.</p> <p>Let's look at the value of the <code>data</code> array to confirm that the conflict was resolved correctly.</p> <pre><code>session = repo.readonly_session(branch=\"main\")\nroot = zarr.open_group(session.store, mode=\"r\")\nroot[\"data\"][0,:]\n\n# array([1, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)\n</code></pre> <p>Lastly, if you make changes to non-conflicting chunks or attributes, you can rebase without having to resolve any conflicts.</p> <pre><code>session1 = repo.writable_session(branch=\"main\")\nsession2 = repo.writable_session(branch=\"main\")\n\nroot1 = zarr.group(session1.store)\nroot2 = zarr.group(session2.store)\n\nroot1[\"data\"][3,:] = 3\nroot2[\"data\"][4,:] = 4\n\nsession1.commit(message=\"Update fourth row of data array\")\n\ntry:\n    session2.rebase(icechunk.ConflictDetector())\n    print(\"Rebase succeeded\")\nexcept icechunk.RebaseFailedError as e:\n    print(e.conflicts)\n\nsession2.commit(message=\"Update fifth row of data array\")\n\n# Rebase succeeded\n</code></pre> <p>And now we can see the data in the <code>data</code> array to confirm that the changes were committed correctly.</p> <pre><code>session = repo.readonly_session(branch=\"main\")\nroot = zarr.open_group(session.store, mode=\"r\")\nroot[\"data\"][:,:]\n\n# array([[1, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n#        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n</code></pre>"},{"location":"icechunk-python/version-control/#limitations","title":"Limitations","text":"<p>At the moment, the rebase functionality is limited to resolving conflicts with attributes on arrays and groups, and conflicts with chunks in arrays. Other types of conflicts are not able to be resolved by icechunk yet and must be resolved manually.</p>"},{"location":"icechunk-python/virtual/","title":"Virtual Datasets","text":"<p>Home / icechunk-python / virtual</p>"},{"location":"icechunk-python/virtual/#virtual-datasets","title":"Virtual Datasets","text":"<p>While Icechunk works wonderfully with native chunks managed by Zarr, there is lots of archival data out there in other formats already. To interoperate with such data, Icechunk supports \"Virtual\" chunks, where any number of chunks in a given dataset may reference external data in existing archival formats, such as netCDF, HDF, GRIB, or TIFF. Virtual chunks are loaded directly from the original source without copying or modifying the original achival data files. This enables Icechunk to manage large datasets from existing data without needing that data to be in Zarr format already.</p> <p>Warning</p> <p>While virtual references are fully supported in Icechunk, creating virtual datasets currently relies on using experimental or pre-release versions of open source tools. For full instructions on how to install the required tools and their current statuses see the tracking issue on Github. With time, these experimental features will make their way into the released packages.</p> <p>To create virtual Icechunk datasets with Python, the community utilizes the kerchunk and VirtualiZarr packages.</p> <p><code>kerchunk</code> allows scanning the metadata of existing data files to extract virtual references. It also provides methods to combine these references into larger virtual datasets, which can be exported to it's reference format.</p> <p><code>VirtualiZarr</code> lets users ingest existing data files into virtual datasets using various different tools under the hood, including <code>kerchunk</code>, <code>xarray</code>, <code>zarr</code>, and now <code>icechunk</code>. It does so by creating virtual references to existing data that can be combined and manipulated to create larger virtual datasets using <code>xarray</code>. These datasets can then be exported to <code>kerchunk</code> reference format or to an <code>Icechunk</code> store, without ever copying or moving the existing data files.</p>"},{"location":"icechunk-python/virtual/#creating-a-virtual-dataset-with-virtualizarr","title":"Creating a virtual dataset with VirtualiZarr","text":"<p>We are going to create a virtual dataset pointing to all of the OISST data for August 2024. This data is distributed publicly as netCDF files on AWS S3, with one netCDF file containing the Sea Surface Temperature (SST) data for each day of the month. We are going to use <code>VirtualiZarr</code> to combine all of these files into a single virtual dataset spanning the entire month, then write that dataset to Icechunk for use in analysis.</p> <p>Note</p> <p>At this point you should have followed the instructions here to install the necessary experimental dependencies.</p> <p>Before we get started, we also need to install <code>fsspec</code> and <code>s3fs</code> for working with data on s3.</p> <pre><code>pip install fsspec s3fs\n</code></pre> <p>First, we need to find all of the files we are interested in, we will do this with fsspec using a <code>glob</code> expression to find every netcdf file in the August 2024 folder in the bucket:</p> <pre><code>import fsspec\n\nfs = fsspec.filesystem('s3')\n\noisst_files = fs.glob('s3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/202408/oisst-avhrr-v02r01.*.nc')\n\noisst_files = sorted(['s3://'+f for f in oisst_files])\n#['s3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100101.nc',\n# 's3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100102.nc',\n# 's3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100103.nc',\n# 's3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100104.nc',\n#...\n#]\n</code></pre> <p>Now that we have the filenames of the data we need, we can create virtual datasets with <code>VirtualiZarr</code>. This may take a minute.</p> <pre><code>from virtualizarr import open_virtual_dataset\n\nvirtual_datasets =[\n    open_virtual_dataset(url, indexes={})\n    for url in oisst_files\n]\n</code></pre> <p>We can now use <code>xarray</code> to combine these virtual datasets into one large virtual dataset (For more details on this operation see <code>VirtualiZarr</code>'s documentation). We know that each of our files share the same structure but with a different date. So we are going to concatenate these datasets on the <code>time</code> dimension.</p> <pre><code>import xarray as xr\n\nvirtual_ds = xr.concat(\n    virtual_datasets,\n    dim='time',\n    coords='minimal',\n    compat='override',\n    combine_attrs='override'\n)\n\n#&lt;xarray.Dataset&gt; Size: 257MB\n#Dimensions:  (time: 31, zlev: 1, lat: 720, lon: 1440)\n#Coordinates:\n#    time     (time) float32 124B ManifestArray&lt;shape=(31,), dtype=float32, ch...\n#    lat      (lat) float32 3kB ManifestArray&lt;shape=(720,), dtype=float32, chu...\n#    zlev     (zlev) float32 4B ManifestArray&lt;shape=(1,), dtype=float32, chunk...\n#    lon      (lon) float32 6kB ManifestArray&lt;shape=(1440,), dtype=float32, ch...\n#Data variables:\n#    sst      (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n#    anom     (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n#    ice      (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n#    err      (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n</code></pre> <p>We have a virtual dataset with 31 timestamps! One hint that this worked correctly is that the readout shows the variables and coordinates as <code>ManifestArray</code> instances, the representation that <code>VirtualiZarr</code> uses for virtual arrays. Let's create an Icechunk repo to write this dataset to.</p> <p>Note</p> <p>You will need to modify the <code>StorageConfig</code> bucket name and method to a bucket you have access to. There are multiple options for configuring S3 access: <code>s3_from_config</code>, <code>s3_from_env</code> and <code>s3_anonymous</code>. For more configuration options, see the configuration page.</p> <p>Note</p> <p>Take note of the <code>virtual_ref_config</code> passed into the <code>RepositoryConfig</code> when creating the store. This allows the icechunk store to have the necessary credentials to access the referenced netCDF data on s3 at read time. For more configuration options, see the configuration page.</p> <pre><code>from icechunk import Repository, StorageConfig, RepositoryConfig, VirtualRefConfig\n\nstorage = StorageConfig.s3_from_config(\n    bucket='YOUR_BUCKET_HERE',\n    prefix='icechunk/oisst',\n    region='us-east-1',\n    credentials=S3Credentials(\n        access_key_id=\"REPLACE_ME\",\n        secret_access_key=\"REPLACE_ME\",\n        session_token=\"REPLACE_ME\"\n    )\n)\n\nrepo = Repository.create(\n    storage=storage,\n    config=RepositoryConfig(\n        virtual_ref_config=VirtualRefConfig.s3_anonymous(region='us-east-1'),\n    )\n)\n</code></pre> <p>With the repo created, lets write our virtual dataset to Icechunk with VirtualiZarr!</p> <pre><code>session = repo.writable_session(\"main\")\nvirtual_ds.virtualize.to_icechunk(session.store())\n</code></pre> <p>The refs are written so lets save our progress by committing to the store.</p> <p>Note</p> <p>Your commit hash will be different! For more on the version control features of Icechunk, see the version control page.</p> <pre><code>session.commit(\"My first virtual store!\")\n\n# 'THAJHTYQABGD2B10D5C0'\n</code></pre> <p>Now we can read the dataset from the store using xarray to confirm everything went as expected. <code>xarray</code> reads directly from the Icechunk store because it is a fully compliant <code>zarr Store</code> instance.</p> <pre><code>ds = xr.open_zarr(\n    store,\n    zarr_version=3,\n    consolidated=False,\n    chunks={},\n)\n\n#&lt;xarray.Dataset&gt; Size: 1GB\n#Dimensions:  (lon: 1440, time: 31, zlev: 1, lat: 720)\n#Coordinates:\n#  * lon      (lon) float32 6kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\n#  * zlev     (zlev) float32 4B 0.0\n#  * time     (time) datetime64[ns] 248B 2024-08-01T12:00:00 ... 2024-08-31T12...\n#  * lat      (lat) float32 3kB -89.88 -89.62 -89.38 -89.12 ... 89.38 89.62 89.88\n#Data variables:\n#    sst      (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n#    ice      (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n#    anom     (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n#    err      (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n</code></pre> <p>Success! We have created our full dataset with 31 timesteps spanning the month of august, all with virtual references to pre-existing data files in object store. This means we can now version control our dataset, allowing us to update it, and roll it back to a previous version without copying or moving any data from the original files.</p> <p>Finally, let's make a plot of the sea surface temperature!</p> <pre><code>ds.sst.isel(time=26, zlev=0).plot(x='lon', y='lat', vmin=0)\n</code></pre> <p></p>"},{"location":"icechunk-python/virtual/#virtual-reference-api","title":"Virtual Reference API","text":"<p>While <code>VirtualiZarr</code> is the easiest way to create virtual datasets with Icechunk, the Store API that it uses to create the datasets in Icechunk is public. <code>IcechunkStore</code> contains a <code>set_virtual_ref</code> method that specifies a virtual ref for a specified chunk.</p>"},{"location":"icechunk-python/virtual/#virtual-reference-storage-support","title":"Virtual Reference Storage Support","text":"<p>Currently, Icechunk supports two types of storage for virtual references:</p>"},{"location":"icechunk-python/virtual/#s3-compatible","title":"S3 Compatible","text":"<p>References to files accessible via S3 compatible storage.</p>"},{"location":"icechunk-python/virtual/#example","title":"Example","text":"<p>Here is how we can set the chunk at key <code>c/0</code> to point to a file on an s3 bucket,<code>mybucket</code>, with the prefix <code>my/data/file.nc</code>:</p> <pre><code>store.set_virtual_ref('c/0', 's3://mybucket/my/data/file.nc', offset=1000, length=200)\n</code></pre>"},{"location":"icechunk-python/virtual/#configuration","title":"Configuration","text":"<p>S3 virtual references require configuring credential for the store to be able to access the specified s3 bucket. See the configuration docs for instructions.</p>"},{"location":"icechunk-python/virtual/#local-filesystem","title":"Local Filesystem","text":"<p>References to files accessible via local filesystem. This requires any file paths to be absolute at this time.</p>"},{"location":"icechunk-python/virtual/#example_1","title":"Example","text":"<p>Here is how we can set the chunk at key <code>c/0</code> to point to a file on my local filesystem located at <code>/path/to/my/file.nc</code>:</p> <pre><code>store.set_virtual_ref('c/0', 'file:///path/to/my/file.nc', offset=20, length=100)\n</code></pre> <p>No extra configuration is necessary for local filesystem references.</p>"},{"location":"icechunk-python/virtual/#virtual-reference-file-format-support","title":"Virtual Reference File Format Support","text":"<p>Currently, Icechunk supports <code>HDF5</code> and <code>netcdf4</code> files for use in virtual references. See the tracking issue for more info.</p>"},{"location":"icechunk-python/xarray/","title":"Xarray","text":"<p>Home / icechunk-python / xarray</p>"},{"location":"icechunk-python/xarray/#icechunk-xarray","title":"Icechunk + Xarray","text":"<p>Icechunk was designed to work seamlessly with Xarray. Xarray users can read and write data to Icechunk using <code>xarray.open_zarr</code> and <code>icechunk.xarray.to_icechunk</code> methods.</p> <p>Warning</p> <p>Using Xarray and Icechunk together currently requires installing Xarray &gt;= 2025.1.1.</p> <pre><code>pip install \"xarray&gt;=2025.1.1\"\n</code></pre> <p><code>to_icechunk</code> vs <code>to_zarr</code></p> <p><code>xarray.Dataset.to_zarr</code> and <code>to_icechunk</code> are nearly functionally identical.</p> <p>In a distributed context, e.g. writes orchestrated with <code>multiprocesssing</code> or a <code>dask.distributed.Client</code> and <code>dask.array</code>, you must use <code>to_icechunk</code>. This will ensure that you can execute a commit that successfully records all remote writes. See these docs on orchestrating parallel writes and these docs on dask.array with distributed for more.</p> <p>If using <code>to_zarr</code>, remember to set <code>zarr_format=3, consolidated=False</code>. Consolidated metadata is unnecessary (and unsupported) in Icechunk. Icechunk already organizes the dataset metadata in a way that makes it very fast to fetch from storage.</p> <p>In this example, we'll explain how to create a new Icechunk repo, write some sample data to it, and append data a second block of data using Icechunk's version control features.</p>"},{"location":"icechunk-python/xarray/#create-a-new-repo","title":"Create a new repo","text":"<p>Similar to the example in quickstart, we'll create an Icechunk repo in S3 or a local file system. You will need to replace the <code>StorageConfig</code> with a bucket or file path that you have access to.</p> <pre><code>import xarray as xr\nimport icechunk\n</code></pre> S3 StorageLocal Storage <pre><code>storage_config = icechunk.s3_storage(\n    bucket=\"icechunk-test\",\n    prefix=\"xarray-demo\"\n)\nrepo = icechunk.Repository.create(storage_config)\n</code></pre> <pre><code>storage_config = icechunk.local_filesystem_storage(\"./icechunk-xarray\")\nrepo = icechunk.Repository.create(storage_config)\n</code></pre>"},{"location":"icechunk-python/xarray/#open-tutorial-dataset-from-xarray","title":"Open tutorial dataset from Xarray","text":"<p>For this demo, we'll open Xarray's RASM tutorial dataset and split it into two blocks. We'll write the two blocks to Icechunk in separate transactions later in the this example.</p> <p>Note</p> <p>Downloading xarray tutorial data requires pooch and netCDF4. These can be installed with</p> <pre><code>pip install pooch netCDF4\n</code></pre> <pre><code>ds = xr.tutorial.open_dataset('rasm')\n\nds1 = ds.isel(time=slice(None, 18))  # part 1\nds2 = ds.isel(time=slice(18, None))  # part 2\n</code></pre>"},{"location":"icechunk-python/xarray/#write-xarray-data-to-icechunk","title":"Write Xarray data to Icechunk","text":"<p>Create a new writable session on the <code>main</code> branch to get the <code>IcechunkStore</code>:</p> <pre><code>session = repo.writable_session(\"main\")\n</code></pre> <p>Writing Xarray data to Icechunk is as easy as calling <code>to_icechunk</code>:</p> <pre><code>from icechunk.xarray import to_icechunk\n\nto_icechunk(ds, session)\n</code></pre> <p>After writing, we commit the changes using the session:</p> <pre><code>session.commit(\"add RASM data to store\")\n# output: 'ME4VKFPA5QAY0B2YSG8G'\n</code></pre>"},{"location":"icechunk-python/xarray/#append-to-an-existing-store","title":"Append to an existing store","text":"<p>Next, we want to add a second block of data to our store. Above, we created <code>ds2</code> for just this reason. Again, we'll use <code>Dataset.to_zarr</code>, this time with <code>append_dim='time'</code>.</p> <pre><code># we have to get a new session after committing\nsession = repo.writable_session(\"main\")\nto_icechunk(ds2, session, append_dim='time')\n</code></pre> <p>And then we'll commit the changes:</p> <pre><code>session.commit(\"append more data\")\n# output: 'WW4V8V34QCZ2NXTD5DXG'\n</code></pre>"},{"location":"icechunk-python/xarray/#reading-data-with-xarray","title":"Reading data with Xarray","text":"<p>To read data stored in Icechunk with Xarray, we'll use <code>xarray.open_zarr</code>:</p> <pre><code>xr.open_zarr(store, consolidated=False)\n# output: &lt;xarray.Dataset&gt; Size: 17MB\n# Dimensions:  (time: 36, y: 205, x: 275)\n# Coordinates:\n#   * time     (time) object 288B 1980-09-16 12:00:00 ... 1983-08-17 00:00:00\n#     xc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#     yc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n# Dimensions without coordinates: y, x\n# Data variables:\n#     Tair     (time, y, x) float64 16MB dask.array&lt;chunksize=(5, 103, 138), meta=np.ndarray&gt;\n# Attributes:\n#     NCO:                       netCDF Operators version 4.7.9 (Homepage = htt...\n#     comment:                   Output from the Variable Infiltration Capacity...\n#     convention:                CF-1.4\n#     history:                   Fri Aug  7 17:57:38 2020: ncatted -a bounds,,d...\n#     institution:               U.W.\n#     nco_openmp_thread_number:  1\n#     output_frequency:          daily\n#     output_mode:               averaged\n#     references:                Based on the initial model of Liang et al., 19...\n#     source:                    RACM R1002RBRxaaa01a\n#     title:                     /workspace/jhamman/processed/R1002RBRxaaa01a/l...\n</code></pre> <p>We can also read data from previous snapshots by checking out prior versions:</p> <pre><code>session = repo.readable_session(snapshot_id='ME4VKFPA5QAY0B2YSG8G')\n\nxr.open_zarr(session.store, consolidated=False)\n# &lt;xarray.Dataset&gt; Size: 9MB\n# Dimensions:  (time: 18, y: 205, x: 275)\n# Coordinates:\n#     xc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#     yc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#   * time     (time) object 144B 1980-09-16 12:00:00 ... 1982-02-15 12:00:00\n# Dimensions without coordinates: y, x\n# Data variables:\n#     Tair     (time, y, x) float64 8MB dask.array&lt;chunksize=(5, 103, 138), meta=np.ndarray&gt;\n# Attributes:\n#     NCO:                       netCDF Operators version 4.7.9 (Homepage = htt...\n#     comment:                   Output from the Variable Infiltration Capacity...\n#     convention:                CF-1.4\n#     history:                   Fri Aug  7 17:57:38 2020: ncatted -a bounds,,d...\n#     institution:               U.W.\n#     nco_openmp_thread_number:  1\n#     output_frequency:          daily\n#     output_mode:               averaged\n#     references:                Based on the initial model of Liang et al., 19...\n#     source:                    RACM R1002RBRxaaa01a\n#     title:                     /workspace/jhamman/processed/R1002RBRxaaa01a/l...\n</code></pre> <p>Notice that this second <code>xarray.Dataset</code> has a time dimension of length 18 whereas the first has a time dimension of length 36.</p>"},{"location":"icechunk-python/xarray/#next-steps","title":"Next steps","text":"<p>For more details on how to use Xarray's Zarr integration, checkout Xarray's documentation.</p>"},{"location":"icechunk-python/cheatsheets/","title":"Index","text":"<p>Home / icechunk-python / cheatsheets</p>"},{"location":"icechunk-python/cheatsheets/#index-of-icechunk-pythoncheatsheets","title":"Index of icechunk-python/cheatsheets","text":"<ul> <li>Icechunk for Git Users</li> </ul>"},{"location":"icechunk-python/cheatsheets/git-users/","title":"Icechunk for Git Users","text":"<p>Home / icechunk-python / cheatsheets / git-users</p>"},{"location":"icechunk-python/cheatsheets/git-users/#icechunk-for-git-users","title":"Icechunk for Git Users","text":"<p>While Icechunk does not work the same way as git, it borrows from a lot of the same concepts. This guide will talk through the version control features of Icechunk from the perspective of a user that is familiar with git.</p>"},{"location":"icechunk-python/cheatsheets/git-users/#repositories","title":"Repositories","text":"<p>The main primitive in Icechunk is the repository. Similar to git, the repository is the entry point for all operations and the source of truth for the data. However there are many important differences.</p> <p>When developing with git, you will commonly have a local and remote copy of the repository. The local copy is where you do all of your work. The remote copy is where you push your changes when you are ready to share them with others. In Icechunk, there is not local or remote repository, but a single repository that typically exists in a cloud storage bucket. This means that every transaction is saved to the same repository that others may be working on. Icechunk uses the consistency guarantees from storage systems to provide strong consistency even when multiple users are working on the same repository.</p>"},{"location":"icechunk-python/cheatsheets/git-users/#working-with-branches","title":"Working with branches","text":"<p>Icechunk has branches similar to git.</p>"},{"location":"icechunk-python/cheatsheets/git-users/#creating-a-branch","title":"Creating a branch","text":"<p>In practice, this means the workflow is different from git. For instance, I wanted to make a new branch based on the <code>main</code> branch on my existing git repository and then commit my changes in git this is how I would do it:</p> <pre><code># Assume currently on main branch\n# create branch\ngit checkout -b my-new-branch\n# stage changes\ngit add myfile.txt\n# commit changes\ngit commit -m \"My new branch\"\n# push to remote\ngit push origin -u my-new-branch\n</code></pre> <p>In Icechunk, you would do the following:</p> <pre><code># We create the branch\nrepo.create_branch(\"my-new-branch\", repo.lookup_branch(\"main\"))\n# create a writable session\nsession = repo.writable_session(\"my-new-branch\")\n...  # make some changes\n# commit the changes\nsession.commit(\"My new branch\")\n</code></pre> <p>Two things to note:</p> <ol> <li>When we create a branch, the branch is now available for any other instance of this <code>Repository</code> object. It is not a local branch, it is created in the repositories storage backend.</li> <li>When we commit the changes are immediately visible to other users of the repository. There is not concept of a local commit, all snapshots happen in the storage backend.</li> </ol>"},{"location":"icechunk-python/cheatsheets/git-users/#checking-out-a-branch","title":"Checking out a branch","text":"<p>In git, you can check out a branch by using the <code>git checkout</code> command. Icechunk does not have the concept of checking out a branch, instead you create <code>Session</code>s that are based on the tip of a branch.</p> <p>We can either check out a branch for read-only access or for read-write access.</p> <pre><code># check out a branch for read-only access\nsession = repo.readonly_session(branch=\"my-new-branch\")\n# check out a branch for read-write access\nsession = repo.writable_session(\"my-new-branch\")\n</code></pre> <p>Once we have checked out a session, the <code>store</code> method will return a <code>Store</code> object that we can use to read and write data to the repository with <code>zarr</code>.</p>"},{"location":"icechunk-python/cheatsheets/git-users/#resetting-a-branch","title":"Resetting a branch","text":"<p>In git, you can reset a branch to previous commit. Similarly, in Icechunk you can reset a branch to a previous snapshot.</p> <pre><code># reset the branch to the previous snapshot\nrepo.reset_branch(\"my-new-branch\", \"198273178639187\")\n</code></pre> <p>Warning</p> <p>This is a destructive operation. It will overwrite the branch reference with the snapshot immediately. It can only be undone by resetting the branch again.</p> <p>At this point, the tip of the branch is now the snapshot <code>198273178639187</code> and any changes made to the branch will be based on this snapshot. This also means the history of the branch is now same as the ancestry of this snapshot.</p>"},{"location":"icechunk-python/cheatsheets/git-users/#branch-history","title":"Branch History","text":"<p>In Icechunk, you can view the history of a branch by using the <code>repo.ancestry()</code> command, similar to the <code>git log</code> command.</p> <pre><code>repo.ancestry(branch=\"my-new-branch\")\n\n#[Snapshot(id='198273178639187', ...), ...]\n</code></pre>"},{"location":"icechunk-python/cheatsheets/git-users/#listing-branches","title":"Listing branches","text":"<p>We can also list all branches in the repository.</p> <pre><code>repo.list_branches()\n\n# ['main', 'my-new-branch']\n</code></pre> <p>You can also view the snapshot that a branch is based on by using the <code>repo.lookup_branch()</code> command.</p> <pre><code>repo.lookup_branch(\"my-new-branch\")\n\n# '198273178639187'\n</code></pre>"},{"location":"icechunk-python/cheatsheets/git-users/#deleting-a-branch","title":"Deleting a branch","text":"<p>You can delete a branch by using the <code>repo.delete_branch()</code> command.</p> <pre><code>repo.delete_branch(\"my-new-branch\")\n</code></pre>"},{"location":"icechunk-python/cheatsheets/git-users/#working-with-tags","title":"Working with tags","text":"<p>Icechunk tags are also similar to git tags.</p>"},{"location":"icechunk-python/cheatsheets/git-users/#creating-a-tag","title":"Creating a tag","text":"<p>We create a tag by providing a name and a snapshot id, similar to the <code>git tag</code> command.</p> <pre><code>repo.create_tag(\"my-new-tag\", \"198273178639187\")\n</code></pre> <p>Just like git tags, Icechunk tags are immutable and cannot be modified. They can however be deleted like git tags:</p> <pre><code>repo.delete_tag(\"my-new-tag\")\n</code></pre> <p>However, unlike git tags once a tag is deleted it cannot be recreated. This will now raise an error:</p> <pre><code>repo.create_tag(\"my-new-tag\", \"198273178639187\")\n\n# IcechunkError: Tag with name 'my-new-tag' already exists\n</code></pre>"},{"location":"icechunk-python/cheatsheets/git-users/#listing-tags","title":"Listing tags","text":"<p>We can also list all tags in the repository.</p> <pre><code>repo.list_tags()\n\n# ['my-new-tag']\n</code></pre>"},{"location":"icechunk-python/cheatsheets/git-users/#viewing-tag-history","title":"Viewing tag history","text":"<p>We can also view the history of a tag by using the <code>repo.ancestry()</code> command.</p> <pre><code>repo.ancestry(tag=\"my-new-tag\")\n</code></pre> <p>This will return a list of snapshots that are ancestors of the tag. Similar to branches we can lookup the snapshot that a tag is based on by using the <code>repo.lookup_tag()</code> command.</p> <pre><code>repo.lookup_tag(\"my-new-tag\")\n\n# '198273178639187'\n</code></pre>"},{"location":"icechunk-python/cheatsheets/git-users/#merging-and-rebasing","title":"Merging and Rebasing","text":"<p>Git supports merging and rebasing branches together. Icechunk currently does not support merging and rebasing branches together. It does support rebasing sessions that share the same branch.</p>"},{"location":"icechunk-python/examples/dask_write/","title":"Dask write","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nThis example uses Dask as a task orchestration framework\nto write or update an array in an Icechunk repository.\nTo write an Xarray object with dask array use `icechunk.xarray.to_icechunk`\n\nTo understand all the available options run:\n```\npython ./examples/dask_write.py --help\npython ./examples/dask_write.py create --help\npython ./examples/dask_write.py update --help\npython ./examples/dask_write.py verify --help\n```\n\nExample usage:\n\n```\npython ./examples/dask_write.py create --url s3://my-bucket/my-icechunk-repo --t-chunks 100000 --x-chunks 4 --y-chunks 4 --chunk-x-size 112 --chunk-y-size 112\npython ./examples/dask_write.py update --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16\npython ./examples/dask_write.py verify --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16\n```\n\nThe work is split into three different commands.\n* `create` initializes the repository and the array, without writing any chunks. For this example\n   we chose a 3D array that simulates a dataset that needs backfilling across its time dimension.\n* `update` can be called multiple times to write a number of \"pancakes\" to the array.\n  It does so by distributing the work among Dask workers, in small tasks, one pancake per task.\n  The example invocation above, will write 1,500 pancakes using 16 Dask workers.\n* `verify` can read a part of the array and check that it contains the required data.\n\nIcechunk can do distributed writes to object store, but currently, it cannot use the Dask array API\n(we are working on it, see https://github.com/earth-mover/icechunk/issues/185).\nDask can still be used to read and write to Icechunk from multiple processes and machines, we just need to use a lower level\nDask API based, for example, in `map/gather`. This mechanism is what we show in this example.\n\"\"\"\n</pre> \"\"\" This example uses Dask as a task orchestration framework to write or update an array in an Icechunk repository. To write an Xarray object with dask array use `icechunk.xarray.to_icechunk`  To understand all the available options run: ``` python ./examples/dask_write.py --help python ./examples/dask_write.py create --help python ./examples/dask_write.py update --help python ./examples/dask_write.py verify --help ```  Example usage:  ``` python ./examples/dask_write.py create --url s3://my-bucket/my-icechunk-repo --t-chunks 100000 --x-chunks 4 --y-chunks 4 --chunk-x-size 112 --chunk-y-size 112 python ./examples/dask_write.py update --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16 python ./examples/dask_write.py verify --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16 ```  The work is split into three different commands. * `create` initializes the repository and the array, without writing any chunks. For this example    we chose a 3D array that simulates a dataset that needs backfilling across its time dimension. * `update` can be called multiple times to write a number of \"pancakes\" to the array.   It does so by distributing the work among Dask workers, in small tasks, one pancake per task.   The example invocation above, will write 1,500 pancakes using 16 Dask workers. * `verify` can read a part of the array and check that it contains the required data.  Icechunk can do distributed writes to object store, but currently, it cannot use the Dask array API (we are working on it, see https://github.com/earth-mover/icechunk/issues/185). Dask can still be used to read and write to Icechunk from multiple processes and machines, we just need to use a lower level Dask API based, for example, in `map/gather`. This mechanism is what we show in this example. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import argparse\nfrom dataclasses import dataclass\nfrom typing import Any, cast\nfrom urllib.parse import urlparse\n</pre> import argparse from dataclasses import dataclass from typing import Any, cast from urllib.parse import urlparse In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>import icechunk\nimport zarr\nfrom dask.distributed import Client\nfrom dask.distributed import print as dprint\n</pre> import icechunk import zarr from dask.distributed import Client from dask.distributed import print as dprint In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass Task:\n    \"\"\"A task distributed to Dask workers\"\"\"\n\n    session: (\n        icechunk.Session\n    )  # The worker will use this Icechunk session to read/write to the dataset\n    time: (\n        int  # The position in the coordinate dimension where the read/write should happen\n    )\n    seed: int  # An RNG seed used to generate or recreate random data for the array\n</pre> @dataclass class Task:     \"\"\"A task distributed to Dask workers\"\"\"      session: (         icechunk.Session     )  # The worker will use this Icechunk session to read/write to the dataset     time: (         int  # The position in the coordinate dimension where the read/write should happen     )     seed: int  # An RNG seed used to generate or recreate random data for the array In\u00a0[\u00a0]: Copied! <pre>def generate_task_array(task: Task, shape: tuple[int, ...]) -&gt; np.typing.ArrayLike:\n    \"\"\"Generates a randm array with the given shape and using the seed in the Task\"\"\"\n    np.random.seed(task.seed)\n    return np.random.rand(*shape)\n</pre> def generate_task_array(task: Task, shape: tuple[int, ...]) -&gt; np.typing.ArrayLike:     \"\"\"Generates a randm array with the given shape and using the seed in the Task\"\"\"     np.random.seed(task.seed)     return np.random.rand(*shape) In\u00a0[\u00a0]: Copied! <pre>def execute_write_task(task: Task) -&gt; icechunk.Session:\n    \"\"\"Execute task as a write task.\n\n    This will read the time coordinade from `task` and write a \"pancake\" in that position,\n    using random data. Random data is generated using the task seed.\n\n    Returns the Icechunk session after the write is done.\n\n    As you can see Icechunk session can be passed to remote workers, and returned from them.\n    The reason to return the session is that we'll need all the remote session, when they are\n    done, to be able to do a single, global commit to Icechunk.\n    \"\"\"\n\n    session = task.session\n    store = session.store\n\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n    dprint(f\"Writing at t={task.time}\")\n    data = generate_task_array(task, array.shape[0:2])\n    array[:, :, task.time] = data\n    dprint(f\"Writing at t={task.time} done\")\n    return session\n</pre> def execute_write_task(task: Task) -&gt; icechunk.Session:     \"\"\"Execute task as a write task.      This will read the time coordinade from `task` and write a \"pancake\" in that position,     using random data. Random data is generated using the task seed.      Returns the Icechunk session after the write is done.      As you can see Icechunk session can be passed to remote workers, and returned from them.     The reason to return the session is that we'll need all the remote session, when they are     done, to be able to do a single, global commit to Icechunk.     \"\"\"      session = task.session     store = session.store      group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])     dprint(f\"Writing at t={task.time}\")     data = generate_task_array(task, array.shape[0:2])     array[:, :, task.time] = data     dprint(f\"Writing at t={task.time} done\")     return session In\u00a0[\u00a0]: Copied! <pre>def execute_read_task(task: Task) -&gt; None:\n    \"\"\"Execute task as a read task.\n\n    This will read the time coordinade from `task` and read a \"pancake\" in that position.\n    Then it will assert the data is valid by re-generating the random data from the passed seed.\n\n    As you can see Icechunk sessions can be passed to remote workers.\n    \"\"\"\n\n    session = task.session\n    store = session.store\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n\n    actual = array[:, :, task.time]\n    expected = generate_task_array(task, array.shape[0:2])\n    np.testing.assert_array_equal(actual, expected)\n    dprint(f\"t={task.time} verified\")\n</pre> def execute_read_task(task: Task) -&gt; None:     \"\"\"Execute task as a read task.      This will read the time coordinade from `task` and read a \"pancake\" in that position.     Then it will assert the data is valid by re-generating the random data from the passed seed.      As you can see Icechunk sessions can be passed to remote workers.     \"\"\"      session = task.session     store = session.store     group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])      actual = array[:, :, task.time]     expected = generate_task_array(task, array.shape[0:2])     np.testing.assert_array_equal(actual, expected)     dprint(f\"t={task.time} verified\") In\u00a0[\u00a0]: Copied! <pre>def storage_config(args: argparse.Namespace) -&gt; dict[str, Any]:\n    \"\"\"Return the Icechunk S3 configuration map\"\"\"\n    bucket = args.url.netloc\n    prefix = args.url.path[1:]\n    return {\n        \"bucket\": bucket,\n        \"prefix\": prefix,\n        \"region\": \"us-east-1\",\n    }\n</pre> def storage_config(args: argparse.Namespace) -&gt; dict[str, Any]:     \"\"\"Return the Icechunk S3 configuration map\"\"\"     bucket = args.url.netloc     prefix = args.url.path[1:]     return {         \"bucket\": bucket,         \"prefix\": prefix,         \"region\": \"us-east-1\",     } In\u00a0[\u00a0]: Copied! <pre>def repository_config(args: argparse.Namespace) -&gt; icechunk.RepositoryConfig:\n    \"\"\"Return the Icechunk repo configuration.\n\n    We lower the default to make sure we write chunks and not inline them.\n    \"\"\"\n    config = icechunk.RepositoryConfig.default()\n    config.inline_chunk_threshold_bytes = 1\n    return config\n</pre> def repository_config(args: argparse.Namespace) -&gt; icechunk.RepositoryConfig:     \"\"\"Return the Icechunk repo configuration.      We lower the default to make sure we write chunks and not inline them.     \"\"\"     config = icechunk.RepositoryConfig.default()     config.inline_chunk_threshold_bytes = 1     return config In\u00a0[\u00a0]: Copied! <pre>def create(args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the create subcommand.\n\n    Creates an Icechunk repo, a root group and an array named \"array\"\n    with the shape passed as arguments.\n\n    Commits the Icechunk repository when done.\n    \"\"\"\n    repo = icechunk.Repository.create(\n        storage=icechunk.s3_storage(**storage_config(args)),\n        config=repository_config(args),\n    )\n\n    session = repo.writable_session(\"main\")\n    store = session.store\n\n    group = zarr.group(store=store, overwrite=True)\n    shape = (\n        args.x_chunks * args.chunk_x_size,\n        args.y_chunks * args.chunk_y_size,\n        args.t_chunks * 1,\n    )\n    chunk_shape = (args.chunk_x_size, args.chunk_y_size, 1)\n\n    group.create_array(\n        \"array\",\n        shape=shape,\n        chunk_shape=chunk_shape,\n        dtype=\"f8\",\n        fill_value=float(\"nan\"),\n    )\n    first_snapshot = session.commit(\"array created\")\n    print(f\"Array initialized, snapshot {first_snapshot}\")\n</pre> def create(args: argparse.Namespace) -&gt; None:     \"\"\"Execute the create subcommand.      Creates an Icechunk repo, a root group and an array named \"array\"     with the shape passed as arguments.      Commits the Icechunk repository when done.     \"\"\"     repo = icechunk.Repository.create(         storage=icechunk.s3_storage(**storage_config(args)),         config=repository_config(args),     )      session = repo.writable_session(\"main\")     store = session.store      group = zarr.group(store=store, overwrite=True)     shape = (         args.x_chunks * args.chunk_x_size,         args.y_chunks * args.chunk_y_size,         args.t_chunks * 1,     )     chunk_shape = (args.chunk_x_size, args.chunk_y_size, 1)      group.create_array(         \"array\",         shape=shape,         chunk_shape=chunk_shape,         dtype=\"f8\",         fill_value=float(\"nan\"),     )     first_snapshot = session.commit(\"array created\")     print(f\"Array initialized, snapshot {first_snapshot}\") In\u00a0[\u00a0]: Copied! <pre>def update(args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the update subcommand.\n\n    Uses Dask to write chunks to the Icechunk repository. Currently Icechunk cannot\n    use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we\n    can still use a lower level API to do the writes:\n    * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.\n    * We use Dask's `map` to ship the `Task` to a worker\n    * The `Task` includes a copy of the Icechunk Session, so workers can do the writes\n    * When workers are done, they send their Session back\n    * When all workers are done (Dask's `gather`), we take all Sessions and do a distributed commit in Icechunk\n    \"\"\"\n\n    repo = icechunk.Repository.open(\n        storage=icechunk.s3_storage(**storage_config(args)),\n        config=repository_config(args),\n    )\n\n    session = repo.writable_session(\"main\")\n\n    tasks = [\n        Task(\n            session=session,\n            time=time,\n            seed=time,\n        )\n        for time in range(args.t_from, args.t_to, 1)\n    ]\n\n    client = Client(n_workers=args.workers, threads_per_worker=1)\n\n    map_result = client.map(execute_write_task, tasks)\n    worker_sessions = client.gather(map_result)\n\n    print(\"Starting distributed commit\")\n    # we can use the current session as the commit coordinator, because it doesn't have any pending changes,\n    # all changes come from the tasks, Icechunk doesn't care about where the changes come from, the only\n    # important thing is to not count changes twice\n    for worker_session in worker_sessions:\n        session.merge(worker_session)\n    commit_res = session.commit(\"distributed commit\")\n    assert commit_res\n    print(\"Distributed commit done\")\n</pre> def update(args: argparse.Namespace) -&gt; None:     \"\"\"Execute the update subcommand.      Uses Dask to write chunks to the Icechunk repository. Currently Icechunk cannot     use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we     can still use a lower level API to do the writes:     * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.     * We use Dask's `map` to ship the `Task` to a worker     * The `Task` includes a copy of the Icechunk Session, so workers can do the writes     * When workers are done, they send their Session back     * When all workers are done (Dask's `gather`), we take all Sessions and do a distributed commit in Icechunk     \"\"\"      repo = icechunk.Repository.open(         storage=icechunk.s3_storage(**storage_config(args)),         config=repository_config(args),     )      session = repo.writable_session(\"main\")      tasks = [         Task(             session=session,             time=time,             seed=time,         )         for time in range(args.t_from, args.t_to, 1)     ]      client = Client(n_workers=args.workers, threads_per_worker=1)      map_result = client.map(execute_write_task, tasks)     worker_sessions = client.gather(map_result)      print(\"Starting distributed commit\")     # we can use the current session as the commit coordinator, because it doesn't have any pending changes,     # all changes come from the tasks, Icechunk doesn't care about where the changes come from, the only     # important thing is to not count changes twice     for worker_session in worker_sessions:         session.merge(worker_session)     commit_res = session.commit(\"distributed commit\")     assert commit_res     print(\"Distributed commit done\") In\u00a0[\u00a0]: Copied! <pre>def verify(args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the verify subcommand.\n\n    Uses Dask to read and verify chunks from the Icechunk repository. Currently Icechunk cannot\n    use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we\n    can still use a lower level API to do the verification:\n    * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.\n    * We use Dask's `map` to ship the `Task` to a worker\n    * The `Task` includes a copy of the Icechunk Store, so workers can do the Icechunk reads\n    \"\"\"\n    repo = icechunk.Repository.open(\n        storage=icechunk.s3_storage(**storage_config(args)),\n        config=repository_config(args),\n    )\n\n    session = repo.writable_session(\"main\")\n    store = session.store\n\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n    print(f\"Found an array with shape: {array.shape}\")\n\n    tasks = [\n        Task(\n            session=session,\n            time=time,\n            seed=time,\n        )\n        for time in range(args.t_from, args.t_to, 1)\n    ]\n\n    client = Client(n_workers=args.workers, threads_per_worker=1)\n\n    map_result = client.map(execute_read_task, tasks)\n    client.gather(map_result)\n    print(\"done, all good\")\n</pre> def verify(args: argparse.Namespace) -&gt; None:     \"\"\"Execute the verify subcommand.      Uses Dask to read and verify chunks from the Icechunk repository. Currently Icechunk cannot     use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we     can still use a lower level API to do the verification:     * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.     * We use Dask's `map` to ship the `Task` to a worker     * The `Task` includes a copy of the Icechunk Store, so workers can do the Icechunk reads     \"\"\"     repo = icechunk.Repository.open(         storage=icechunk.s3_storage(**storage_config(args)),         config=repository_config(args),     )      session = repo.writable_session(\"main\")     store = session.store      group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])     print(f\"Found an array with shape: {array.shape}\")      tasks = [         Task(             session=session,             time=time,             seed=time,         )         for time in range(args.t_from, args.t_to, 1)     ]      client = Client(n_workers=args.workers, threads_per_worker=1)      map_result = client.map(execute_read_task, tasks)     client.gather(map_result)     print(\"done, all good\") In\u00a0[\u00a0]: Copied! <pre>def main() -&gt; None:\n    \"\"\"Main entry point for the script.\n\n    Parses arguments and delegates to a subcommand.\n    \"\"\"\n\n    global_parser = argparse.ArgumentParser(prog=\"dask_write\")\n    global_parser.add_argument(\n        \"--url\",\n        type=str,\n        help=\"url for the repository: s3://bucket/optional-prefix/repository-name\",\n        required=True,\n    )\n    subparsers = global_parser.add_subparsers(title=\"subcommands\", required=True)\n\n    create_parser = subparsers.add_parser(\"create\", help=\"create repo and array\")\n    create_parser.add_argument(\n        \"--x-chunks\", type=int, help=\"number of chunks in the x dimension\", default=4\n    )\n    create_parser.add_argument(\n        \"--y-chunks\", type=int, help=\"number of chunks in the y dimension\", default=4\n    )\n    create_parser.add_argument(\n        \"--t-chunks\", type=int, help=\"number of chunks in the t dimension\", default=1000\n    )\n    create_parser.add_argument(\n        \"--chunk-x-size\",\n        type=int,\n        help=\"size of chunks in the x dimension\",\n        default=112,\n    )\n    create_parser.add_argument(\n        \"--chunk-y-size\",\n        type=int,\n        help=\"size of chunks in the y dimension\",\n        default=112,\n    )\n    create_parser.set_defaults(command=\"create\")\n\n    update_parser = subparsers.add_parser(\"update\", help=\"add chunks to the array\")\n    update_parser.add_argument(\n        \"--t-from\",\n        type=int,\n        help=\"time position where to start adding chunks (included)\",\n        required=True,\n    )\n    update_parser.add_argument(\n        \"--t-to\",\n        type=int,\n        help=\"time position where to stop adding chunks (not included)\",\n        required=True,\n    )\n    update_parser.add_argument(\n        \"--workers\", type=int, help=\"number of workers to use\", required=True\n    )\n    update_parser.set_defaults(command=\"update\")\n\n    verify_parser = subparsers.add_parser(\"verify\", help=\"verify array chunks\")\n    verify_parser.add_argument(\n        \"--t-from\",\n        type=int,\n        help=\"time position where to start adding chunks (included)\",\n        required=True,\n    )\n    verify_parser.add_argument(\n        \"--t-to\",\n        type=int,\n        help=\"time position where to stop adding chunks (not included)\",\n        required=True,\n    )\n    verify_parser.add_argument(\n        \"--workers\", type=int, help=\"number of workers to use\", required=True\n    )\n    verify_parser.set_defaults(command=\"verify\")\n\n    args = global_parser.parse_args()\n    url = urlparse(args.url, \"s3\")\n    if (\n        url.scheme != \"s3\"\n        or url.netloc == \"\"\n        or url.path == \"\"\n        or url.params != \"\"\n        or url.query != \"\"\n        or url.fragment != \"\"\n    ):\n        raise ValueError(f\"Invalid url {args.url}\")\n\n    args.url = url\n\n    match args.command:\n        case \"create\":\n            create(args)\n        case \"update\":\n            update(args)\n        case \"verify\":\n            verify(args)\n</pre> def main() -&gt; None:     \"\"\"Main entry point for the script.      Parses arguments and delegates to a subcommand.     \"\"\"      global_parser = argparse.ArgumentParser(prog=\"dask_write\")     global_parser.add_argument(         \"--url\",         type=str,         help=\"url for the repository: s3://bucket/optional-prefix/repository-name\",         required=True,     )     subparsers = global_parser.add_subparsers(title=\"subcommands\", required=True)      create_parser = subparsers.add_parser(\"create\", help=\"create repo and array\")     create_parser.add_argument(         \"--x-chunks\", type=int, help=\"number of chunks in the x dimension\", default=4     )     create_parser.add_argument(         \"--y-chunks\", type=int, help=\"number of chunks in the y dimension\", default=4     )     create_parser.add_argument(         \"--t-chunks\", type=int, help=\"number of chunks in the t dimension\", default=1000     )     create_parser.add_argument(         \"--chunk-x-size\",         type=int,         help=\"size of chunks in the x dimension\",         default=112,     )     create_parser.add_argument(         \"--chunk-y-size\",         type=int,         help=\"size of chunks in the y dimension\",         default=112,     )     create_parser.set_defaults(command=\"create\")      update_parser = subparsers.add_parser(\"update\", help=\"add chunks to the array\")     update_parser.add_argument(         \"--t-from\",         type=int,         help=\"time position where to start adding chunks (included)\",         required=True,     )     update_parser.add_argument(         \"--t-to\",         type=int,         help=\"time position where to stop adding chunks (not included)\",         required=True,     )     update_parser.add_argument(         \"--workers\", type=int, help=\"number of workers to use\", required=True     )     update_parser.set_defaults(command=\"update\")      verify_parser = subparsers.add_parser(\"verify\", help=\"verify array chunks\")     verify_parser.add_argument(         \"--t-from\",         type=int,         help=\"time position where to start adding chunks (included)\",         required=True,     )     verify_parser.add_argument(         \"--t-to\",         type=int,         help=\"time position where to stop adding chunks (not included)\",         required=True,     )     verify_parser.add_argument(         \"--workers\", type=int, help=\"number of workers to use\", required=True     )     verify_parser.set_defaults(command=\"verify\")      args = global_parser.parse_args()     url = urlparse(args.url, \"s3\")     if (         url.scheme != \"s3\"         or url.netloc == \"\"         or url.path == \"\"         or url.params != \"\"         or url.query != \"\"         or url.fragment != \"\"     ):         raise ValueError(f\"Invalid url {args.url}\")      args.url = url      match args.command:         case \"create\":             create(args)         case \"update\":             update(args)         case \"verify\":             verify(args) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"icechunk-python/notebooks/conflict/","title":"Conflict","text":"In\u00a0[1]: Copied! <pre>import icechunk\nimport numpy as np\nimport zarr\n</pre> import icechunk import numpy as np import zarr In\u00a0[2]: Copied! <pre>repo = icechunk.Repository.create(icechunk.in_memory_storage())\nsession = repo.writable_session(branch=\"main\")\nroot = zarr.group(session.store)\nroot.attrs[\"foo\"] = \"bar\"\nroot.create_array(\"data\", shape=(10, 10), chunks=(1, 1), dtype=np.int32)\nsession.commit(message=\"Add foo attribute and data array\")\n</pre> repo = icechunk.Repository.create(icechunk.in_memory_storage()) session = repo.writable_session(branch=\"main\") root = zarr.group(session.store) root.attrs[\"foo\"] = \"bar\" root.create_array(\"data\", shape=(10, 10), chunks=(1, 1), dtype=np.int32) session.commit(message=\"Add foo attribute and data array\") Out[2]: <pre>'98VJK6SE2G35EPVCRK3G'</pre> In\u00a0[3]: Copied! <pre>session1 = repo.writable_session(branch=\"main\")\nsession2 = repo.writable_session(branch=\"main\")\n\nroot1 = zarr.group(session1.store)\nroot2 = zarr.group(session2.store)\n\nroot1.attrs[\"foo\"] = \"bar\"\nroot2.attrs[\"foo\"] = \"baz\"\n\nprint(session1.commit(message=\"Update foo attribute on root group\"))\nprint(session2.commit(message=\"Update foo attribute on root group\"))\n</pre> session1 = repo.writable_session(branch=\"main\") session2 = repo.writable_session(branch=\"main\")  root1 = zarr.group(session1.store) root2 = zarr.group(session2.store)  root1.attrs[\"foo\"] = \"bar\" root2.attrs[\"foo\"] = \"baz\"  print(session1.commit(message=\"Update foo attribute on root group\")) print(session2.commit(message=\"Update foo attribute on root group\"))  <pre>7KH81ACN3YQ1SKYN3GE0\n</pre> <pre>\n---------------------------------------------------------------------------\nConflictError                             Traceback (most recent call last)\nCell In[3], line 11\n      8 root2.attrs[\"foo\"] = \"baz\"\n     10 print(session1.commit(message=\"Update foo attribute on root group\"))\n---&gt; 11 print(session2.commit(message=\"Update foo attribute on root group\"))\n\nFile ~/Developer/icechunk/icechunk-python/python/icechunk/session.py:224, in Session.commit(self, message, metadata)\n    222     return self._session.commit(message, metadata)\n    223 except PyConflictError as e:\n--&gt; 224     raise ConflictError(e) from None\n\nConflictError: Failed to commit, expected parent: Some(\"98VJK6SE2G35EPVCRK3G\"), actual parent: Some(\"7KH81ACN3YQ1SKYN3GE0\")</pre> In\u00a0[4]: Copied! <pre>session2.rebase(icechunk.ConflictDetector())\n</pre> session2.rebase(icechunk.ConflictDetector()) <pre>\n---------------------------------------------------------------------------\nRebaseFailedError                         Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 session2.rebase(icechunk.ConflictDetector())\n\nFile ~/Developer/icechunk/icechunk-python/python/icechunk/session.py:247, in Session.rebase(self, solver)\n    245     self._session.rebase(solver)\n    246 except PyRebaseFailedError as e:\n--&gt; 247     raise RebaseFailedError(e) from None\n\nRebaseFailedError: Rebase failed on snapshot 7KH81ACN3YQ1SKYN3GE0: 1 conflicts found</pre> In\u00a0[5]: Copied! <pre>try:\n    session2.rebase(icechunk.ConflictDetector())\nexcept icechunk.RebaseFailedError as e:\n    print(e.conflicts)\n</pre> try:     session2.rebase(icechunk.ConflictDetector()) except icechunk.RebaseFailedError as e:     print(e.conflicts)  <pre>[Conflict(UserAttributesDoubleUpdate, path=/)]\n</pre> In\u00a0[6]: Copied! <pre>session2.rebase(icechunk.BasicConflictSolver(on_user_attributes_conflict=icechunk.VersionSelection.UseOurs))\n</pre> session2.rebase(icechunk.BasicConflictSolver(on_user_attributes_conflict=icechunk.VersionSelection.UseOurs)) In\u00a0[7]: Copied! <pre>session2.commit(message=\"Update foo attribute on root group\")\n</pre> session2.commit(message=\"Update foo attribute on root group\") Out[7]: <pre>'NXZY2WJNS5T15EXAPFBG'</pre> In\u00a0[8]: Copied! <pre>session1 = repo.writable_session(branch=\"main\")\nsession2 = repo.writable_session(branch=\"main\")\n\nroot1 = zarr.group(session1.store)\nroot2 = zarr.group(session2.store)\n\nroot1[\"data\"][0,0] = 1\nroot2[\"data\"][0,:] = 2\n</pre> session1 = repo.writable_session(branch=\"main\") session2 = repo.writable_session(branch=\"main\")  root1 = zarr.group(session1.store) root2 = zarr.group(session2.store)  root1[\"data\"][0,0] = 1 root2[\"data\"][0,:] = 2 In\u00a0[9]: Copied! <pre>print(session2.commit(message=\"Update first row of data array\"))\nprint(session1.commit(message=\"Update first element of data array\"))\n</pre> print(session2.commit(message=\"Update first row of data array\")) print(session1.commit(message=\"Update first element of data array\"))  <pre>AHSAVG18T7JMDE433Z0G\n</pre> <pre>\n---------------------------------------------------------------------------\nConflictError                             Traceback (most recent call last)\nCell In[9], line 2\n      1 print(session2.commit(message=\"Update first row of data array\"))\n----&gt; 2 print(session1.commit(message=\"Update first element of data array\"))\n\nFile ~/Developer/icechunk/icechunk-python/python/icechunk/session.py:224, in Session.commit(self, message, metadata)\n    222     return self._session.commit(message, metadata)\n    223 except PyConflictError as e:\n--&gt; 224     raise ConflictError(e) from None\n\nConflictError: Failed to commit, expected parent: Some(\"NXZY2WJNS5T15EXAPFBG\"), actual parent: Some(\"AHSAVG18T7JMDE433Z0G\")</pre> In\u00a0[10]: Copied! <pre>try:\n    session1.rebase(icechunk.ConflictDetector())\nexcept icechunk.RebaseFailedError as e:\n    for conflict in e.conflicts:\n        print(f\"Conflict at {conflict.path}: {conflict.conflicted_chunks}\")\n</pre> try:     session1.rebase(icechunk.ConflictDetector()) except icechunk.RebaseFailedError as e:     for conflict in e.conflicts:         print(f\"Conflict at {conflict.path}: {conflict.conflicted_chunks}\") <pre>Conflict at /data: [[0, 0]]\n</pre> In\u00a0[11]: Copied! <pre>session1.rebase(icechunk.BasicConflictSolver(on_chunk_conflict=icechunk.VersionSelection.UseOurs))\nsession1.commit(message=\"Update first element of data array\")\n</pre> session1.rebase(icechunk.BasicConflictSolver(on_chunk_conflict=icechunk.VersionSelection.UseOurs)) session1.commit(message=\"Update first element of data array\") Out[11]: <pre>'7JG0TDW9SM3GEA9E9WY0'</pre> In\u00a0[12]: Copied! <pre>session = repo.readonly_session(branch=\"main\")\nroot = zarr.open_group(session.store, mode=\"r\")\nroot[\"data\"][0,:]\n</pre> session = repo.readonly_session(branch=\"main\") root = zarr.open_group(session.store, mode=\"r\") root[\"data\"][0,:] Out[12]: <pre>array([1, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)</pre> In\u00a0[13]: Copied! <pre>session1 = repo.writable_session(branch=\"main\")\nsession2 = repo.writable_session(branch=\"main\")\n\nroot1 = zarr.group(session1.store)\nroot2 = zarr.group(session2.store)\n\nroot1[\"data\"][3,:] = 3\nroot2[\"data\"][4,:] = 4\n\n\nsession1.commit(message=\"Update fourth row of data array\")\n\ntry:\n    session2.rebase(icechunk.ConflictDetector())\n    print(\"Rebase succeeded\")\nexcept icechunk.RebaseFailedError as e:\n    print(e.conflicts)\n\nsession2.commit(message=\"Update fifth row of data array\")\n</pre> session1 = repo.writable_session(branch=\"main\") session2 = repo.writable_session(branch=\"main\")  root1 = zarr.group(session1.store) root2 = zarr.group(session2.store)  root1[\"data\"][3,:] = 3 root2[\"data\"][4,:] = 4   session1.commit(message=\"Update fourth row of data array\")  try:     session2.rebase(icechunk.ConflictDetector())     print(\"Rebase succeeded\") except icechunk.RebaseFailedError as e:     print(e.conflicts)  session2.commit(message=\"Update fifth row of data array\") <pre>Rebase succeeded\n</pre> Out[13]: <pre>'ADZTD57KD9V9W65DW2QG'</pre> In\u00a0[14]: Copied! <pre>session = repo.readonly_session(branch=\"main\")\nroot = zarr.open_group(session.store, mode=\"r\")\nroot[\"data\"][:,:]\n</pre> session = repo.readonly_session(branch=\"main\") root = zarr.open_group(session.store, mode=\"r\") root[\"data\"][:,:] Out[14]: <pre>array([[1, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"icechunk-python/notebooks/demo-azure-blob/","title":"Demo azure blob","text":"In\u00a0[1]: Copied! <pre>import icechunk\n\nrepo = icechunk.Repository.open_or_create(\n    icechunk.azure_storage(\n        container=\"icechunk-demo\",\n        prefix=\"icechunk-demo-dataset\"\n    )\n)\nrepo\n</pre> import icechunk  repo = icechunk.Repository.open_or_create(     icechunk.azure_storage(         container=\"icechunk-demo\",         prefix=\"icechunk-demo-dataset\"     ) ) repo Out[1]: <pre>&lt;icechunk.repository.Repository at 0x236567df3b0&gt;</pre>"},{"location":"icechunk-python/notebooks/demo-dummy-data/","title":"Icechunk with dummy data","text":"In\u00a0[1]: Copied! <pre>import math\n\nimport numpy as np\nimport zarr\nfrom icechunk import Repository, in_memory_storage\n</pre> import math  import numpy as np import zarr from icechunk import Repository, in_memory_storage In\u00a0[2]: Copied! <pre>repo = Repository.create(\n    storage=in_memory_storage()\n)\nrepo\n</pre> repo = Repository.create(     storage=in_memory_storage() ) repo Out[2]: <pre>&lt;icechunk.repository.Repository at 0x118a19fd0&gt;</pre> <p>This dictionary will contain array paths and data that were written to Icechunk, so that we can check correctness.</p> In\u00a0[3]: Copied! <pre>expected = {}\n</pre> expected = {} <p>These two utility functions generate and write dummy array data to a group.</p> In\u00a0[16]: Copied! <pre>def generate_array_chunks(size: int, dtype=np.int32):\n    # dim sizes\n    nz = 64\n    nt = 128\n    nx = ny = int(math.sqrt(size / nz / nt))\n\n    # chunk sizes\n    ct = 2\n    cz = 8\n    cx = max(nx // 3, 1)\n    cy = max(ny // 2, 1)\n    chunks = (cx, cy, cz, ct)\n    shape = (nx, ny, nz, nt)\n\n    array = np.arange(nx * ny * nz * nt, dtype=dtype).reshape(shape)\n\n    return array, chunks\n\n\ndef create_array(*, group, name, size, dtype, fill_value) -&gt; np.ndarray:\n    dims = (\"x\", \"y\", \"z\", \"t\")\n    attrs = {\"description\": \"icechunk test data\"}\n\n    array, chunks = generate_array_chunks(size=size, dtype=dtype)\n\n    arr = group.create_array(\n        name=name,\n        shape=array.shape,\n        dtype=dtype,\n        fill_value=fill_value,\n        chunks=chunks,\n        dimension_names=dims,\n        attributes=attrs,\n        overwrite=True,\n    )\n\n    arr[:] = array\n\n    return array\n</pre> def generate_array_chunks(size: int, dtype=np.int32):     # dim sizes     nz = 64     nt = 128     nx = ny = int(math.sqrt(size / nz / nt))      # chunk sizes     ct = 2     cz = 8     cx = max(nx // 3, 1)     cy = max(ny // 2, 1)     chunks = (cx, cy, cz, ct)     shape = (nx, ny, nz, nt)      array = np.arange(nx * ny * nz * nt, dtype=dtype).reshape(shape)      return array, chunks   def create_array(*, group, name, size, dtype, fill_value) -&gt; np.ndarray:     dims = (\"x\", \"y\", \"z\", \"t\")     attrs = {\"description\": \"icechunk test data\"}      array, chunks = generate_array_chunks(size=size, dtype=dtype)      arr = group.create_array(         name=name,         shape=array.shape,         dtype=dtype,         fill_value=fill_value,         chunks=chunks,         dimension_names=dims,         attributes=attrs,         overwrite=True,     )      arr[:] = array      return array In\u00a0[6]: Copied! <pre>session = repo.writable_session(\"main\")\nstore = session.store\n</pre> session = repo.writable_session(\"main\") store = session.store In\u00a0[7]: Copied! <pre>root_group = zarr.group(store=store, overwrite=True)\nroot_group.attrs[\"foo\"] = \"foo\"\ndict(root_group.attrs)  # check that it was written\n</pre> root_group = zarr.group(store=store, overwrite=True) root_group.attrs[\"foo\"] = \"foo\" dict(root_group.attrs)  # check that it was written Out[7]: <pre>{'foo': 'foo'}</pre> <p>Commit that change</p> In\u00a0[8]: Copied! <pre>first_commit = session.commit(\"wrote a root group attribute\")\nfirst_commit\n</pre> first_commit = session.commit(\"wrote a root group attribute\") first_commit Out[8]: <pre>'ACT3H2K88PB63QXF8EM0'</pre> In\u00a0[10]: Copied! <pre>session = repo.writable_session(\"main\")\nroot_group = zarr.open_group(session.store)\n</pre> session = repo.writable_session(\"main\") root_group = zarr.open_group(session.store) In\u00a0[17]: Copied! <pre>expected[\"root-foo\"] = create_array(\n    group=root_group,\n    name=\"root-foo\",\n    size=1 * 1024 * 256,\n    dtype=np.int32,\n    fill_value=-1,\n)\n</pre> expected[\"root-foo\"] = create_array(     group=root_group,     name=\"root-foo\",     size=1 * 1024 * 256,     dtype=np.int32,     fill_value=-1, ) In\u00a0[18]: Copied! <pre>print(root_group.members())\n</pre> print(root_group.members()) <pre>(('root-foo', &lt;Array &lt;icechunk.store.IcechunkStore object at 0x118ce2ed0&gt;/root-foo shape=(5, 5, 64, 128) dtype=int32&gt;),)\n</pre> In\u00a0[19]: Copied! <pre>dict(root_group[\"root-foo\"].attrs)\n</pre> dict(root_group[\"root-foo\"].attrs) Out[19]: <pre>{'description': 'icechunk test data'}</pre> In\u00a0[20]: Copied! <pre>root_group[\"root-foo\"].attrs[\"update\"] = \"new attr\"\n</pre> root_group[\"root-foo\"].attrs[\"update\"] = \"new attr\" In\u00a0[21]: Copied! <pre>second_commit = session.commit(\"added array, updated attr\")\nsecond_commit\n</pre> second_commit = session.commit(\"added array, updated attr\") second_commit Out[21]: <pre>'HFAB7VA15EC54TXB1HSG'</pre> In\u00a0[22]: Copied! <pre>assert len(root_group[\"root-foo\"].attrs) == 2\nassert len(root_group.members()) == 1\n</pre> assert len(root_group[\"root-foo\"].attrs) == 2 assert len(root_group.members()) == 1 In\u00a0[24]: Copied! <pre>session = repo.readonly_session(snapshot=first_commit)\nroot_group = zarr.open_group(session.store, mode=\"r\")\n\ntry:\n    root_group.attrs[\"update\"] = \"new attr 2\"\n    session.commit(\"new attr 2\")\nexcept ValueError as e:\n    print(e)\nelse:\n    raise ValueError(\"should have failed\")\n</pre> session = repo.readonly_session(snapshot=first_commit) root_group = zarr.open_group(session.store, mode=\"r\")  try:     root_group.attrs[\"update\"] = \"new attr 2\"     session.commit(\"new attr 2\") except ValueError as e:     print(e) else:     raise ValueError(\"should have failed\") <pre>store error: cannot write to read-only store\n</pre> In\u00a0[25]: Copied! <pre>session = repo.writable_session(\"main\")\nroot_group = zarr.open_group(session.store)\nroot_group[\"root-foo\"].attrs[\"update\"] = \"new attr 2\"\nthird_commit = session.commit(\"new attr 2\")\nthird_commit\n</pre> session = repo.writable_session(\"main\") root_group = zarr.open_group(session.store) root_group[\"root-foo\"].attrs[\"update\"] = \"new attr 2\" third_commit = session.commit(\"new attr 2\") third_commit Out[25]: <pre>'CV7SH3YAJSFMNM1BGD3G'</pre> In\u00a0[26]: Copied! <pre>session = repo.writable_session(\"main\")\nroot_group = zarr.open_group(session.store)\n\nroot_group.attrs[\"update\"] = \"new attr 2\"\nfourth_commit = session.commit(\"rewrote array\")\nfourth_commit\n</pre> session = repo.writable_session(\"main\") root_group = zarr.open_group(session.store)  root_group.attrs[\"update\"] = \"new attr 2\" fourth_commit = session.commit(\"rewrote array\") fourth_commit Out[26]: <pre>'RJE8ETHY3GJE8APJC6SG'</pre> In\u00a0[27]: Copied! <pre>{k: v.dtype for k, v in expected.items()}\n</pre> {k: v.dtype for k, v in expected.items()} Out[27]: <pre>{'root-foo': dtype('int32')}</pre> In\u00a0[29]: Copied! <pre>session = repo.writable_session(\"main\")\nstore = session.store\nroot_group = zarr.open_group(store)\n\nnewgroup = zarr.group(store=store, path=\"group1/\")\nexpected[\"group1/foo1\"] = create_array(\n    group=newgroup, name=\"foo1\", dtype=np.float32, size=1 * 1024 * 128, fill_value=-1234\n)\nexpected[\"group1/foo2\"] = create_array(\n    group=newgroup, name=\"foo2\", dtype=np.float16, size=1 * 1024 * 64, fill_value=-1234\n)\nnewgroup = zarr.group(store=store, path=\"group2/\")\nexpected[\"group2/foo3\"] = create_array(\n    group=newgroup, name=\"foo3\", dtype=np.int64, size=1 * 1024 * 32, fill_value=-1234\n)\nfifth_commit = session.commit(\"added groups and arrays\")\nfifth_commit\n</pre> session = repo.writable_session(\"main\") store = session.store root_group = zarr.open_group(store)  newgroup = zarr.group(store=store, path=\"group1/\") expected[\"group1/foo1\"] = create_array(     group=newgroup, name=\"foo1\", dtype=np.float32, size=1 * 1024 * 128, fill_value=-1234 ) expected[\"group1/foo2\"] = create_array(     group=newgroup, name=\"foo2\", dtype=np.float16, size=1 * 1024 * 64, fill_value=-1234 ) newgroup = zarr.group(store=store, path=\"group2/\") expected[\"group2/foo3\"] = create_array(     group=newgroup, name=\"foo3\", dtype=np.int64, size=1 * 1024 * 32, fill_value=-1234 ) fifth_commit = session.commit(\"added groups and arrays\") fifth_commit Out[29]: <pre>'9HE2NMYV2RXR0NBYFZNG'</pre> In\u00a0[30]: Copied! <pre>session = repo.writable_session(\"main\")\nstore = session.store\nroot_group = zarr.open_group(store)\n\nexpected[\"root-foo\"] = create_array(\n    group=root_group,\n    name=\"root-foo\",\n    size=1 * 1024 * 128,\n    dtype=np.int32,\n    fill_value=-1,\n)\n</pre> session = repo.writable_session(\"main\") store = session.store root_group = zarr.open_group(store)  expected[\"root-foo\"] = create_array(     group=root_group,     name=\"root-foo\",     size=1 * 1024 * 128,     dtype=np.int32,     fill_value=-1, ) In\u00a0[31]: Copied! <pre>session.commit(\"overwrote root-foo\")\n</pre> session.commit(\"overwrote root-foo\") Out[31]: <pre>'4HY5BRC305VX36HHVCWG'</pre> In\u00a0[32]: Copied! <pre>root_group.members()\n</pre> root_group.members() Out[32]: <pre>(('group1',\n  &lt;Group &lt;icechunk.store.IcechunkStore object at 0x11e47bfd0&gt;/group1&gt;),\n ('group2',\n  &lt;Group &lt;icechunk.store.IcechunkStore object at 0x11e47bfd0&gt;/group2&gt;),\n ('root-foo',\n  &lt;Array &lt;icechunk.store.IcechunkStore object at 0x11e47bfd0&gt;/root-foo shape=(4, 4, 64, 128) dtype=int32&gt;))</pre> In\u00a0[33]: Copied! <pre>root_group[\"group1\"].members()\n</pre> root_group[\"group1\"].members() Out[33]: <pre>(('foo1',\n  &lt;Array &lt;icechunk.store.IcechunkStore object at 0x11e47bfd0&gt;/group1/foo1 shape=(4, 4, 64, 128) dtype=float32&gt;),\n ('foo2',\n  &lt;Array &lt;icechunk.store.IcechunkStore object at 0x11e47bfd0&gt;/group1/foo2 shape=(2, 2, 64, 128) dtype=float16&gt;))</pre> In\u00a0[34]: Copied! <pre>root_group[\"group2\"].members()\n</pre> root_group[\"group2\"].members() Out[34]: <pre>(('foo3',\n  &lt;Array &lt;icechunk.store.IcechunkStore object at 0x11e47bfd0&gt;/group2/foo3 shape=(2, 2, 64, 128) dtype=int64&gt;),)</pre> In\u00a0[36]: Copied! <pre>session = repo.writable_session(\"main\")\nroot_group = zarr.open_group(session.store)\n\narray = root_group[\"group2/foo3\"]\nprint(array)\n\narray.resize((array.shape[0] * 2, *array.shape[1:]))\nprint(array)\narray[array.shape[0] // 2 :, ...] = expected[\"group2/foo3\"]\nprint(array[2:, 0, 0, 0])\nexpected[\"group2/foo3\"] = np.concatenate([expected[\"group2/foo3\"]] * 2, axis=0)\n\nsession.commit(\"appended to group2/foo3\")\n</pre> session = repo.writable_session(\"main\") root_group = zarr.open_group(session.store)  array = root_group[\"group2/foo3\"] print(array)  array.resize((array.shape[0] * 2, *array.shape[1:])) print(array) array[array.shape[0] // 2 :, ...] = expected[\"group2/foo3\"] print(array[2:, 0, 0, 0]) expected[\"group2/foo3\"] = np.concatenate([expected[\"group2/foo3\"]] * 2, axis=0)  session.commit(\"appended to group2/foo3\") <pre>&lt;Array &lt;icechunk.store.IcechunkStore object at 0x11ed54590&gt;/group2/foo3 shape=(2, 2, 64, 128) dtype=int64&gt;\n&lt;Array &lt;icechunk.store.IcechunkStore object at 0x11ed54590&gt;/group2/foo3 shape=(4, 2, 64, 128) dtype=int64&gt;\n[    0 16384]\n</pre> Out[36]: <pre>'20P32F6NVC6ZPYDX4NEG'</pre>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#icechunk-with-dummy-data","title":"Icechunk with dummy data\u00b6","text":"<p>This demo illustrates how to use Icechunk as a Zarr store</p>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#create-a-new-zarr-store-backed-by-icechunk","title":"Create a new Zarr store backed by Icechunk\u00b6","text":"<p>This example uses an in-memory store.</p>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#a-versioned-transactional-zarr-store","title":"A versioned transactional Zarr store\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#open-the-root-group-write-an-attribute-commit","title":"Open the root group, write an attribute, commit\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#add-a-array-to-the-root-group","title":"Add a array to the root group\u00b6","text":"<p>We save the created array in <code>expected</code> to check that the write was correct (later).</p>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#writing-and-committing-when-not-on-head-will-fail","title":"Writing and Committing when not on <code>HEAD</code> will fail.\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#checkout-head-make-a-change-and-commit","title":"Checkout <code>HEAD</code>, make a change, and commit.\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#create-a-hierarchy","title":"Create a hierarchy\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#overwrite-an-array","title":"Overwrite an array\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#examine-the-hierarchy","title":"Examine the hierarchy\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#append","title":"Append\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-gcs/","title":"Demo gcs","text":"In\u00a0[1]: Copied! <pre>import icechunk\n\nrepo = icechunk.Repository.open_or_create(\n    icechunk.gcs_storage(\n        bucket=\"icechunk-demo\",\n        prefix=\"icechunk-demo-dataset\",\n        service_account_file=\"/Users/matthew.earthmover/Developer/keys/icechunk-service-account.json\"\n    )\n)\nrepo\n</pre> import icechunk  repo = icechunk.Repository.open_or_create(     icechunk.gcs_storage(         bucket=\"icechunk-demo\",         prefix=\"icechunk-demo-dataset\",         service_account_file=\"/Users/matthew.earthmover/Developer/keys/icechunk-service-account.json\"     ) ) repo Out[1]: <pre>&lt;icechunk.repository.Repository at 0x1067a7d90&gt;</pre>"},{"location":"icechunk-python/notebooks/demo-s3/","title":"Xarray/Zarr/Icechunk on S3","text":"In\u00a0[\u00a0]: Copied! <pre>import zarr\nfrom icechunk import Repository, s3_storage\n</pre> import zarr from icechunk import Repository, s3_storage In\u00a0[\u00a0]: Copied! <pre>s3_storage = s3_storage(\n    bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\", from_env=True\n)\n</pre> s3_storage = s3_storage(     bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\", from_env=True ) In\u00a0[\u00a0]: Copied! <pre>repo = Repository.create(\n    storage=s3_storage,\n)\n</pre> repo = Repository.create(     storage=s3_storage, ) In\u00a0[\u00a0]: Copied! <pre>import xarray as xr\n</pre> import xarray as xr In\u00a0[\u00a0]: Copied! <pre>import fsspec\n\nfs = fsspec.filesystem(\"s3\")\n</pre> import fsspec  fs = fsspec.filesystem(\"s3\") In\u00a0[\u00a0]: Copied! <pre>oscar = xr.open_dataset(\n    fs.open(\"s3://earthmover-sample-data/netcdf/oscar_vel2018.nc\"),\n    chunks={},\n    engine=\"h5netcdf\",\n)\noscar\n</pre> oscar = xr.open_dataset(     fs.open(\"s3://earthmover-sample-data/netcdf/oscar_vel2018.nc\"),     chunks={},     engine=\"h5netcdf\", ) oscar In\u00a0[\u00a0]: Copied! <pre>session = repo.session(\"main\")\n\ngroup = zarr.group(store=session.store, overwrite=True)\ngroup\n</pre> session = repo.session(\"main\")  group = zarr.group(store=session.store, overwrite=True) group In\u00a0[\u00a0]: Copied! <pre>import time\n\nfor var in oscar:\n    session = repo.writable_session(\"main\")\n    group = zarr.open_group(store=session.store)\n    print(var)\n    tic = time.time()\n    group.create_array(\n        name=var,\n        shape=oscar[var].shape,\n        chunk_shape=(1, 1, 481, 1201),\n        fill_value=-1234567,\n        dtype=oscar[var].dtype,\n        data=oscar[var],\n        overwrite=True,\n    )\n    print(session.commit(f\"wrote {var}\"))\n    print(f\"committed; {time.time() - tic} seconds\")\n</pre> import time  for var in oscar:     session = repo.writable_session(\"main\")     group = zarr.open_group(store=session.store)     print(var)     tic = time.time()     group.create_array(         name=var,         shape=oscar[var].shape,         chunk_shape=(1, 1, 481, 1201),         fill_value=-1234567,         dtype=oscar[var].dtype,         data=oscar[var],         overwrite=True,     )     print(session.commit(f\"wrote {var}\"))     print(f\"committed; {time.time() - tic} seconds\") In\u00a0[\u00a0]: Copied! <pre>main_snapshot_id = repo.lookup_branch(\"main\")\nrepo.ancestry(main_snapshot_id)\n</pre> main_snapshot_id = repo.lookup_branch(\"main\") repo.ancestry(main_snapshot_id) In\u00a0[\u00a0]: Copied! <pre>import zarr\nfrom icechunk import Repository, s3_storage\n\ns3_storage = s3_storage(\n    bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\", from_env=True\n)\n</pre> import zarr from icechunk import Repository, s3_storage  s3_storage = s3_storage(     bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\", from_env=True ) In\u00a0[\u00a0]: Copied! <pre>repo = Repository.open(\n    storage=s3_storage,\n)\nrepo\n</pre> repo = Repository.open(     storage=s3_storage, ) repo <p>Look at history</p> In\u00a0[\u00a0]: Copied! <pre>[s.message for s in repo.ancestry(branch=\"main\")]\n</pre> [s.message for s in repo.ancestry(branch=\"main\")] In\u00a0[\u00a0]: Copied! <pre>session = repo.readonly_session(branch=\"main\")\nroot_group = zarr.open_group(store=session.store)\n</pre> session = repo.readonly_session(branch=\"main\") root_group = zarr.open_group(store=session.store) In\u00a0[\u00a0]: Copied! <pre>root_group.members()\n</pre> root_group.members() In\u00a0[\u00a0]: Copied! <pre>root_group.members()\n</pre> root_group.members() In\u00a0[\u00a0]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>plt.imshow(root_group[\"u\"][20, 0, :, :], cmap=mpl.cm.RdBu_r, vmin=-0.5, vmax=0.5)\nplt.gcf().set_size_inches((9, 5))\nplt.colorbar(location=\"bottom\", orientation=\"horizontal\", shrink=0.5, aspect=30)\n</pre> plt.imshow(root_group[\"u\"][20, 0, :, :], cmap=mpl.cm.RdBu_r, vmin=-0.5, vmax=0.5) plt.gcf().set_size_inches((9, 5)) plt.colorbar(location=\"bottom\", orientation=\"horizontal\", shrink=0.5, aspect=30)"},{"location":"icechunk-python/notebooks/demo-s3/#xarrayzarricechunk-on-s3","title":"Xarray/Zarr/Icechunk on S3\u00b6","text":"<p>You will need to run this notebook in a <code>conda</code> environment created from <code>environment.yml</code>.</p>"},{"location":"icechunk-python/notebooks/demo-s3/#create-a-new-zarr-store-backed-by-icechunk","title":"Create a new Zarr store backed by Icechunk\u00b6","text":"<p>This example uses a S3 store</p>"},{"location":"icechunk-python/notebooks/demo-s3/#real-data","title":"Real data\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-s3/#write-to-icechunk","title":"Write to icechunk\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-s3/#open-store","title":"Open store\u00b6","text":""},{"location":"icechunk-python/notebooks/memorystore/","title":"Memorystore","text":"In\u00a0[1]: Copied! <pre>import icechunk\nimport zarr\n</pre> import icechunk import zarr <p>Lets create an in-memory icechunk store</p> In\u00a0[2]: Copied! <pre>repo = icechunk.Repository.create(\n    storage=icechunk.in_memory_storage()\n)\nrepo\n</pre> repo = icechunk.Repository.create(     storage=icechunk.in_memory_storage() ) repo Out[2]: <pre>&lt;icechunk.repository.Repository at 0x1077cad50&gt;</pre> <p>Ok! Lets create some data!</p> In\u00a0[4]: Copied! <pre>session = repo.writable_session(\"main\")\ngroup = zarr.group(store=session.store, overwrite=True)\ngroup\n</pre> session = repo.writable_session(\"main\") group = zarr.group(store=session.store, overwrite=True) group Out[4]: <pre>&lt;Group &lt;icechunk.store.IcechunkStore object at 0x1053c55d0&gt;&gt;</pre> In\u00a0[6]: Copied! <pre>air_temp = group.create_array(\n    \"air_temp\", shape=(1000, 1000), chunks=(100, 100), dtype=\"i4\"\n)\nair_temp\n</pre> air_temp = group.create_array(     \"air_temp\", shape=(1000, 1000), chunks=(100, 100), dtype=\"i4\" ) air_temp Out[6]: <pre>&lt;Array &lt;icechunk.store.IcechunkStore object at 0x1053c55d0&gt;/air_temp shape=(1000, 1000) dtype=int32&gt;</pre> In\u00a0[8]: Copied! <pre>async for key in session.store.list():\n    print(key)\n</pre> async for key in session.store.list():     print(key) <pre>zarr.json\nair_temp/zarr.json\n</pre> In\u00a0[9]: Copied! <pre>air_temp[:, :] = 42\n</pre> air_temp[:, :] = 42 In\u00a0[10]: Copied! <pre>air_temp[200, 6]\n</pre> air_temp[200, 6] Out[10]: <pre>array(42, dtype=int32)</pre> <p>Now that we have set the values, lets commit</p> In\u00a0[11]: Copied! <pre>snapshot_id = session.commit(\"Initial commit\")\nsnapshot_id\n</pre> snapshot_id = session.commit(\"Initial commit\") snapshot_id Out[11]: <pre>'VRW71ARWS4QF5R2S7SQ0'</pre> <p>Lets get another session</p> In\u00a0[12]: Copied! <pre>from typing import cast\n\n\nsession = repo.writable_session(\"main\")\ngroup = zarr.open_group(session.store)\nair_temp = cast(zarr.Array, group[\"air_temp\"])\n</pre> from typing import cast   session = repo.writable_session(\"main\") group = zarr.open_group(session.store) air_temp = cast(zarr.Array, group[\"air_temp\"]) <p>Okay now we can change the data</p> In\u00a0[13]: Copied! <pre>air_temp[:, :] = 54\n</pre> air_temp[:, :] = 54 In\u00a0[14]: Copied! <pre>air_temp[200, 6]\n</pre> air_temp[200, 6] Out[14]: <pre>array(54, dtype=int32)</pre> <p>And we can commit again</p> In\u00a0[15]: Copied! <pre>new_snapshot_id = session.commit(\"Change air temp to 54\")\nnew_snapshot_id\n</pre> new_snapshot_id = session.commit(\"Change air temp to 54\") new_snapshot_id Out[15]: <pre>'W8VV15HGFVJ40XTFNCBG'</pre> <p>Cool, now lets checkout the original snapshot and see if the value is 42 again</p> In\u00a0[16]: Copied! <pre>group = zarr.open_group(session.store, mode='r')\nair_temp = cast(zarr.Array, group[\"air_temp\"])\nair_temp[200, 6]\n</pre> group = zarr.open_group(session.store, mode='r') air_temp = cast(zarr.Array, group[\"air_temp\"]) air_temp[200, 6]  Out[16]: <pre>array(54, dtype=int32)</pre>"},{"location":"icechunk-python/notebooks/version-control/","title":"Version Control with Icechunk","text":"In\u00a0[1]: Copied! <pre>import zarr\nfrom icechunk import Repository, in_memory_storage\n</pre> import zarr from icechunk import Repository, in_memory_storage In\u00a0[2]: Copied! <pre>repo = Repository.create(\n    storage=in_memory_storage()\n)\nrepo\n</pre> repo = Repository.create(     storage=in_memory_storage() ) repo Out[2]: <pre>&lt;icechunk.repository.Repository at 0x120a008d0&gt;</pre> <ol> <li>Why not checkout main by default?</li> <li>Why can I create snapshots on the <code>None</code> branch</li> </ol> In\u00a0[3]: Copied! <pre>session = repo.writable_session(\"main\")\nroot_group = zarr.group(store=session.store)\n</pre> session = repo.writable_session(\"main\") root_group = zarr.group(store=session.store) In\u00a0[4]: Copied! <pre>root_group.attrs[\"attr\"] = \"first_attr\"\n</pre> root_group.attrs[\"attr\"] = \"first_attr\" In\u00a0[5]: Copied! <pre>first_commit = session.commit(\"first commit\")\nfirst_commit\n</pre> first_commit = session.commit(\"first commit\") first_commit Out[5]: <pre>'79J80EBBK359F947HMKG'</pre> In\u00a0[6]: Copied! <pre>dict(root_group.attrs)\n</pre> dict(root_group.attrs) Out[6]: <pre>{'attr': 'first_attr'}</pre> In\u00a0[7]: Copied! <pre>session = repo.writable_session(\"main\")\nroot_group = zarr.group(store=session.store)\n\nroot_group.attrs[\"attr\"] = \"second_attr\"\nsecond_commit = session.commit(\"second commit\")\nsecond_commit\n</pre> session = repo.writable_session(\"main\") root_group = zarr.group(store=session.store)  root_group.attrs[\"attr\"] = \"second_attr\" second_commit = session.commit(\"second commit\") second_commit Out[7]: <pre>'ZBV0MEHKYQ2E718M145G'</pre> In\u00a0[8]: Copied! <pre>repo.lookup_branch(\"main\")\n</pre> repo.lookup_branch(\"main\") Out[8]: <pre>'ZBV0MEHKYQ2E718M145G'</pre> <p>Here's where we are:</p> In\u00a0[9]: Copied! <pre>session.snapshot_id, dict(root_group.attrs)\n</pre> session.snapshot_id, dict(root_group.attrs) Out[9]: <pre>('ZBV0MEHKYQ2E718M145G', {'attr': 'second_attr'})</pre> In\u00a0[10]: Copied! <pre>session = repo.readonly_session(snapshot=first_commit)\nroot_group = zarr.open_group(store=session.store, mode=\"r\")\ndict(root_group.attrs)\n</pre> session = repo.readonly_session(snapshot=first_commit) root_group = zarr.open_group(store=session.store, mode=\"r\") dict(root_group.attrs) Out[10]: <pre>{'attr': 'first_attr'}</pre> In\u00a0[11]: Copied! <pre>try:\n    root_group.attrs[\"attr\"] = \"will_fail\"\n    session.commit(\"this should fail\")\nexcept Exception as e:\n    print(e)\n</pre> try:     root_group.attrs[\"attr\"] = \"will_fail\"     session.commit(\"this should fail\") except Exception as e:     print(e) <pre>store error: cannot write to read-only store\n</pre> In\u00a0[12]: Copied! <pre>repo.create_branch(\"feature\", first_commit)\n</pre> repo.create_branch(\"feature\", first_commit) In\u00a0[13]: Copied! <pre>assert repo.lookup_branch(\"feature\") == first_commit\n</pre> assert repo.lookup_branch(\"feature\") == first_commit In\u00a0[14]: Copied! <pre>session = repo.writable_session(\"feature\")\nroot_group = zarr.group(store=session.store)\ndict(root_group.attrs)\n</pre> session = repo.writable_session(\"feature\") root_group = zarr.group(store=session.store) dict(root_group.attrs) Out[14]: <pre>{'attr': 'first_attr'}</pre> In\u00a0[15]: Copied! <pre>root_group.attrs[\"attr\"] = \"new_branch_attr\"\nnew_branch_commit = session.commit(\"commit on new branch\")\nnew_branch_commit\n</pre> root_group.attrs[\"attr\"] = \"new_branch_attr\" new_branch_commit = session.commit(\"commit on new branch\") new_branch_commit Out[15]: <pre>'5KV7D595W3KJVKFBRP3G'</pre> In\u00a0[16]: Copied! <pre>repo.create_tag(\"v1\", snapshot_id=first_commit)\n</pre> repo.create_tag(\"v1\", snapshot_id=first_commit) In\u00a0[17]: Copied! <pre>repo.create_tag(\"v2\", snapshot_id=second_commit)\n</pre> repo.create_tag(\"v2\", snapshot_id=second_commit) In\u00a0[18]: Copied! <pre>session = repo.readonly_session(tag=\"v1\")\nsession.snapshot_id\n</pre> session = repo.readonly_session(tag=\"v1\") session.snapshot_id Out[18]: <pre>'79J80EBBK359F947HMKG'</pre> In\u00a0[19]: Copied! <pre>repo.list_tags()\n</pre> repo.list_tags() Out[19]: <pre>{'v1', 'v2'}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"icechunk-python/notebooks/version-control/#version-control-with-icechunk","title":"Version Control with Icechunk\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#create-a-new-zarr-store-backed-by-icechunk","title":"Create a new Zarr store backed by Icechunk\u00b6","text":"<p>This example uses an in-memory store.</p>"},{"location":"icechunk-python/notebooks/version-control/#snapshotting","title":"Snapshotting\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#concepts","title":"Concepts\u00b6","text":"<ol> <li><code>store.commit</code> creates a snapshot of the data.</li> <li>Every snapshot is associated with a snapshot ID.</li> <li>Use the snapshot ID to time-travel within your data's history.</li> </ol>"},{"location":"icechunk-python/notebooks/version-control/#create-a-snapshot","title":"Create a snapshot\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#view-the-current-snapshot-id","title":"View the current snapshot ID\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#time-travel-to-a-snapshot","title":"Time-travel to a snapshot\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#snapshotting-is-only-allowed-at-the-tip-of-a-branch","title":"Snapshotting is only allowed at the tip of a branch\u00b6","text":"<p>TODO: need better error message</p>"},{"location":"icechunk-python/notebooks/version-control/#branching","title":"Branching\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#create-a-new-branch","title":"Create a new branch\u00b6","text":"<p>We will create a new branch starting at <code>first_commit</code></p>"},{"location":"icechunk-python/notebooks/version-control/#tagging","title":"Tagging\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#creating-a-new-tag","title":"Creating a new tag\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#time-travel-to-a-tag","title":"Time-travel to a tag\u00b6","text":"<p>Pass the <code>tag</code> argument to <code>checkout</code></p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-Icechunk/","title":"Icechunk Performance - Icechunk","text":"In\u00a0[1]: Copied! <pre>import xarray as xr\nimport zarr\nimport dask\nimport fsspec\nfrom dask.diagnostics import ProgressBar\n\nimport icechunk\nfrom icechunk import Repository, s3_storage\n\nprint('xarray:  ', xr.__version__)\nprint('dask:    ', dask.__version__)\nprint('zarr:    ', zarr.__version__)\nprint('icechunk:', icechunk.__version__)\n</pre> import xarray as xr import zarr import dask import fsspec from dask.diagnostics import ProgressBar  import icechunk from icechunk import Repository, s3_storage  print('xarray:  ', xr.__version__) print('dask:    ', dask.__version__) print('zarr:    ', zarr.__version__) print('icechunk:', icechunk.__version__) <pre>xarray:   2025.1.1.dev4+g1e5045a8\ndask:     2024.12.1\nzarr:     3.0.0\nicechunk: 0.1.0-alpha.15\n</pre> In\u00a0[2]: Copied! <pre>zarr.config.set(\n    {\n        'threading.max_workers': 16,\n        'async.concurrency': 128\n    }\n)\n</pre> zarr.config.set(     {         'threading.max_workers': 16,         'async.concurrency': 128     } ) Out[2]: <pre>&lt;donfig.config_obj.ConfigSet at 0x10f4b7090&gt;</pre> In\u00a0[3]: Copied! <pre>url = \"https://nsf-ncar-era5.s3.amazonaws.com/e5.oper.an.pl/194106/e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc\"\n%time \nds = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1})\nds = ds.drop_encoding()\n</pre> url = \"https://nsf-ncar-era5.s3.amazonaws.com/e5.oper.an.pl/194106/e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc\" %time  ds = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1}) ds = ds.drop_encoding() <pre>CPU times: user 1 \u03bcs, sys: 0 ns, total: 1 \u03bcs\nWall time: 3.81 \u03bcs\n</pre> <pre>/var/folders/42/2n_m99nx043g08tcp0krm4fw0000gn/T/ipykernel_52365/4188050799.py:3: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 1. This could degrade performance. Instead, consider rechunking after loading.\n  ds = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1})\n</pre> In\u00a0[4]: Copied! <pre>print(ds)\n</pre> print(ds) <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:    (time: 24, level: 37, latitude: 721, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n  * level      (level) float64 296B 1.0 2.0 3.0 5.0 ... 925.0 950.0 975.0 1e+03\n  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n  * time       (time) datetime64[ns] 192B 1941-06-01 ... 1941-06-01T23:00:00\nData variables:\n    PV         (time, level, latitude, longitude) float32 4GB dask.array&lt;chunksize=(1, 37, 721, 1440), meta=np.ndarray&gt;\n    utc_date   (time) int32 96B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\nAttributes:\n    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB 1 data to netC...\n    NETCDF_VERSION:       4.8.1\n    CONVERSION_PLATFORM:  Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 1...\n    CONVERSION_DATE:      Wed May 10 06:33:49 MDT 2023\n    Conventions:          CF-1.6\n    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n    history:              Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e...\n    NCO:                  netCDF Operators version 5.0.3 (Homepage = http://n...\n</pre> In\u00a0[4]: Copied! <pre>with ProgressBar():\n    dsl = ds.load()\n</pre> with ProgressBar():     dsl = ds.load() <pre>[########################################] | 100% Completed | 129.03 s\n</pre> In\u00a0[6]: Copied! <pre>prefix = \"ryan/icechunk-tests-era5-999\"\nstore = Repository.create(\n    storage=s3_storage(\n        bucket=\"icechunk-test\",\n        prefix=prefix, \n        from_env=True,\n    ),\n    mode=\"w\"\n)\nstore\n</pre> prefix = \"ryan/icechunk-tests-era5-999\" store = Repository.create(     storage=s3_storage(         bucket=\"icechunk-test\",         prefix=prefix,          from_env=True,     ),     mode=\"w\" ) store Out[6]: <pre>&lt;icechunk.IcechunkStore at 0x7f9eb84402c0&gt;</pre> In\u00a0[7]: Copied! <pre>store.branch, store.snapshot_id\n</pre> store.branch, store.snapshot_id Out[7]: <pre>('main', 'B8ZZN2YZS6NQKM17X68G')</pre> In\u00a0[8]: Copied! <pre>encoding = {\n    \"PV\": {\n        \"codecs\": [zarr.codecs.BytesCodec(), zarr.codecs.ZstdCodec()],\n        \"chunks\": (1, 1, 721, 1440)\n    }\n}\n</pre> encoding = {     \"PV\": {         \"codecs\": [zarr.codecs.BytesCodec(), zarr.codecs.ZstdCodec()],         \"chunks\": (1, 1, 721, 1440)     } } <p>Note that Dask is not required to obtain good performance when reading and writing. Zarr and Icechunk use multithreading and asyncio internally.</p> In\u00a0[9]: Copied! <pre>%time dsl.to_zarr(store, zarr_format=3, consolidated=False, encoding=encoding)\n</pre> %time dsl.to_zarr(store, zarr_format=3, consolidated=False, encoding=encoding) <pre>CPU times: user 54 s, sys: 1.56 s, total: 55.5 s\nWall time: 18.9 s\n</pre> Out[9]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7f9ea8781ec0&gt;</pre> In\u00a0[43]: Copied! <pre># with ProgressBar():\n#     (dsl\n#      .chunk({\"time\": 1, \"level\": 10})\n#      .to_zarr(store, zarr_format=3, consolidated=False, encoding=encoding)\n#     )\n</pre> # with ProgressBar(): #     (dsl #      .chunk({\"time\": 1, \"level\": 10}) #      .to_zarr(store, zarr_format=3, consolidated=False, encoding=encoding) #     ) <pre>[########################################] | 100% Completed | 18.02 ss\n</pre> In\u00a0[10]: Copied! <pre>store.commit(\"wrote data\")\n</pre> store.commit(\"wrote data\") Out[10]: <pre>'AS64P9SQ7NY1P22P8GS0'</pre> In\u00a0[11]: Copied! <pre>store = Repository.open(\n    storage=s3_storage(\n        bucket=\"icechunk-test\",\n        prefix=prefix,\n        from_env=True,\n    ),\n    mode=\"r\"\n)\n</pre> store = Repository.open(     storage=s3_storage(         bucket=\"icechunk-test\",         prefix=prefix,         from_env=True,     ),     mode=\"r\" ) In\u00a0[12]: Copied! <pre>%time dsic = xr.open_dataset(store, consolidated=False, engine=\"zarr\")\n</pre> %time dsic = xr.open_dataset(store, consolidated=False, engine=\"zarr\") <pre>CPU times: user 16.8 ms, sys: 2.45 ms, total: 19.2 ms\nWall time: 97.4 ms\n</pre> In\u00a0[13]: Copied! <pre>print(dsic)\n</pre> print(dsic) <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:    (level: 37, latitude: 721, longitude: 1440, time: 24)\nCoordinates:\n  * level      (level) float64 296B 1.0 2.0 3.0 5.0 ... 925.0 950.0 975.0 1e+03\n  * latitude   (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n  * time       (time) datetime64[ns] 192B 1941-06-01 ... 1941-06-01T23:00:00\nData variables:\n    PV         (time, level, latitude, longitude) float32 4GB ...\n    utc_date   (time) int32 96B ...\nAttributes:\n    CONVERSION_DATE:      Wed May 10 06:33:49 MDT 2023\n    CONVERSION_PLATFORM:  Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 1...\n    Conventions:          CF-1.6\n    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n    NCO:                  netCDF Operators version 5.0.3 (Homepage = http://n...\n    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB 1 data to netC...\n    NETCDF_VERSION:       4.8.1\n    history:              Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e...\n</pre> In\u00a0[14]: Copied! <pre>%time dsic.PV[0, 0, 0, 0].values\n</pre> %time dsic.PV[0, 0, 0, 0].values <pre>CPU times: user 16.8 ms, sys: 78 \u03bcs, total: 16.8 ms\nWall time: 102 ms\n</pre> Out[14]: <pre>array(0.00710905, dtype=float32)</pre> <p>As with writing, Dask is not required for performant reading of the data. In this example we can load the entire dataset (nearly 4GB) in 8s.</p> In\u00a0[15]: Copied! <pre>%time _ = dsic.compute()\n</pre> %time _ = dsic.compute() <pre>CPU times: user 11 s, sys: 3.67 s, total: 14.7 s\nWall time: 2.03 s\n</pre> In\u00a0[16]: Copied! <pre>xr.testing.assert_identical(_, ds)\n</pre> xr.testing.assert_identical(_, ds) In\u00a0[17]: Copied! <pre>dsicc = dsic.chunk({\"time\": 1, \"level\": 10})\n</pre> dsicc = dsic.chunk({\"time\": 1, \"level\": 10}) In\u00a0[19]: Copied! <pre>from dask.diagnostics import ProgressBar\nwith ProgressBar():\n    _ = dsicc.compute()\n</pre> from dask.diagnostics import ProgressBar with ProgressBar():     _ = dsicc.compute() <pre>[########################################] | 100% Completed | 2.13 sms\n</pre> In\u00a0[45]: Copied! <pre>actual = _\nactual\n</pre> actual = _ actual Out[45]: <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:    (latitude: 721, level: 37, time: 24, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n  * level      (level) float64 296B 1.0 2.0 3.0 5.0 ... 925.0 950.0 975.0 1e+03\n  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n  * time       (time) datetime64[ns] 192B 1941-06-01 ... 1941-06-01T23:00:00\nData variables:\n    utc_date   (time) int32 96B 1941060100 1941060101 ... 1941060122 1941060123\n    PV         (time, level, latitude, longitude) float32 4GB 0.007109 ... -1...\nAttributes:\n    CONVERSION_DATE:      Wed May 10 06:33:49 MDT 2023\n    CONVERSION_PLATFORM:  Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 1...\n    Conventions:          CF-1.6\n    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n    NCO:                  netCDF Operators version 5.0.3 (Homepage = http://n...\n    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB 1 data to netC...\n    NETCDF_VERSION:       4.8.1\n    history:              Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>latitude: 721</li><li>level: 37</li><li>time: 24</li><li>longitude: 1440</li></ul></li><li>Coordinates: (4)<ul><li>latitude(latitude)float6490.0 89.75 89.5 ... -89.75 -90.0long_name :latitudeshort_name :latunits :degrees_north<pre>array([ 90.  ,  89.75,  89.5 , ..., -89.5 , -89.75, -90.  ])</pre></li><li>level(level)float641.0 2.0 3.0 ... 950.0 975.0 1e+03alternate_units :millibarlong_name :pressure levelshort_name :plevunits :hPa<pre>array([   1.,    2.,    3.,    5.,    7.,   10.,   20.,   30.,   50.,   70.,\n        100.,  125.,  150.,  175.,  200.,  225.,  250.,  300.,  350.,  400.,\n        450.,  500.,  550.,  600.,  650.,  700.,  750.,  775.,  800.,  825.,\n        850.,  875.,  900.,  925.,  950.,  975., 1000.])</pre></li><li>longitude(longitude)float640.0 0.25 0.5 ... 359.2 359.5 359.8long_name :longitudeshort_name :lonunits :degrees_east<pre>array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n       3.5975e+02])</pre></li><li>time(time)datetime64[ns]1941-06-01 ... 1941-06-01T23:00:00long_name :time<pre>array(['1941-06-01T00:00:00.000000000', '1941-06-01T01:00:00.000000000',\n       '1941-06-01T02:00:00.000000000', '1941-06-01T03:00:00.000000000',\n       '1941-06-01T04:00:00.000000000', '1941-06-01T05:00:00.000000000',\n       '1941-06-01T06:00:00.000000000', '1941-06-01T07:00:00.000000000',\n       '1941-06-01T08:00:00.000000000', '1941-06-01T09:00:00.000000000',\n       '1941-06-01T10:00:00.000000000', '1941-06-01T11:00:00.000000000',\n       '1941-06-01T12:00:00.000000000', '1941-06-01T13:00:00.000000000',\n       '1941-06-01T14:00:00.000000000', '1941-06-01T15:00:00.000000000',\n       '1941-06-01T16:00:00.000000000', '1941-06-01T17:00:00.000000000',\n       '1941-06-01T18:00:00.000000000', '1941-06-01T19:00:00.000000000',\n       '1941-06-01T20:00:00.000000000', '1941-06-01T21:00:00.000000000',\n       '1941-06-01T22:00:00.000000000', '1941-06-01T23:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (2)<ul><li>utc_date(time)int321941060100 ... 1941060123long_name :UTC date yyyy-mm-dd hh:00:00 as yyyymmddhhunits :Gregorian_year month day hour<pre>array([1941060100, 1941060101, 1941060102, 1941060103, 1941060104,\n       1941060105, 1941060106, 1941060107, 1941060108, 1941060109,\n       1941060110, 1941060111, 1941060112, 1941060113, 1941060114,\n       1941060115, 1941060116, 1941060117, 1941060118, 1941060119,\n       1941060120, 1941060121, 1941060122, 1941060123], dtype=int32)</pre></li><li>PV(time, level, latitude, longitude)float320.007109 0.007109 ... -1.53e-05QuantizeGranularBitGroomNumberOfSignificantDigits :7ecmwf_local_table :128ecmwf_parameter :60grid_specification :0.25 degree x 0.25 degree from 90N to 90S and 0E to 359.75E (721 x 1440 Latitude/Longitude)long_name :Potential vorticitymaximum_value :0.02379735186696053minimum_value :-0.03200835734605789original_format :WMO GRIB 1 with ECMWF local tablerda_dataset :ds633.0rda_dataset_doi :DOI: 10.5065/BH6N-5N20rda_dataset_group :ERA5 atmospheric pressure level analysis [netCDF4]rda_dataset_url :https:/rda.ucar.edu/datasets/ds633.0/short_name :pvunits :K m**2 kg**-1 s**-1<pre>array([[[[ 7.10904598e-03,  7.10904598e-03,  7.10904598e-03, ...,\n           7.10904598e-03,  7.10904598e-03,  7.10904598e-03],\n         [ 7.19106197e-03,  7.19010830e-03,  7.18915462e-03, ...,\n           7.19487667e-03,  7.19392300e-03,  7.19201565e-03],\n         [ 7.24160671e-03,  7.23969936e-03,  7.23779202e-03, ...,\n           7.24732876e-03,  7.24542141e-03,  7.24351406e-03],\n         ...,\n         [-1.38288736e-02, -1.38307810e-02, -1.38336420e-02, ...,\n          -1.38231516e-02, -1.38250589e-02, -1.38269663e-02],\n         [-1.36028528e-02, -1.36038065e-02, -1.36047602e-02, ...,\n          -1.35999918e-02, -1.36009455e-02, -1.36018991e-02],\n         [-1.36286020e-02, -1.36286020e-02, -1.36286020e-02, ...,\n          -1.36286020e-02, -1.36286020e-02, -1.36286020e-02]],\n\n        [[ 4.26043943e-03,  4.26043943e-03,  4.26043943e-03, ...,\n           4.26043943e-03,  4.26043943e-03,  4.26043943e-03],\n         [ 4.38370183e-03,  4.38346341e-03,  4.38322499e-03, ...,\n           4.38465551e-03,  4.38441709e-03,  4.38394025e-03],\n         [ 4.43472341e-03,  4.43424657e-03,  4.43400815e-03, ...,\n           4.43591550e-03,  4.43543866e-03,  4.43496183e-03],\n...\n          -9.63546336e-06, -9.58330929e-06, -9.51625407e-06],\n         [-2.10795552e-05, -2.10572034e-05, -2.10348517e-05, ...,\n          -2.11540610e-05, -2.11317092e-05, -2.11019069e-05],\n         [-1.52979046e-05, -1.52979046e-05, -1.52979046e-05, ...,\n          -1.52979046e-05, -1.52979046e-05, -1.52979046e-05]],\n\n        [[-9.31322575e-09, -9.31322575e-09, -9.31322575e-09, ...,\n          -9.31322575e-09, -9.31322575e-09, -9.31322575e-09],\n         [ 2.04890966e-08,  2.04890966e-08,  2.04890966e-08, ...,\n           2.04890966e-08,  2.04890966e-08,  2.04890966e-08],\n         [-1.67638063e-08, -1.67638063e-08, -1.67638063e-08, ...,\n          -1.67638063e-08, -1.67638063e-08, -1.67638063e-08],\n         ...,\n         [-9.44919884e-06, -9.41194594e-06, -9.37469304e-06, ...,\n          -9.63546336e-06, -9.58330929e-06, -9.51625407e-06],\n         [-2.10795552e-05, -2.10572034e-05, -2.10348517e-05, ...,\n          -2.11540610e-05, -2.11317092e-05, -2.11019069e-05],\n         [-1.52979046e-05, -1.52979046e-05, -1.52979046e-05, ...,\n          -1.52979046e-05, -1.52979046e-05, -1.52979046e-05]]]],\n      dtype=float32)</pre></li></ul></li><li>Indexes: (4)<ul><li>latitudePandasIndex<pre>PandasIndex(Index([  90.0,  89.75,   89.5,  89.25,   89.0,  88.75,   88.5,  88.25,   88.0,\n        87.75,\n       ...\n       -87.75,  -88.0, -88.25,  -88.5, -88.75,  -89.0, -89.25,  -89.5, -89.75,\n        -90.0],\n      dtype='float64', name='latitude', length=721))</pre></li><li>levelPandasIndex<pre>PandasIndex(Index([   1.0,    2.0,    3.0,    5.0,    7.0,   10.0,   20.0,   30.0,   50.0,\n         70.0,  100.0,  125.0,  150.0,  175.0,  200.0,  225.0,  250.0,  300.0,\n        350.0,  400.0,  450.0,  500.0,  550.0,  600.0,  650.0,  700.0,  750.0,\n        775.0,  800.0,  825.0,  850.0,  875.0,  900.0,  925.0,  950.0,  975.0,\n       1000.0],\n      dtype='float64', name='level'))</pre></li><li>longitudePandasIndex<pre>PandasIndex(Index([   0.0,   0.25,    0.5,   0.75,    1.0,   1.25,    1.5,   1.75,    2.0,\n         2.25,\n       ...\n        357.5, 357.75,  358.0, 358.25,  358.5, 358.75,  359.0, 359.25,  359.5,\n       359.75],\n      dtype='float64', name='longitude', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1941-06-01 00:00:00', '1941-06-01 01:00:00',\n               '1941-06-01 02:00:00', '1941-06-01 03:00:00',\n               '1941-06-01 04:00:00', '1941-06-01 05:00:00',\n               '1941-06-01 06:00:00', '1941-06-01 07:00:00',\n               '1941-06-01 08:00:00', '1941-06-01 09:00:00',\n               '1941-06-01 10:00:00', '1941-06-01 11:00:00',\n               '1941-06-01 12:00:00', '1941-06-01 13:00:00',\n               '1941-06-01 14:00:00', '1941-06-01 15:00:00',\n               '1941-06-01 16:00:00', '1941-06-01 17:00:00',\n               '1941-06-01 18:00:00', '1941-06-01 19:00:00',\n               '1941-06-01 20:00:00', '1941-06-01 21:00:00',\n               '1941-06-01 22:00:00', '1941-06-01 23:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (9)CONVERSION_DATE :Wed May 10 06:33:49 MDT 2023CONVERSION_PLATFORM :Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 17 08:14:12 UTC 2020 (c6bab98) x86_64 x86_64 x86_64 GNU/LinuxConventions :CF-1.6DATA_SOURCE :ECMWF: https://cds.climate.copernicus.eu, Copernicus Climate Data StoreNCO :netCDF Operators version 5.0.3 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)NETCDF_COMPRESSION :NCO: Precision-preserving compression to netCDF4/HDF5 (see \"history\" and \"NCO\" global attributes below for specifics).NETCDF_CONVERSION :CISL RDA: Conversion from ECMWF GRIB 1 data to netCDF4.NETCDF_VERSION :4.8.1history :Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.unc.nc e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc</li></ul> In\u00a0[46]: Copied! <pre>xr.testing.assert_identical(actual, dsl)\n</pre> xr.testing.assert_identical(actual, dsl) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-Icechunk/#icechunk-performance-icechunk","title":"Icechunk Performance - Icechunk\u00b6","text":"<p>Using data from the NCAR ERA5 AWS Public Dataset.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-Icechunk/#load-data-from-hdf5-file","title":"Load Data from HDF5 File\u00b6","text":"<p>This illustrates how loading directly from HDF5 files on S3 can be slow, even with Dask.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-Icechunk/#initialize-icechunk-repo","title":"Initialize Icechunk Repo\u00b6","text":""},{"location":"icechunk-python/notebooks/performance/era5_xarray-Icechunk/#store-data-to-icechunk","title":"Store Data To Icechunk\u00b6","text":"<p>We specify encoding to set both compression and chunk size.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-Icechunk/#read-data-back","title":"Read Data Back\u00b6","text":""},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr2/","title":"Icechunk Performance - Zarr V2","text":"In\u00a0[1]: Copied! <pre>import xarray as xr\nimport zarr\nimport dask\nimport fsspec\nfrom dask.diagnostics import ProgressBar\n\nprint('xarray:  ', xr.__version__)\nprint('dask:    ', dask.__version__)\nprint('zarr:    ', zarr.__version__)\n</pre> import xarray as xr import zarr import dask import fsspec from dask.diagnostics import ProgressBar  print('xarray:  ', xr.__version__) print('dask:    ', dask.__version__) print('zarr:    ', zarr.__version__) <pre>xarray:   2024.7.0\ndask:     2024.6.2\nzarr:     2.18.2\n</pre> In\u00a0[6]: Copied! <pre>url = \"https://nsf-ncar-era5.s3.amazonaws.com/e5.oper.an.pl/194106/e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc\"\n%time dsc = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1}).drop_encoding()\n</pre> url = \"https://nsf-ncar-era5.s3.amazonaws.com/e5.oper.an.pl/194106/e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc\" %time dsc = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1}).drop_encoding() <pre>CPU times: user 123 ms, sys: 44.5 ms, total: 168 ms\nWall time: 1.91 s\n</pre> <pre>/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:277: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 1. This could degrade performance. Instead, consider rechunking after loading.\n  warnings.warn(\n</pre> In\u00a0[7]: Copied! <pre>print(ds)\n</pre> print(ds) <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:    (time: 24, level: 37, latitude: 721, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n  * level      (level) float64 296B 1.0 2.0 3.0 5.0 ... 925.0 950.0 975.0 1e+03\n  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n  * time       (time) datetime64[ns] 192B 1941-06-01 ... 1941-06-01T23:00:00\nData variables:\n    PV         (time, level, latitude, longitude) float32 4GB dask.array&lt;chunksize=(1, 37, 721, 1440), meta=np.ndarray&gt;\n    utc_date   (time) int32 96B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\nAttributes:\n    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB 1 data to netC...\n    NETCDF_VERSION:       4.8.1\n    CONVERSION_PLATFORM:  Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 1...\n    CONVERSION_DATE:      Wed May 10 06:33:49 MDT 2023\n    Conventions:          CF-1.6\n    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n    history:              Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e...\n    NCO:                  netCDF Operators version 5.0.3 (Homepage = http://n...\n</pre> In\u00a0[8]: Copied! <pre>with ProgressBar():\n    dsl = ds.load()\n</pre> with ProgressBar():     dsl = ds.load() <pre>[########################################] | 100% Completed | 61.19 ss\n</pre> In\u00a0[9]: Copied! <pre>encoding = {\n    \"PV\": {\n        \"compressor\": zarr.Zstd(),\n        \"chunks\": (1, 1, 721, 1440)\n    }\n}\n</pre> encoding = {     \"PV\": {         \"compressor\": zarr.Zstd(),         \"chunks\": (1, 1, 721, 1440)     } } In\u00a0[17]: Copied! <pre>target_url = \"s3://icechunk-test/ryan/zarr-v2/test-era5-11\"\nstore = zarr.storage.FSStore(target_url)\n</pre> target_url = \"s3://icechunk-test/ryan/zarr-v2/test-era5-11\" store = zarr.storage.FSStore(target_url) In\u00a0[18]: Copied! <pre>%time dsl.to_zarr(store, consolidated=False, encoding=encoding, mode=\"w\")\n</pre> %time dsl.to_zarr(store, consolidated=False, encoding=encoding, mode=\"w\") <pre>CPU times: user 21.4 s, sys: 3.73 s, total: 25.1 s\nWall time: 31.8 s\n</pre> Out[18]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7efac8869fc0&gt;</pre> In\u00a0[22]: Copied! <pre># with dask\ndslc = dsl.chunk({\"time\": 1, \"level\": 1})\nstore_d = zarr.storage.FSStore(target_url + '-dask')\nwith ProgressBar():\n    dslc.to_zarr(store_d, consolidated=False, encoding=encoding, mode=\"w\")\n</pre> # with dask dslc = dsl.chunk({\"time\": 1, \"level\": 1}) store_d = zarr.storage.FSStore(target_url + '-dask') with ProgressBar():     dslc.to_zarr(store_d, consolidated=False, encoding=encoding, mode=\"w\") <pre>[########################################] | 100% Completed | 12.30 s\n</pre> In\u00a0[12]: Copied! <pre>%time dss = xr.open_dataset(store, consolidated=False, engine=\"zarr\")\n</pre> %time dss = xr.open_dataset(store, consolidated=False, engine=\"zarr\") <pre>CPU times: user 50.4 ms, sys: 7.21 ms, total: 57.6 ms\nWall time: 487 ms\n</pre> In\u00a0[13]: Copied! <pre>%time dss.PV[0, 0, 0, 0].values\n</pre> %time dss.PV[0, 0, 0, 0].values <pre>CPU times: user 15.2 ms, sys: 671 \u03bcs, total: 15.9 ms\nWall time: 97.4 ms\n</pre> Out[13]: <pre>array(0.00710905, dtype=float32)</pre> In\u00a0[23]: Copied! <pre>%time _ = dss.compute()\n</pre> %time _ = dss.compute() <pre>CPU times: user 8.6 s, sys: 1.53 s, total: 10.1 s\nWall time: 22.6 s\n</pre> In\u00a0[15]: Copied! <pre>dssd = xr.open_dataset(store, consolidated=False, engine=\"zarr\").chunk({\"time\": 1, \"level\": 10})\n</pre> dssd = xr.open_dataset(store, consolidated=False, engine=\"zarr\").chunk({\"time\": 1, \"level\": 10}) In\u00a0[16]: Copied! <pre>with ProgressBar():\n    _ = dssd.compute()\n</pre> with ProgressBar():     _ = dssd.compute() <pre>[########################################] | 100% Completed | 4.55 sms\n</pre> In\u00a0[22]: Copied! <pre>1893510506 / 2 / 1e6\n</pre> 1893510506 / 2 / 1e6 Out[22]: <pre>946.755253</pre> In\u00a0[20]: Copied! <pre>group = zarr.open_group(store, mode=\"r\")\ngroup.info\n</pre> group = zarr.open_group(store, mode=\"r\") group.info Out[20]: Name/Typezarr.hierarchy.GroupRead-onlyTrueStore typezarr.storage.FSStoreNo. members6No. arrays6No. groups0ArraysPV, latitude, level, longitude, time, utc_date In\u00a0[21]: Copied! <pre>group.PV.info\n</pre> group.PV.info Out[21]: Name/PVTypezarr.core.ArrayData typefloat32Shape(24, 37, 721, 1440)Chunk shape(1, 1, 721, 1440)OrderCRead-onlyTrueCompressorZstd(level=1)Store typezarr.storage.FSStoreNo. bytes3687828480 (3.4G)No. bytes stored1893510506 (1.8G)Storage ratio1.9Chunks initialized888/888 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr2/#icechunk-performance-zarr-v2","title":"Icechunk Performance - Zarr V2\u00b6","text":"<p>Using data from the NCAR ERA5 AWS Public Dataset.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr2/#load-data-from-hdf5-file","title":"Load Data from HDF5 File\u00b6","text":"<p>This illustrates how loading directly from HDF5 files on S3 can be slow, even with Dask.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr2/#write-zarr-store-no-dask","title":"Write Zarr Store - No Dask\u00b6","text":""},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr2/#read-data-back","title":"Read Data Back\u00b6","text":""},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr3/","title":"Icechunk Performance - Zarr V3","text":"In\u00a0[1]: Copied! <pre>import xarray as xr\nimport zarr\nimport dask\nimport fsspec\nfrom dask.diagnostics import ProgressBar\n\nprint('xarray:  ', xr.__version__)\nprint('dask:    ', dask.__version__)\nprint('zarr:    ', zarr.__version__)\n</pre> import xarray as xr import zarr import dask import fsspec from dask.diagnostics import ProgressBar  print('xarray:  ', xr.__version__) print('dask:    ', dask.__version__) print('zarr:    ', zarr.__version__) <pre>xarray:   0.9.7.dev3734+g26081d4f\ndask:     2024.9.1+8.g70f56e28\nzarr:     3.0.0b1.dev8+g9bbfd88\n</pre> In\u00a0[2]: Copied! <pre>zarr.config.set(\n    {\n        'threading.max_workers': 16,\n        'async.concurrency': 128\n    }\n)\n</pre> zarr.config.set(     {         'threading.max_workers': 16,         'async.concurrency': 128     } ) Out[2]: <pre>&lt;donfig.config_obj.ConfigSet at 0x7f5a7a52bf50&gt;</pre> In\u00a0[3]: Copied! <pre>url = \"https://nsf-ncar-era5.s3.amazonaws.com/e5.oper.an.pl/194106/e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc\"\n%time ds = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1})\nds = ds.drop_encoding()\n</pre> url = \"https://nsf-ncar-era5.s3.amazonaws.com/e5.oper.an.pl/194106/e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.nc\" %time ds = xr.open_dataset(fsspec.open(url).open(), engine=\"h5netcdf\", chunks={\"time\": 1}) ds = ds.drop_encoding() <pre>CPU times: user 277 ms, sys: 37.5 ms, total: 315 ms\nWall time: 2.33 s\n</pre> <pre>/srv/conda/envs/icechunk/lib/python3.12/site-packages/xarray/backends/api.py:357: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 1. This could degrade performance. Instead, consider rechunking after loading.\n  var_chunks = _get_chunk(var, chunks, chunkmanager)\n</pre> In\u00a0[4]: Copied! <pre>print(ds)\n</pre> print(ds) <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:    (time: 24, level: 37, latitude: 721, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n  * level      (level) float64 296B 1.0 2.0 3.0 5.0 ... 925.0 950.0 975.0 1e+03\n  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n  * time       (time) datetime64[ns] 192B 1941-06-01 ... 1941-06-01T23:00:00\nData variables:\n    PV         (time, level, latitude, longitude) float32 4GB dask.array&lt;chunksize=(1, 37, 721, 1440), meta=np.ndarray&gt;\n    utc_date   (time) int32 96B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\nAttributes:\n    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB 1 data to netC...\n    NETCDF_VERSION:       4.8.1\n    CONVERSION_PLATFORM:  Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 1...\n    CONVERSION_DATE:      Wed May 10 06:33:49 MDT 2023\n    Conventions:          CF-1.6\n    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n    history:              Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e...\n    NCO:                  netCDF Operators version 5.0.3 (Homepage = http://n...\n</pre> In\u00a0[5]: Copied! <pre>with ProgressBar():\n    dsl = ds.load()\n</pre> with ProgressBar():     dsl = ds.load() <pre>[########################################] | 100% Completed | 62.20 ss\n</pre> In\u00a0[6]: Copied! <pre>encoding = {\n    \"PV\": {\n        \"codecs\": [zarr.codecs.BytesCodec(), zarr.codecs.ZstdCodec()],\n        \"chunks\": (1, 1, 721, 1440)\n    }\n}\n</pre> encoding = {     \"PV\": {         \"codecs\": [zarr.codecs.BytesCodec(), zarr.codecs.ZstdCodec()],         \"chunks\": (1, 1, 721, 1440)     } } In\u00a0[7]: Copied! <pre>import s3fs\ns3 = s3fs.S3FileSystem(use_listings_cache=False)\n</pre> import s3fs s3 = s3fs.S3FileSystem(use_listings_cache=False) In\u00a0[10]: Copied! <pre>target_path = \"icechunk-test/ryan/zarr-v3/test-era5-v3-919\"\nstore = zarr.storage.RemoteStore(s3, mode=\"w\", path=target_path)\n</pre> target_path = \"icechunk-test/ryan/zarr-v3/test-era5-v3-919\" store = zarr.storage.RemoteStore(s3, mode=\"w\", path=target_path) In\u00a0[11]: Copied! <pre>%time dsl.to_zarr(store, consolidated=False, zarr_format=3, encoding=encoding, mode=\"w\")\n</pre> %time dsl.to_zarr(store, consolidated=False, zarr_format=3, encoding=encoding, mode=\"w\") <pre>CPU times: user 36.2 s, sys: 2.53 s, total: 38.7 s\nWall time: 15.8 s\n</pre> Out[11]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7f5a3839efc0&gt;</pre> In\u00a0[48]: Copied! <pre># with dask\ndslc = dsl.chunk({\"time\": 1, \"level\": 1})\nstore_d = zarr.storage.RemoteStore(s3, mode=\"w\", path=target_url + \"-dask\")\nwith ProgressBar():\n    dslc.to_zarr(store_d, consolidated=False, zarr_format=3, encoding=encoding, mode=\"w\")\n</pre> # with dask dslc = dsl.chunk({\"time\": 1, \"level\": 1}) store_d = zarr.storage.RemoteStore(s3, mode=\"w\", path=target_url + \"-dask\") with ProgressBar():     dslc.to_zarr(store_d, consolidated=False, zarr_format=3, encoding=encoding, mode=\"w\") <pre>[########################################] | 100% Completed | 12.60 s\n</pre> In\u00a0[12]: Copied! <pre>#store = zarr.storage.RemoteStore(s3, mode=\"r\", path=target_url)\n%time dss = xr.open_dataset(store, consolidated=False, zarr_format=3, engine=\"zarr\")\n</pre> #store = zarr.storage.RemoteStore(s3, mode=\"r\", path=target_url) %time dss = xr.open_dataset(store, consolidated=False, zarr_format=3, engine=\"zarr\") <pre>CPU times: user 35.6 ms, sys: 0 ns, total: 35.6 ms\nWall time: 343 ms\n</pre> In\u00a0[13]: Copied! <pre>dss\n</pre> dss Out[13]: <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:    (time: 24, level: 37, latitude: 721, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float64 6kB 90.0 89.75 89.5 ... -89.5 -89.75 -90.0\n  * level      (level) float64 296B 1.0 2.0 3.0 5.0 ... 925.0 950.0 975.0 1e+03\n  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n  * time       (time) datetime64[ns] 192B 1941-06-01 ... 1941-06-01T23:00:00\nData variables:\n    PV         (time, level, latitude, longitude) float32 4GB ...\n    utc_date   (time) int32 96B ...\nAttributes:\n    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB 1 data to netC...\n    NETCDF_VERSION:       4.8.1\n    CONVERSION_PLATFORM:  Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 1...\n    CONVERSION_DATE:      Wed May 10 06:33:49 MDT 2023\n    Conventions:          CF-1.6\n    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n    history:              Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e...\n    NCO:                  netCDF Operators version 5.0.3 (Homepage = http://n...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 24</li><li>level: 37</li><li>latitude: 721</li><li>longitude: 1440</li></ul></li><li>Coordinates: (4)<ul><li>latitude(latitude)float6490.0 89.75 89.5 ... -89.75 -90.0long_name :latitudeshort_name :latunits :degrees_north<pre>array([ 90.  ,  89.75,  89.5 , ..., -89.5 , -89.75, -90.  ])</pre></li><li>level(level)float641.0 2.0 3.0 ... 950.0 975.0 1e+03long_name :pressure levelshort_name :plevunits :hPaalternate_units :millibar<pre>array([   1.,    2.,    3.,    5.,    7.,   10.,   20.,   30.,   50.,   70.,\n        100.,  125.,  150.,  175.,  200.,  225.,  250.,  300.,  350.,  400.,\n        450.,  500.,  550.,  600.,  650.,  700.,  750.,  775.,  800.,  825.,\n        850.,  875.,  900.,  925.,  950.,  975., 1000.])</pre></li><li>longitude(longitude)float640.0 0.25 0.5 ... 359.2 359.5 359.8long_name :longitudeshort_name :lonunits :degrees_east<pre>array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n       3.5975e+02])</pre></li><li>time(time)datetime64[ns]1941-06-01 ... 1941-06-01T23:00:00long_name :time<pre>array(['1941-06-01T00:00:00.000000000', '1941-06-01T01:00:00.000000000',\n       '1941-06-01T02:00:00.000000000', '1941-06-01T03:00:00.000000000',\n       '1941-06-01T04:00:00.000000000', '1941-06-01T05:00:00.000000000',\n       '1941-06-01T06:00:00.000000000', '1941-06-01T07:00:00.000000000',\n       '1941-06-01T08:00:00.000000000', '1941-06-01T09:00:00.000000000',\n       '1941-06-01T10:00:00.000000000', '1941-06-01T11:00:00.000000000',\n       '1941-06-01T12:00:00.000000000', '1941-06-01T13:00:00.000000000',\n       '1941-06-01T14:00:00.000000000', '1941-06-01T15:00:00.000000000',\n       '1941-06-01T16:00:00.000000000', '1941-06-01T17:00:00.000000000',\n       '1941-06-01T18:00:00.000000000', '1941-06-01T19:00:00.000000000',\n       '1941-06-01T20:00:00.000000000', '1941-06-01T21:00:00.000000000',\n       '1941-06-01T22:00:00.000000000', '1941-06-01T23:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (2)<ul><li>PV(time, level, latitude, longitude)float32...long_name :Potential vorticityshort_name :pvunits :K m**2 kg**-1 s**-1original_format :WMO GRIB 1 with ECMWF local tableecmwf_local_table :128ecmwf_parameter :60minimum_value :-0.03200835734605789maximum_value :0.023797351866960526grid_specification :0.25 degree x 0.25 degree from 90N to 90S and 0E to 359.75E (721 x 1440 Latitude/Longitude)rda_dataset :ds633.0rda_dataset_url :https:/rda.ucar.edu/datasets/ds633.0/rda_dataset_doi :DOI: 10.5065/BH6N-5N20rda_dataset_group :ERA5 atmospheric pressure level analysis [netCDF4]QuantizeGranularBitGroomNumberOfSignificantDigits :7<pre>[921957120 values with dtype=float32]</pre></li><li>utc_date(time)int32...long_name :UTC date yyyy-mm-dd hh:00:00 as yyyymmddhhunits :Gregorian_year month day hour<pre>[24 values with dtype=int32]</pre></li></ul></li><li>Indexes: (4)<ul><li>latitudePandasIndex<pre>PandasIndex(Index([  90.0,  89.75,   89.5,  89.25,   89.0,  88.75,   88.5,  88.25,   88.0,\n        87.75,\n       ...\n       -87.75,  -88.0, -88.25,  -88.5, -88.75,  -89.0, -89.25,  -89.5, -89.75,\n        -90.0],\n      dtype='float64', name='latitude', length=721))</pre></li><li>levelPandasIndex<pre>PandasIndex(Index([   1.0,    2.0,    3.0,    5.0,    7.0,   10.0,   20.0,   30.0,   50.0,\n         70.0,  100.0,  125.0,  150.0,  175.0,  200.0,  225.0,  250.0,  300.0,\n        350.0,  400.0,  450.0,  500.0,  550.0,  600.0,  650.0,  700.0,  750.0,\n        775.0,  800.0,  825.0,  850.0,  875.0,  900.0,  925.0,  950.0,  975.0,\n       1000.0],\n      dtype='float64', name='level'))</pre></li><li>longitudePandasIndex<pre>PandasIndex(Index([   0.0,   0.25,    0.5,   0.75,    1.0,   1.25,    1.5,   1.75,    2.0,\n         2.25,\n       ...\n        357.5, 357.75,  358.0, 358.25,  358.5, 358.75,  359.0, 359.25,  359.5,\n       359.75],\n      dtype='float64', name='longitude', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1941-06-01 00:00:00', '1941-06-01 01:00:00',\n               '1941-06-01 02:00:00', '1941-06-01 03:00:00',\n               '1941-06-01 04:00:00', '1941-06-01 05:00:00',\n               '1941-06-01 06:00:00', '1941-06-01 07:00:00',\n               '1941-06-01 08:00:00', '1941-06-01 09:00:00',\n               '1941-06-01 10:00:00', '1941-06-01 11:00:00',\n               '1941-06-01 12:00:00', '1941-06-01 13:00:00',\n               '1941-06-01 14:00:00', '1941-06-01 15:00:00',\n               '1941-06-01 16:00:00', '1941-06-01 17:00:00',\n               '1941-06-01 18:00:00', '1941-06-01 19:00:00',\n               '1941-06-01 20:00:00', '1941-06-01 21:00:00',\n               '1941-06-01 22:00:00', '1941-06-01 23:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (9)DATA_SOURCE :ECMWF: https://cds.climate.copernicus.eu, Copernicus Climate Data StoreNETCDF_CONVERSION :CISL RDA: Conversion from ECMWF GRIB 1 data to netCDF4.NETCDF_VERSION :4.8.1CONVERSION_PLATFORM :Linux r1i4n4 4.12.14-95.51-default #1 SMP Fri Apr 17 08:14:12 UTC 2020 (c6bab98) x86_64 x86_64 x86_64 GNU/LinuxCONVERSION_DATE :Wed May 10 06:33:49 MDT 2023Conventions :CF-1.6NETCDF_COMPRESSION :NCO: Precision-preserving compression to netCDF4/HDF5 (see \"history\" and \"NCO\" global attributes below for specifics).history :Wed May 10 06:34:19 2023: ncks -4 --ppc default=7 e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.unc.nc e5.oper.an.pl.128_060_pv.ll025sc.1941060100_1941060123.ncNCO :netCDF Operators version 5.0.3 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)</li></ul> In\u00a0[14]: Copied! <pre>%time dss.PV[0, 0, 0, 0].values\n</pre> %time dss.PV[0, 0, 0, 0].values <pre>CPU times: user 15.7 ms, sys: 0 ns, total: 15.7 ms\nWall time: 101 ms\n</pre> Out[14]: <pre>array(0.00710905, dtype=float32)</pre> In\u00a0[16]: Copied! <pre>%time _ = dss.compute()\n</pre> %time _ = dss.compute() <pre>CPU times: user 8.41 s, sys: 1.19 s, total: 9.6 s\nWall time: 5.11 s\n</pre> In\u00a0[17]: Copied! <pre>dssd = xr.open_dataset(store, consolidated=False, engine=\"zarr\").chunk({\"time\": 1, \"level\": 10})\n</pre> dssd = xr.open_dataset(store, consolidated=False, engine=\"zarr\").chunk({\"time\": 1, \"level\": 10}) In\u00a0[18]: Copied! <pre>with ProgressBar():\n    _ = dssd.compute()\n</pre> with ProgressBar():     _ = dssd.compute() <pre>[########################################] | 100% Completed | 6.26 sms\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr3/#icechunk-performance-zarr-v3","title":"Icechunk Performance - Zarr V3\u00b6","text":"<p>Using data from the NCAR ERA5 AWS Public Dataset.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr3/#load-data-from-hdf5-file","title":"Load Data from HDF5 File\u00b6","text":"<p>This illustrates how loading directly from HDF5 files on S3 can be slow, even with Dask.</p>"},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr3/#write-zarr-store-no-dask","title":"Write Zarr Store - No Dask\u00b6","text":""},{"location":"icechunk-python/notebooks/performance/era5_xarray-zarr3/#read-data-back","title":"Read Data Back\u00b6","text":""}]}